{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Movie Recommendation System\n",
        "\n",
        "A comprehensive recommendation system based on MovieLens 100k + TMDB + ViT Image Features + BERT Text Features\n",
        "\n",
        "## Key Features\n",
        "- **Smart Quantity Control**: Set target number of movies with automatic progress management\n",
        "- **Image Feature Extraction**: ViT model for extracting visual features from posters and stills  \n",
        "- **Text Feature Extraction**: BERT model for processing movie overviews and taglines\n",
        "- **Cast & Crew Statistics**: Extract high-frequency actors and directors as features\n",
        "- **Multi-dimensional Feature Engineering**: Fusion of numerical, categorical, text, and image features\n",
        "- **Performance Comparison**: Comprehensive evaluation against traditional recommendation systems\n",
        "\n",
        "## System Architecture\n",
        "1. **Data Preparation**: MovieLens + TMDB + User Filtering\n",
        "2. **Image Processing**: Download → Selection → ViT Feature Extraction\n",
        "3. **Text Processing**: BERT Feature Extraction\n",
        "4. **Feature Engineering**: Multimodal Feature Fusion\n",
        "5. **Recommendation System**: Multi-algorithm Performance Comparison\n",
        "\n",
        "## Innovation Points\n",
        "- Multimodal feature fusion (text + image + traditional features)\n",
        "- Intelligent image selection (5 most diverse images from 10)\n",
        "- Comprehensive performance evaluation (reproducing best HybridRec configuration)\n",
        "\n",
        "## Technical Stack\n",
        "- **Computer Vision**: ViT (Vision Transformer) for image understanding\n",
        "- **Natural Language Processing**: BERT for semantic text analysis\n",
        "- **Recommendation Algorithms**: Collaborative Filtering, Content-based, Hybrid approaches\n",
        "- **Evaluation Metrics**: RMSE, MAE with statistical significance testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: pillow in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: torch in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.7.1)\n",
            "Requirement already satisfied: torchvision in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (0.22.1)\n",
            "Requirement already satisfied: scikit-learn in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: opencv-python in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: colorama in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: filelock in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.8.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Enhanced Multimodal Recommendation System\n",
            "============================================================\n",
            "Target movie count: 42\n",
            "Images per movie: 10 -> select 5\n",
            "Image directories: multimodal_images -> selected_images\n",
            "Data directory: multimodal_data\n",
            "API request interval: 0.3 seconds\n",
            "\n",
            "Environment setup completed\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install requests pandas pillow tqdm transformers torch torchvision scikit-learn opencv-python\n",
        "!pip install --upgrade scikit-image\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from transformers import BertTokenizer, BertModel, ViTImageProcessor, ViTModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"Enhanced Multimodal Recommendation System\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ===================== Core Configuration Parameters =====================\n",
        "# Target movie count - user configurable\n",
        "TARGET_MOVIE_COUNT = 42  # Set the desired number of movies to process\n",
        "\n",
        "# API Configuration\n",
        "TMDB_API_KEY = \"6ba3eb883961b80c06d196906b976afe\"\n",
        "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "TMDB_IMAGE_BASE_URL = \"https://image.tmdb.org/t/p/original\"\n",
        "\n",
        "# File path configuration\n",
        "IMAGE_DIR = \"multimodal_images\"          # Original image directory\n",
        "SELECTED_IMAGE_DIR = \"selected_images\"    # Selected image directory\n",
        "DATA_DIR = \"multimodal_data\"             # Data storage directory\n",
        "PROGRESS_FILE = \"multimodal_progress.json\" # Progress tracking file\n",
        "\n",
        "# Processing parameters\n",
        "IMAGES_PER_MOVIE = 10    # Number of images to download per movie\n",
        "SELECTED_IMAGES = 5      # Number of images to select after filtering\n",
        "DELAY_BETWEEN_REQUESTS = 0.3  # API request interval (seconds)\n",
        "\n",
        "# Create directories\n",
        "for directory in [IMAGE_DIR, SELECTED_IMAGE_DIR, DATA_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Target movie count: {TARGET_MOVIE_COUNT}\")\n",
        "print(f\"Images per movie: {IMAGES_PER_MOVIE} -> select {SELECTED_IMAGES}\")\n",
        "print(f\"Image directories: {IMAGE_DIR} -> {SELECTED_IMAGE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"API request interval: {DELAY_BETWEEN_REQUESTS} seconds\")\n",
        "print(\"\\nEnvironment setup completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loading and Progress Check\n",
            "==================================================\n",
            "MovieLens movie data loaded: 1682 movies\n",
            "IMDB ID mapping loaded: 42 entries\n",
            "User rating data loaded: 100,000 ratings\n",
            "Number of users: 943\n",
            "Number of movies: 1,682\n",
            "User information loaded: 943 users\n",
            "\n",
            "Target movie processing: 42 / 42 valid movies\n",
            "Data coverage: 100.0%\n",
            "\n",
            "Target movie list:\n",
            "    1.   1. Toy Story (1995) -> tt0114709\n",
            "    2.   2. GoldenEye (1995) -> tt0113189\n",
            "    3.   3. Four Rooms (1995) -> tt0113101\n",
            "    4.   4. Get Shorty (1995) -> tt0113161\n",
            "    5.   5. Copycat (1995) -> tt0112722\n",
            "    6.   6. Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) -> tt0115012\n",
            "    7.   7. Twelve Monkeys (1995) -> tt0114746\n",
            "    8.   8. Babe (1995) -> tt0112431\n",
            "    9.   9. Dead Man Walking (1995) -> tt0112818\n",
            "   10.  10. Richard III (1995) -> tt0114279\n",
            "   ... and 32 more movies\n",
            "\n",
            "Target movie list saved: multimodal_data\\target_movies.csv\n"
          ]
        }
      ],
      "source": [
        "# Data loading and intelligent progress management\n",
        "print(\"Data Loading and Progress Check\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load MovieLens movie data\n",
        "try:\n",
        "    movies_df = pd.read_csv('movielens_movies.csv')\n",
        "    print(f\"MovieLens movie data loaded: {len(movies_df)} movies\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: movielens_movies.csv not found\")\n",
        "    print(\"Please run generate_imdb_mapping.py to generate movie information\")\n",
        "\n",
        "# Load IMDB ID mapping\n",
        "try:\n",
        "    with open('imdb/progress_mapping.json', 'r', encoding='utf-8') as f:\n",
        "        imdb_mapping = json.load(f)\n",
        "    imdb_mapping = {int(k): v for k, v in imdb_mapping.items()}\n",
        "    print(f\"IMDB ID mapping loaded: {len(imdb_mapping)} entries\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: IMDB ID mapping file not found\")\n",
        "\n",
        "# Load user rating data\n",
        "try:\n",
        "    ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', \n",
        "                           names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "    print(f\"User rating data loaded: {len(ratings_df):,} ratings\")\n",
        "    print(f\"Number of users: {ratings_df['user_id'].nunique():,}\")\n",
        "    print(f\"Number of movies: {ratings_df['movie_id'].nunique():,}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: ml-100k/u.data file not found\")\n",
        "\n",
        "# Load user information\n",
        "try:\n",
        "    users_df = pd.read_csv('ml-100k/u.user', sep='|', \n",
        "                         names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "    print(f\"User information loaded: {len(users_df)} users\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: ml-100k/u.user file not found\")\n",
        "\n",
        "# Intelligent target movie selection\n",
        "if 'movies_df' in locals() and 'imdb_mapping' in locals():\n",
        "    valid_movies = movies_df[movies_df['movie_id'].isin(imdb_mapping.keys())].copy()\n",
        "    valid_movies['imdb_id'] = valid_movies['movie_id'].map(imdb_mapping)\n",
        "    \n",
        "    # Select target movies based on TARGET_MOVIE_COUNT\n",
        "    target_movies = valid_movies.head(TARGET_MOVIE_COUNT).copy()\n",
        "    \n",
        "    print(f\"\\nTarget movie processing: {len(target_movies)} / {len(valid_movies)} valid movies\")\n",
        "    print(f\"Data coverage: {len(target_movies)/len(valid_movies)*100:.1f}%\")\n",
        "    \n",
        "    # Display target movie list\n",
        "    print(f\"\\nTarget movie list:\")\n",
        "    for i, (_, movie) in enumerate(target_movies.head(10).iterrows(), 1):\n",
        "        print(f\"   {i:2d}. {movie['movie_id']:3d}. {movie['title']} ({movie['year']}) -> tt{movie['imdb_id']}\")\n",
        "    \n",
        "    if len(target_movies) > 10:\n",
        "        print(f\"   ... and {len(target_movies) - 10} more movies\")\n",
        "        \n",
        "    # Save target movie list\n",
        "    target_file = os.path.join(DATA_DIR, 'target_movies.csv')\n",
        "    target_movies.to_csv(target_file, index=False)\n",
        "    print(f\"\\nTarget movie list saved: {target_file}\")\n",
        "else:\n",
        "    print(\"ERROR: Data loading failed, cannot continue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Data Filtering and Cleaning\n",
            "==================================================\n",
            "Original rating data: 100,000 entries\n",
            "Target movie ratings: 5,845 entries\n",
            "Data filtering rate: 5.8%\n",
            "\n",
            "User activity analysis:\n",
            "   Total users: 766\n",
            "   Average ratings per user: 7.6\n",
            "   Median ratings per user: 6.0\n",
            "   Most active user: 42 ratings\n",
            "   Least active user: 1 ratings\n",
            "\n",
            "Data cleaning results:\n",
            "   Minimum ratings requirement: 3 entries\n",
            "   Users retained: 630 / 766 (82.2%)\n",
            "   Ratings retained: 5,635 / 5,845 (96.4%)\n",
            "\n",
            "Final cleaned dataset:\n",
            "   Rating records: 5,635 entries\n",
            "   Number of users: 630\n",
            "   Number of movies: 42\n",
            "   Average ratings per user: 8.9\n",
            "   Average ratings per movie: 134.2\n",
            "\n",
            "User demographic distribution:\n",
            "   Age range: 7-73 years\n",
            "   Average age: 32.0 years\n",
            "   Gender distribution: {'M': np.int64(4329), 'F': np.int64(1306)}\n",
            "   Top 5 occupations: {'student': np.int64(1351), 'other': np.int64(595), 'educator': np.int64(517), 'engineer': np.int64(489), 'programmer': np.int64(475)}\n",
            "\n",
            "Rating distribution:\n",
            "   1 stars: 231 entries (4.1%)\n",
            "   2 stars: 547 entries (9.7%)\n",
            "   3 stars: 1,433 entries (25.4%)\n",
            "   4 stars: 2,071 entries (36.8%)\n",
            "   5 stars: 1,353 entries (24.0%)\n",
            "   Average rating: 3.67\n",
            "\n",
            "Cleaned data saved: multimodal_data\\cleaned_ratings_data.csv\n",
            "Updated target movie count: 42 movies (with actual rating data)\n",
            "\n",
            "Rating matrix dimensions:\n",
            "   User-Movie matrix: 630 x 42 = 26,460 possible ratings\n",
            "   Actual ratings: 5,635\n",
            "   Sparsity: 78.70% (percentage of missing values)\n",
            "   Density: 21.30% (percentage of observed values)\n"
          ]
        }
      ],
      "source": [
        "# User data filtering and cleaning\n",
        "print(\"User Data Filtering and Cleaning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'target_movies' in locals() and 'ratings_df' in locals():\n",
        "    # Filter rating data for target movies\n",
        "    target_movie_ids = set(target_movies['movie_id'].values)\n",
        "    filtered_ratings = ratings_df[ratings_df['movie_id'].isin(target_movie_ids)].copy()\n",
        "    \n",
        "    print(f\"Original rating data: {len(ratings_df):,} entries\")\n",
        "    print(f\"Target movie ratings: {len(filtered_ratings):,} entries\")\n",
        "    print(f\"Data filtering rate: {len(filtered_ratings)/len(ratings_df)*100:.1f}%\")\n",
        "    \n",
        "    # Analyze user activity\n",
        "    user_activity = filtered_ratings['user_id'].value_counts()\n",
        "    print(f\"\\nUser activity analysis:\")\n",
        "    print(f\"   Total users: {len(user_activity):,}\")\n",
        "    print(f\"   Average ratings per user: {user_activity.mean():.1f}\")\n",
        "    print(f\"   Median ratings per user: {user_activity.median():.1f}\")\n",
        "    print(f\"   Most active user: {user_activity.max()} ratings\")\n",
        "    print(f\"   Least active user: {user_activity.min()} ratings\")\n",
        "    \n",
        "    # Clean low-activity users (less than 3 ratings)\n",
        "    MIN_RATINGS_PER_USER = 3\n",
        "    active_users = user_activity[user_activity >= MIN_RATINGS_PER_USER].index\n",
        "    cleaned_ratings = filtered_ratings[filtered_ratings['user_id'].isin(active_users)].copy()\n",
        "    \n",
        "    print(f\"\\nData cleaning results:\")\n",
        "    print(f\"   Minimum ratings requirement: {MIN_RATINGS_PER_USER} entries\")\n",
        "    print(f\"   Users retained: {len(active_users):,} / {len(user_activity):,} ({len(active_users)/len(user_activity)*100:.1f}%)\")\n",
        "    print(f\"   Ratings retained: {len(cleaned_ratings):,} / {len(filtered_ratings):,} ({len(cleaned_ratings)/len(filtered_ratings)*100:.1f}%)\")\n",
        "    \n",
        "    # Merge user information and handle missing values\n",
        "    if 'users_df' in locals():\n",
        "        cleaned_ratings_with_users = cleaned_ratings.merge(users_df, on='user_id', how='left')\n",
        "        \n",
        "        # Check and clean missing user information\n",
        "        missing_user_info = cleaned_ratings_with_users['age'].isna().sum()\n",
        "        if missing_user_info > 0:\n",
        "            print(f\"WARNING: Missing user information: {missing_user_info:,} entries ({missing_user_info/len(cleaned_ratings_with_users)*100:.1f}%)\")\n",
        "            # Remove records with missing user information\n",
        "            cleaned_ratings_with_users.dropna(subset=['age', 'gender', 'occupation'], inplace=True)\n",
        "            print(f\"Ratings after cleaning: {len(cleaned_ratings_with_users):,} entries\")\n",
        "        \n",
        "        print(f\"\\nFinal cleaned dataset:\")\n",
        "        print(f\"   Rating records: {len(cleaned_ratings_with_users):,} entries\")\n",
        "        print(f\"   Number of users: {cleaned_ratings_with_users['user_id'].nunique():,}\")\n",
        "        print(f\"   Number of movies: {cleaned_ratings_with_users['movie_id'].nunique():,}\")\n",
        "        print(f\"   Average ratings per user: {len(cleaned_ratings_with_users)/cleaned_ratings_with_users['user_id'].nunique():.1f}\")\n",
        "        print(f\"   Average ratings per movie: {len(cleaned_ratings_with_users)/cleaned_ratings_with_users['movie_id'].nunique():.1f}\")\n",
        "        \n",
        "        # User feature analysis\n",
        "        print(f\"\\nUser demographic distribution:\")\n",
        "        print(f\"   Age range: {cleaned_ratings_with_users['age'].min()}-{cleaned_ratings_with_users['age'].max()} years\")\n",
        "        print(f\"   Average age: {cleaned_ratings_with_users['age'].mean():.1f} years\")\n",
        "        print(f\"   Gender distribution: {dict(cleaned_ratings_with_users['gender'].value_counts())}\")\n",
        "        print(f\"   Top 5 occupations: {dict(cleaned_ratings_with_users['occupation'].value_counts().head())}\")\n",
        "        \n",
        "        # Rating distribution analysis\n",
        "        rating_dist = cleaned_ratings_with_users['rating'].value_counts().sort_index()\n",
        "        print(f\"\\nRating distribution:\")\n",
        "        for rating, count in rating_dist.items():\n",
        "            print(f\"   {rating} stars: {count:,} entries ({count/len(cleaned_ratings_with_users)*100:.1f}%)\")\n",
        "        print(f\"   Average rating: {cleaned_ratings_with_users['rating'].mean():.2f}\")\n",
        "        \n",
        "        # Save cleaned data\n",
        "        cleaned_data_file = os.path.join(DATA_DIR, 'cleaned_ratings_data.csv')\n",
        "        cleaned_ratings_with_users.to_csv(cleaned_data_file, index=False)\n",
        "        print(f\"\\nCleaned data saved: {cleaned_data_file}\")\n",
        "        \n",
        "        # Update target_movies to include only movies with ratings\n",
        "        rated_movie_ids = set(cleaned_ratings_with_users['movie_id'].unique())\n",
        "        target_movies = target_movies[target_movies['movie_id'].isin(rated_movie_ids)].copy()\n",
        "        print(f\"Updated target movie count: {len(target_movies)} movies (with actual rating data)\")\n",
        "        \n",
        "        # Print rating matrix dimension information\n",
        "        n_users = cleaned_ratings_with_users['user_id'].nunique()\n",
        "        n_movies = cleaned_ratings_with_users['movie_id'].nunique()\n",
        "        sparsity = 1 - len(cleaned_ratings_with_users) / (n_users * n_movies)\n",
        "        print(f\"\\nRating matrix dimensions:\")\n",
        "        print(f\"   User-Movie matrix: {n_users} x {n_movies} = {n_users * n_movies:,} possible ratings\")\n",
        "        print(f\"   Actual ratings: {len(cleaned_ratings_with_users):,}\")\n",
        "        print(f\"   Sparsity: {sparsity*100:.2f}% (percentage of missing values)\")\n",
        "        print(f\"   Density: {(1-sparsity)*100:.2f}% (percentage of observed values)\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: Required data missing, cannot perform user data filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TMDB Data Acquisition and Image Download\n",
            "==================================================\n",
            "Progress loaded: 25 processed, 0 failed\n",
            "Remaining to process: 17 movies\n",
            "\n",
            "TMDB processor ready\n"
          ]
        }
      ],
      "source": [
        "# TMDB data acquisition and image download processor\n",
        "print(\"TMDB Data Acquisition and Image Download\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class EnhancedTMDBProcessor:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "        self.processed_movies = set()\n",
        "        self.failed_movies = set()\n",
        "        self.movie_features = {}\n",
        "        self.load_progress()\n",
        "    \n",
        "    def load_progress(self):\n",
        "        \"\"\"Load processing progress\"\"\"\n",
        "        if os.path.exists(PROGRESS_FILE):\n",
        "            try:\n",
        "                with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "                    progress = json.load(f)\n",
        "                self.processed_movies = set(progress.get(\"processed\", []))\n",
        "                self.failed_movies = set(progress.get(\"failed\", []))\n",
        "                self.movie_features = progress.get(\"movie_features\", {})\n",
        "                print(f\"Progress loaded: {len(self.processed_movies)} processed, {len(self.failed_movies)} failed\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Failed to load progress: {str(e)}\")\n",
        "    \n",
        "    def save_progress(self):\n",
        "        \"\"\"Save processing progress\"\"\"\n",
        "        try:\n",
        "            progress = {\n",
        "                \"processed\": list(self.processed_movies),\n",
        "                \"failed\": list(self.failed_movies),\n",
        "                \"movie_features\": self.movie_features,\n",
        "                \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"target_count\": TARGET_MOVIE_COUNT,\n",
        "                \"total_processed\": len(self.processed_movies),\n",
        "                \"total_failed\": len(self.failed_movies)\n",
        "            }\n",
        "            with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(progress, f, ensure_ascii=False, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save progress: {str(e)}\")\n",
        "    \n",
        "    def find_movie_by_imdb_id(self, imdb_id):\n",
        "        \"\"\"Find TMDB movie by IMDB ID\"\"\"\n",
        "        if not imdb_id.startswith('tt'):\n",
        "            imdb_id = f\"tt{imdb_id}\"\n",
        "        \n",
        "        url = f\"{TMDB_BASE_URL}/find/{imdb_id}\"\n",
        "        params = {\"api_key\": self.api_key, \"external_source\": \"imdb_id\"}\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if data.get(\"movie_results\"):\n",
        "                    return data[\"movie_results\"][0]\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"     ERROR: Search failed: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def get_movie_details(self, tmdb_id):\n",
        "        \"\"\"Get detailed movie information\"\"\"\n",
        "        url = f\"{TMDB_BASE_URL}/movie/{tmdb_id}\"\n",
        "        params = {\n",
        "            \"api_key\": self.api_key,\n",
        "            \"append_to_response\": \"credits,keywords,videos,images\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"     ERROR: Failed to get details: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def clean_filename(self, filename):\n",
        "        \"\"\"Clean filename for safe storage\"\"\"\n",
        "        invalid_chars = '<>:\"/\\\\|?*'\n",
        "        for char in invalid_chars:\n",
        "            filename = filename.replace(char, '_')\n",
        "        return filename[:100]\n",
        "    \n",
        "    def download_image(self, url, save_path):\n",
        "        \"\"\"Download and process image\"\"\"\n",
        "        if os.path.exists(save_path):\n",
        "            return True\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                if img.size[0] < 50 or img.size[1] < 50:\n",
        "                    return False\n",
        "                \n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "                \n",
        "                # Resize to 512x512 for ViT processing\n",
        "                img = img.resize((512, 512), Image.Resampling.LANCZOS)\n",
        "                img.save(save_path, \"JPEG\", quality=90, optimize=True)\n",
        "                return True\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"       ERROR: Download failed: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def download_movie_images(self, movie_id, title, tmdb_movie):\n",
        "        \"\"\"Download movie images - up to 10 images\"\"\"\n",
        "        movie_folder = os.path.join(IMAGE_DIR, f\"{movie_id:04d}_{self.clean_filename(title)}\")\n",
        "        os.makedirs(movie_folder, exist_ok=True)\n",
        "        \n",
        "        downloaded = []\n",
        "        \n",
        "        # 1. Download poster\n",
        "        if tmdb_movie.get(\"poster_path\"):\n",
        "            poster_url = f\"{TMDB_IMAGE_BASE_URL}{tmdb_movie['poster_path']}\"\n",
        "            poster_path = os.path.join(movie_folder, \"poster.jpg\")\n",
        "            if self.download_image(poster_url, poster_path):\n",
        "                downloaded.append((\"poster\", poster_path))\n",
        "        \n",
        "        # 2. Download backdrop\n",
        "        if tmdb_movie.get(\"backdrop_path\"):\n",
        "            backdrop_url = f\"{TMDB_IMAGE_BASE_URL}{tmdb_movie['backdrop_path']}\"\n",
        "            backdrop_path = os.path.join(movie_folder, \"backdrop.jpg\")\n",
        "            if self.download_image(backdrop_url, backdrop_path):\n",
        "                downloaded.append((\"backdrop\", backdrop_path))\n",
        "        \n",
        "        # 3. Download stills (up to 8)\n",
        "        if \"images\" in tmdb_movie and \"backdrops\" in tmdb_movie[\"images\"]:\n",
        "            backdrops = tmdb_movie[\"images\"][\"backdrops\"][:8]\n",
        "            for i, backdrop_info in enumerate(backdrops):\n",
        "                backdrop_url = f\"{TMDB_IMAGE_BASE_URL}{backdrop_info['file_path']}\"\n",
        "                still_path = os.path.join(movie_folder, f\"still_{i+1}.jpg\")\n",
        "                if self.download_image(backdrop_url, still_path):\n",
        "                    downloaded.append((f\"still_{i+1}\", still_path))\n",
        "                    \n",
        "                # Limit to maximum of 10 images\n",
        "                if len(downloaded) >= IMAGES_PER_MOVIE:\n",
        "                    break\n",
        "        \n",
        "        return downloaded\n",
        "    \n",
        "    def extract_movie_features(self, tmdb_movie, movielens_info):\n",
        "        \"\"\"Extract comprehensive movie features\"\"\"\n",
        "        features = {\n",
        "            # MovieLens original information\n",
        "            \"movielens_id\": movielens_info[\"movie_id\"],\n",
        "            \"movielens_title\": movielens_info[\"title\"],\n",
        "            \"movielens_year\": movielens_info[\"year\"],\n",
        "            \"movielens_imdb_id\": movielens_info[\"imdb_id\"],\n",
        "            \n",
        "            # TMDB basic information\n",
        "            \"tmdb_id\": tmdb_movie.get(\"id\"),\n",
        "            \"title\": tmdb_movie.get(\"title\", \"\"),\n",
        "            \"original_title\": tmdb_movie.get(\"original_title\", \"\"),\n",
        "            \"overview\": tmdb_movie.get(\"overview\", \"\"),\n",
        "            \"tagline\": tmdb_movie.get(\"tagline\", \"\"),\n",
        "            \"release_date\": tmdb_movie.get(\"release_date\", \"\"),\n",
        "            \"runtime\": tmdb_movie.get(\"runtime\", 0),\n",
        "            \"status\": tmdb_movie.get(\"status\", \"\"),\n",
        "            \n",
        "            # Rating information\n",
        "            \"vote_average\": tmdb_movie.get(\"vote_average\", 0),\n",
        "            \"vote_count\": tmdb_movie.get(\"vote_count\", 0),\n",
        "            \"popularity\": tmdb_movie.get(\"popularity\", 0),\n",
        "            \n",
        "            # Classification information\n",
        "            \"genres\": \"|\".join([g[\"name\"] for g in tmdb_movie.get(\"genres\", [])]),\n",
        "            \"original_language\": tmdb_movie.get(\"original_language\", \"\"),\n",
        "            \"production_countries\": \"|\".join([c[\"name\"] for c in tmdb_movie.get(\"production_countries\", [])]),\n",
        "            \"production_companies\": \"|\".join([c[\"name\"] for c in tmdb_movie.get(\"production_companies\", [])]),\n",
        "            \n",
        "            # Financial information\n",
        "            \"budget\": tmdb_movie.get(\"budget\", 0),\n",
        "            \"revenue\": tmdb_movie.get(\"revenue\", 0),\n",
        "        }\n",
        "        \n",
        "        # Cast and crew information\n",
        "        if \"credits\" in tmdb_movie:\n",
        "            credits = tmdb_movie[\"credits\"]\n",
        "            \n",
        "            # Directors\n",
        "            directors = [crew[\"name\"] for crew in credits.get(\"crew\", []) if crew.get(\"job\") == \"Director\"]\n",
        "            features[\"directors\"] = \"|\".join(directors)\n",
        "            \n",
        "            # Cast\n",
        "            cast = [actor[\"name\"] for actor in credits.get(\"cast\", [])[:10]]\n",
        "            features[\"cast\"] = \"|\".join(cast)\n",
        "        \n",
        "        # Keywords\n",
        "        if \"keywords\" in tmdb_movie and \"keywords\" in tmdb_movie[\"keywords\"]:\n",
        "            keywords = [kw[\"name\"] for kw in tmdb_movie[\"keywords\"][\"keywords\"][:15]]\n",
        "            features[\"keywords\"] = \"|\".join(keywords)\n",
        "        \n",
        "        return features\n",
        "\n",
        "# Check progress status\n",
        "processor = EnhancedTMDBProcessor(TMDB_API_KEY)\n",
        "current_processed = len(processor.processed_movies)\n",
        "\n",
        "#print(f\"Current processing progress: {current_processed}/{TARGET_MOVIE_COUNT}\")\n",
        "\n",
        "# Intelligent progress management\n",
        "if current_processed >= TARGET_MOVIE_COUNT:\n",
        "    print(f\"Target achieved ({current_processed} >= {TARGET_MOVIE_COUNT})\")\n",
        "    print(\"Skipping download phase, using existing data\")\n",
        "    SKIP_DOWNLOAD = True\n",
        "else:\n",
        "    need_to_process = TARGET_MOVIE_COUNT - current_processed\n",
        "    print(f\"Remaining to process: {need_to_process} movies\")\n",
        "    SKIP_DOWNLOAD = False\n",
        "    \n",
        "print(\"\\nTMDB processor ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Similarity Calculation and Selection\n",
            "==================================================\n",
            "Current similarity calculation method: clip_features\n",
            "Available methods:\n",
            "Traditional Computer Vision Methods:\n",
            "  - 'histogram': HSV color histogram correlation\n",
            "  - 'ssim': Structural Similarity Index\n",
            "  - 'perceptual_hash': Perceptual hashing based on image structure\n",
            "  - 'combined': Weighted combination of traditional methods\n",
            "Deep Learning Methods:\n",
            "  - 'cnn_features': Pre-trained ResNet-50 CNN features\n",
            "  - 'clip_features': CLIP multimodal model features\n",
            "\n",
            "Method Characteristics:\n",
            "  Traditional methods: Fast, lightweight, good for basic similarity\n",
            "  CNN features: Captures high-level visual features, better semantic understanding\n",
            "  CLIP features: Best semantic similarity, trained on image-text pairs\n",
            "\n",
            "Starting image selection for 42 movies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   0%|          | 0/42 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Using similarity method: clip_features\n",
            "       Loading CLIP model (first time only)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b1ba04ded7543aaa1c5828589c443ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "589b39e8bdc04b429eecb1777f541a7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c87d0a4a2614367ab9834e3f9679dab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d492f22d2bb464da6e3bf71d02c65ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d97525919ff64b7d9ee5543f07d5a986",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f4d3d20a961422ba4a3d693836ebb76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88459d8a302243a0964b0da74c02ef2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec4eaa499c744622b7a0bc3ead511af7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0700ecf28355423f86c72c9f4eb2e5cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   2%|▏         | 1/42 [00:19<13:12, 19.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 1. Toy Story: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   5%|▍         | 2/42 [00:22<06:37,  9.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 2. GoldenEye: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   7%|▋         | 3/42 [00:26<04:30,  6.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 3. Four Rooms: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  10%|▉         | 4/42 [00:29<03:27,  5.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 4. Get Shorty: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  12%|█▏        | 5/42 [00:32<02:51,  4.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 5. Copycat: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  14%|█▍        | 6/42 [00:33<02:08,  3.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 6. Shanghai Triad (Yao a yao yao dao waipo qiao): 7 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  17%|█▋        | 7/42 [00:37<01:59,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 7. Twelve Monkeys: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  19%|█▉        | 8/42 [00:40<01:53,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 8. Babe: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  21%|██▏       | 9/42 [00:43<01:47,  3.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 9. Dead Man Walking: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  24%|██▍       | 10/42 [00:44<01:26,  2.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 10. Richard III: 7 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  26%|██▌       | 11/42 [00:47<01:27,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 11. Seven (Se7en): 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  29%|██▊       | 12/42 [00:50<01:27,  2.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 12. Usual Suspects, The: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  31%|███       | 13/42 [00:54<01:27,  3.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 13. Mighty Aphrodite: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  33%|███▎      | 14/42 [00:57<01:25,  3.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 14. Postino, Il: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  36%|███▌      | 15/42 [01:00<01:23,  3.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 15. Mr. Holland's Opus: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  38%|███▊      | 16/42 [01:03<01:20,  3.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 16. French Twist (Gazon maudit): 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  40%|████      | 17/42 [01:06<01:17,  3.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 17. From Dusk Till Dawn: 10 -> 5 images\n",
            "   SUCCESS: 18. White Balloon, The: 4 -> 4 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  45%|████▌     | 19/42 [01:08<00:46,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 19. Antonia's Line: 7 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  48%|████▊     | 20/42 [01:10<00:43,  1.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 20. Angels and Insects: 8 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  50%|█████     | 21/42 [01:13<00:48,  2.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 21. Muppet Treasure Island: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  52%|█████▏    | 22/42 [01:16<00:50,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 22. Braveheart: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  55%|█████▍    | 23/42 [01:19<00:50,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 23. Taxi Driver: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  57%|█████▋    | 24/42 [01:22<00:50,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 24. Rumble in the Bronx: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  60%|█████▉    | 25/42 [01:25<00:50,  2.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 25. Birdcage, The: 10 -> 5 images\n",
            "   SUCCESS: 26. Brothers McMullen, The: 4 -> 4 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  64%|██████▍   | 27/42 [01:31<00:42,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 27. Bad Boys: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  67%|██████▋   | 28/42 [01:36<00:47,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 28. Apollo 13: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  69%|██████▉   | 29/42 [01:41<00:49,  3.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 29. Batman Forever: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  71%|███████▏  | 30/42 [01:46<00:50,  4.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 30. Belle de jour: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  74%|███████▍  | 31/42 [01:51<00:48,  4.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 31. Crimson Tide: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  76%|███████▌  | 32/42 [01:56<00:46,  4.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 32. Crumb: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  79%|███████▊  | 33/42 [02:01<00:42,  4.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 33. Desperado: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  81%|████████  | 34/42 [02:06<00:38,  4.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 34. Doom Generation, The: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  83%|████████▎ | 35/42 [02:12<00:34,  4.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 35. Free Willy 2: The Adventure Home: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  86%|████████▌ | 36/42 [02:15<00:27,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 36. Mad Love: 9 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  88%|████████▊ | 37/42 [02:16<00:17,  3.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 37. Nadja: 6 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  90%|█████████ | 38/42 [02:20<00:13,  3.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 38. Net, The: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  93%|█████████▎| 39/42 [02:23<00:10,  3.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 39. Strange Days: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  95%|█████████▌| 40/42 [02:26<00:06,  3.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 40. To Wong Foo, Thanks for Everything! Julie Newmar: 10 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  98%|█████████▊| 41/42 [02:28<00:03,  3.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 41. Billy Madison: 9 -> 5 images\n",
            "       Using similarity method: clip_features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images: 100%|██████████| 42/42 [02:31<00:00,  3.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 42. Clerks: 10 -> 5 images\n",
            "\n",
            "Image selection statistics:\n",
            "   Similarity method used: clip_features\n",
            "   Movies processed: 42\n",
            "   Total selected images: 208\n",
            "   Average per movie: 5.0 images\n",
            "   Updated data saved: multimodal_data\\movie_features_with_selected_images.json\n",
            "\n",
            "Selection details:\n",
            "   Original images total: 391\n",
            "   Selected images total: 208\n",
            "   Selection rate: 53.2%\n",
            "\n",
            "==================================================\n",
            "To change similarity calculation method, modify the SIMILARITY_METHOD variable:\n",
            "# Traditional methods:\n",
            "SIMILARITY_METHOD = 'histogram'       # Color-based similarity (fastest)\n",
            "SIMILARITY_METHOD = 'ssim'            # Structural similarity\n",
            "SIMILARITY_METHOD = 'perceptual_hash' # Content-based similarity\n",
            "SIMILARITY_METHOD = 'combined'        # Combination of traditional methods\n",
            "\n",
            "# Deep learning methods (require additional libraries):\n",
            "SIMILARITY_METHOD = 'cnn_features'    # CNN features (good semantic understanding)\n",
            "SIMILARITY_METHOD = 'clip_features'   # CLIP features (best semantic similarity)\n",
            "\n",
            "Installation requirements for deep learning methods:\n",
            "pip install torch torchvision        # For CNN features\n",
            "pip install transformers torch       # For CLIP features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Image similarity calculation and selection - select 5 most diverse from 10\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "import hashlib\n",
        "\n",
        "# Deep learning imports (optional - install if using deep learning methods)\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision.transforms as transforms\n",
        "    import torchvision.models as models\n",
        "    from transformers import CLIPProcessor, CLIPModel\n",
        "    DEEP_LEARNING_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DEEP_LEARNING_AVAILABLE = False\n",
        "    print(\"Deep learning libraries not available. Install torch, torchvision, and transformers for CNN and CLIP methods.\")\n",
        "\n",
        "print(\"Image Similarity Calculation and Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# SIMILARITY CALCULATION METHOD SELECTION\n",
        "SIMILARITY_METHOD = \"clip_features\"  # Options: \"histogram\", \"ssim\", \"perceptual_hash\", \"combined\", \"cnn_features\", \"clip_features\"\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Remove or replace invalid characters for Windows file/folder names\"\"\"\n",
        "    # Replace invalid characters with underscore\n",
        "    invalid_chars = r'[<>:\"/\\\\|?*]'\n",
        "    sanitized = re.sub(invalid_chars, '_', filename)\n",
        "    \n",
        "    # Remove any trailing dots or spaces (also invalid in Windows)\n",
        "    sanitized = sanitized.rstrip('. ')\n",
        "    \n",
        "    # Ensure the name isn't empty after sanitization\n",
        "    if not sanitized.strip():\n",
        "        sanitized = \"unnamed\"\n",
        "    \n",
        "    return sanitized\n",
        "\n",
        "def calculate_histogram_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity between two images using HSV histogram comparison\"\"\"\n",
        "    try:\n",
        "        # Read images\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "        \n",
        "        if img1 is None or img2 is None:\n",
        "            return 0.0\n",
        "        \n",
        "        # Convert to HSV color space\n",
        "        hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
        "        hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
        "        \n",
        "        # Calculate histograms\n",
        "        hist1 = cv2.calcHist([hsv1], [0, 1, 2], None, [50, 60, 60], [0, 180, 0, 256, 0, 256])\n",
        "        hist2 = cv2.calcHist([hsv2], [0, 1, 2], None, [50, 60, 60], [0, 180, 0, 256, 0, 256])\n",
        "        \n",
        "        # Calculate correlation\n",
        "        correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
        "        return max(0.0, correlation)  # Ensure non-negative\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       Histogram similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_ssim_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity between two images using Structural Similarity Index (SSIM)\"\"\"\n",
        "    try:\n",
        "        # Read images\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "        \n",
        "        if img1 is None or img2 is None:\n",
        "            return 0.0\n",
        "        \n",
        "        # Convert to grayscale\n",
        "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "        # Resize to same dimensions for comparison\n",
        "        height = min(gray1.shape[0], gray2.shape[0])\n",
        "        width = min(gray1.shape[1], gray2.shape[1])\n",
        "        \n",
        "        gray1_resized = cv2.resize(gray1, (width, height))\n",
        "        gray2_resized = cv2.resize(gray2, (width, height))\n",
        "        \n",
        "        # Calculate SSIM\n",
        "        similarity_score = ssim(gray1_resized, gray2_resized)\n",
        "        return max(0.0, similarity_score)  # Ensure non-negative\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       SSIM similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_perceptual_hash_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity between two images using perceptual hashing\"\"\"\n",
        "    try:\n",
        "        def perceptual_hash(image_path, hash_size=8):\n",
        "            \"\"\"Calculate perceptual hash of an image\"\"\"\n",
        "            # Open and convert to grayscale\n",
        "            img = Image.open(image_path).convert('L')\n",
        "            \n",
        "            # Resize to hash_size x hash_size\n",
        "            img = img.resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
        "            \n",
        "            # Convert to numpy array\n",
        "            pixels = np.array(img)\n",
        "            \n",
        "            # Calculate average pixel value\n",
        "            avg = pixels.mean()\n",
        "            \n",
        "            # Create binary hash\n",
        "            binary_hash = pixels > avg\n",
        "            \n",
        "            return binary_hash.flatten()\n",
        "        \n",
        "        # Calculate hashes\n",
        "        hash1 = perceptual_hash(img1_path)\n",
        "        hash2 = perceptual_hash(img2_path)\n",
        "        \n",
        "        # Calculate Hamming distance (number of different bits)\n",
        "        hamming_distance = np.sum(hash1 != hash2)\n",
        "        \n",
        "        # Convert to similarity (0 = identical, 1 = completely different)\n",
        "        max_distance = len(hash1)\n",
        "        similarity = 1.0 - (hamming_distance / max_distance)\n",
        "        \n",
        "        return max(0.0, similarity)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       Perceptual hash similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_combined_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity using a combination of multiple methods\"\"\"\n",
        "    try:\n",
        "        # Weight for each method\n",
        "        weights = {\n",
        "            'histogram': 0.4,\n",
        "            'ssim': 0.4,\n",
        "            'perceptual': 0.2\n",
        "        }\n",
        "        \n",
        "        # Calculate individual similarities\n",
        "        hist_sim = calculate_histogram_similarity(img1_path, img2_path)\n",
        "        ssim_sim = calculate_ssim_similarity(img1_path, img2_path)\n",
        "        hash_sim = calculate_perceptual_hash_similarity(img1_path, img2_path)\n",
        "        \n",
        "        # Weighted combination\n",
        "        combined_sim = (weights['histogram'] * hist_sim + \n",
        "                       weights['ssim'] * ssim_sim + \n",
        "                       weights['perceptual'] * hash_sim)\n",
        "        \n",
        "        return max(0.0, combined_sim)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       Combined similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "# Global variables for CNN model (to avoid reloading)\n",
        "_cnn_model = None\n",
        "_cnn_transform = None\n",
        "\n",
        "def calculate_cnn_features_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity using pre-trained CNN features (ResNet-50)\"\"\"\n",
        "    if not DEEP_LEARNING_AVAILABLE:\n",
        "        print(\"       CNN features method requires torch and torchvision\")\n",
        "        return 0.0\n",
        "    \n",
        "    global _cnn_model, _cnn_transform\n",
        "    \n",
        "    try:\n",
        "        # Load model and transform once\n",
        "        if _cnn_model is None:\n",
        "            print(\"       Loading ResNet-50 model (first time only)...\")\n",
        "            _cnn_model = models.resnet50(pretrained=True)\n",
        "            _cnn_model.eval()\n",
        "            # Remove the final classification layer to get features\n",
        "            _cnn_model = torch.nn.Sequential(*list(_cnn_model.children())[:-1])\n",
        "            \n",
        "            _cnn_transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                   std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        \n",
        "        def extract_cnn_features(image_path):\n",
        "            \"\"\"Extract CNN features from an image\"\"\"\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image_tensor = _cnn_transform(image).unsqueeze(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                features = _cnn_model(image_tensor)\n",
        "                features = features.squeeze().numpy()\n",
        "            \n",
        "            return features\n",
        "        \n",
        "        # Extract features\n",
        "        features1 = extract_cnn_features(img1_path)\n",
        "        features2 = extract_cnn_features(img2_path)\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        dot_product = np.dot(features1, features2)\n",
        "        norm1 = np.linalg.norm(features1)\n",
        "        norm2 = np.linalg.norm(features2)\n",
        "        \n",
        "        if norm1 == 0 or norm2 == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        cosine_sim = dot_product / (norm1 * norm2)\n",
        "        # Convert from [-1, 1] to [0, 1]\n",
        "        similarity = (cosine_sim + 1) / 2\n",
        "        \n",
        "        return max(0.0, min(1.0, similarity))\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       CNN features similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "# Global variables for CLIP model\n",
        "_clip_model = None\n",
        "_clip_processor = None\n",
        "\n",
        "def calculate_clip_features_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity using CLIP model features\"\"\"\n",
        "    if not DEEP_LEARNING_AVAILABLE:\n",
        "        print(\"       CLIP features method requires transformers library\")\n",
        "        return 0.0\n",
        "    \n",
        "    global _clip_model, _clip_processor\n",
        "    \n",
        "    try:\n",
        "        # Load CLIP model and processor once\n",
        "        if _clip_model is None:\n",
        "            print(\"       Loading CLIP model (first time only)...\")\n",
        "            _clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "            _clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        \n",
        "        def extract_clip_features(image_path):\n",
        "            \"\"\"Extract CLIP features from an image\"\"\"\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = _clip_processor(images=image, return_tensors=\"pt\")\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                image_features = _clip_model.get_image_features(**inputs)\n",
        "                # Normalize features\n",
        "                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
        "            \n",
        "            return image_features.squeeze().numpy()\n",
        "        \n",
        "        # Extract features\n",
        "        features1 = extract_clip_features(img1_path)\n",
        "        features2 = extract_clip_features(img2_path)\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        cosine_sim = np.dot(features1, features2)\n",
        "        # CLIP features are already normalized, so dot product gives cosine similarity\n",
        "        # Convert from [-1, 1] to [0, 1]\n",
        "        similarity = (cosine_sim + 1) / 2\n",
        "        \n",
        "        return max(0.0, min(1.0, similarity))\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       CLIP features similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_image_similarity(img1_path, img2_path, method=SIMILARITY_METHOD):\n",
        "    \"\"\"Calculate similarity between two images using specified method\"\"\"\n",
        "    method_functions = {\n",
        "        'histogram': calculate_histogram_similarity,\n",
        "        'ssim': calculate_ssim_similarity,\n",
        "        'perceptual_hash': calculate_perceptual_hash_similarity,\n",
        "        'combined': calculate_combined_similarity,\n",
        "        'cnn_features': calculate_cnn_features_similarity,\n",
        "        'clip_features': calculate_clip_features_similarity\n",
        "    }\n",
        "    \n",
        "    if method not in method_functions:\n",
        "        print(f\"   WARNING: Unknown similarity method '{method}', using 'histogram'\")\n",
        "        method = 'histogram'\n",
        "    \n",
        "    # Check if deep learning methods are available\n",
        "    if method in ['cnn_features', 'clip_features'] and not DEEP_LEARNING_AVAILABLE:\n",
        "        print(f\"   WARNING: Deep learning method '{method}' not available, using 'histogram'\")\n",
        "        method = 'histogram'\n",
        "    \n",
        "    return method_functions[method](img1_path, img2_path)\n",
        "\n",
        "def select_diverse_images(image_paths, target_count=5, similarity_method=SIMILARITY_METHOD):\n",
        "    \"\"\"Select the most diverse target_count images from the image list\"\"\"\n",
        "    if len(image_paths) <= target_count:\n",
        "        return image_paths\n",
        "    \n",
        "    print(f\"       Using similarity method: {similarity_method}\")\n",
        "    \n",
        "    # Calculate similarity matrix for all image pairs\n",
        "    n = len(image_paths)\n",
        "    similarity_matrix = np.zeros((n, n))\n",
        "    \n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            sim = calculate_image_similarity(image_paths[i], image_paths[j], similarity_method)\n",
        "            similarity_matrix[i][j] = sim\n",
        "            similarity_matrix[j][i] = sim\n",
        "    \n",
        "    # Greedy algorithm to select most dissimilar images\n",
        "    selected_indices = [0]  # Start with first image\n",
        "    \n",
        "    for _ in range(target_count - 1):\n",
        "        max_min_sim = -1\n",
        "        best_candidate = -1\n",
        "        \n",
        "        for candidate in range(n):\n",
        "            if candidate in selected_indices:\n",
        "                continue\n",
        "            \n",
        "            # Calculate minimum similarity with already selected images\n",
        "            min_sim = min(similarity_matrix[candidate][selected] for selected in selected_indices)\n",
        "            \n",
        "            if min_sim > max_min_sim:\n",
        "                max_min_sim = min_sim\n",
        "                best_candidate = candidate\n",
        "        \n",
        "        if best_candidate != -1:\n",
        "            selected_indices.append(best_candidate)\n",
        "    \n",
        "    return [image_paths[i] for i in selected_indices]\n",
        "\n",
        "# Print current configuration\n",
        "print(f\"Current similarity calculation method: {SIMILARITY_METHOD}\")\n",
        "print(\"Available methods:\")\n",
        "print(\"Traditional Computer Vision Methods:\")\n",
        "print(\"  - 'histogram': HSV color histogram correlation\")\n",
        "print(\"  - 'ssim': Structural Similarity Index\")\n",
        "print(\"  - 'perceptual_hash': Perceptual hashing based on image structure\")\n",
        "print(\"  - 'combined': Weighted combination of traditional methods\")\n",
        "\n",
        "if DEEP_LEARNING_AVAILABLE:\n",
        "    print(\"Deep Learning Methods:\")\n",
        "    print(\"  - 'cnn_features': Pre-trained ResNet-50 CNN features\")\n",
        "    print(\"  - 'clip_features': CLIP multimodal model features\")\n",
        "else:\n",
        "    print(\"Deep Learning Methods (Not Available - install requirements):\")\n",
        "    print(\"  - 'cnn_features': Pre-trained ResNet-50 CNN features [REQUIRES: torch, torchvision]\")\n",
        "    print(\"  - 'clip_features': CLIP multimodal model features [REQUIRES: transformers]\")\n",
        "\n",
        "print()\n",
        "print(\"Method Characteristics:\")\n",
        "print(\"  Traditional methods: Fast, lightweight, good for basic similarity\")\n",
        "print(\"  CNN features: Captures high-level visual features, better semantic understanding\")\n",
        "print(\"  CLIP features: Best semantic similarity, trained on image-text pairs\")\n",
        "print()\n",
        "\n",
        "# Filter images for each movie\n",
        "if 'all_movie_features' in locals() and all_movie_features:\n",
        "    print(f\"Starting image selection for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    selected_image_stats = []\n",
        "    total_selected = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Selecting images\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        \n",
        "        # Get image paths\n",
        "        image_paths = movie_features.get('image_paths', [])\n",
        "        \n",
        "        if not image_paths:\n",
        "            print(f\"   WARNING: {movie_id}. {title}: No images found\")\n",
        "            continue\n",
        "        \n",
        "        # Verify image files exist\n",
        "        valid_paths = [path for path in image_paths if os.path.exists(path)]\n",
        "        \n",
        "        if len(valid_paths) == 0:\n",
        "            print(f\"   ERROR: {movie_id}. {title}: Image files do not exist\")\n",
        "            continue\n",
        "        \n",
        "        # Select most diverse images\n",
        "        selected_paths = select_diverse_images(valid_paths, SELECTED_IMAGES, SIMILARITY_METHOD)\n",
        "        \n",
        "        # Create selected image directory and copy images\n",
        "        # FIXED: Sanitize the title to remove invalid characters\n",
        "        sanitized_title = sanitize_filename(title[:50])\n",
        "        selected_folder = os.path.join(SELECTED_IMAGE_DIR, f\"{movie_id:04d}_{sanitized_title}\")\n",
        "        \n",
        "        try:\n",
        "            os.makedirs(selected_folder, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Failed to create directory for {movie_id}. {title}: {str(e)}\")\n",
        "            # Fallback: use only movie ID as folder name\n",
        "            selected_folder = os.path.join(SELECTED_IMAGE_DIR, f\"{movie_id:04d}\")\n",
        "            os.makedirs(selected_folder, exist_ok=True)\n",
        "        \n",
        "        copied_paths = []\n",
        "        for i, src_path in enumerate(selected_paths):\n",
        "            filename = f\"selected_{i+1}.jpg\"\n",
        "            dst_path = os.path.join(selected_folder, filename)\n",
        "            \n",
        "            try:\n",
        "                # Copy image\n",
        "                img = Image.open(src_path)\n",
        "                img.save(dst_path, \"JPEG\", quality=90)\n",
        "                copied_paths.append(dst_path)\n",
        "            except Exception as e:\n",
        "                print(f\"     ERROR: Failed to copy image: {str(e)}\")\n",
        "        \n",
        "        # Update feature data\n",
        "        movie_features['selected_image_paths'] = copied_paths\n",
        "        movie_features['selected_images_count'] = len(copied_paths)\n",
        "        movie_features['similarity_method_used'] = SIMILARITY_METHOD\n",
        "        \n",
        "        selected_image_stats.append({\n",
        "            'movie_id': movie_id,\n",
        "            'title': title,\n",
        "            'original_count': len(valid_paths),\n",
        "            'selected_count': len(copied_paths)\n",
        "        })\n",
        "        \n",
        "        total_selected += len(copied_paths)\n",
        "        \n",
        "        print(f\"   SUCCESS: {movie_id}. {title}: {len(valid_paths)} -> {len(copied_paths)} images\")\n",
        "    \n",
        "    # Save updated feature data\n",
        "    updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_selected_images.json')\n",
        "    with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"\\nImage selection statistics:\")\n",
        "    print(f\"   Similarity method used: {SIMILARITY_METHOD}\")\n",
        "    print(f\"   Movies processed: {len(selected_image_stats)}\")\n",
        "    print(f\"   Total selected images: {total_selected}\")\n",
        "    print(f\"   Average per movie: {total_selected/len(selected_image_stats):.1f} images\")\n",
        "    print(f\"   Updated data saved: {updated_features_file}\")\n",
        "    \n",
        "    # Display selection details\n",
        "    stats_df = pd.DataFrame(selected_image_stats)\n",
        "    print(f\"\\nSelection details:\")\n",
        "    print(f\"   Original images total: {stats_df['original_count'].sum()}\")\n",
        "    print(f\"   Selected images total: {stats_df['selected_count'].sum()}\")\n",
        "    print(f\"   Selection rate: {stats_df['selected_count'].sum()/stats_df['original_count'].sum()*100:.1f}%\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: No movie feature data available for image selection\")\n",
        "\n",
        "# Example of how to change similarity method:\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"To change similarity calculation method, modify the SIMILARITY_METHOD variable:\")\n",
        "print(\"# Traditional methods:\")\n",
        "print(\"SIMILARITY_METHOD = 'histogram'       # Color-based similarity (fastest)\")\n",
        "print(\"SIMILARITY_METHOD = 'ssim'            # Structural similarity\") \n",
        "print(\"SIMILARITY_METHOD = 'perceptual_hash' # Content-based similarity\")\n",
        "print(\"SIMILARITY_METHOD = 'combined'        # Combination of traditional methods\")\n",
        "print()\n",
        "print(\"# Deep learning methods (require additional libraries):\")\n",
        "print(\"SIMILARITY_METHOD = 'cnn_features'    # CNN features (good semantic understanding)\")\n",
        "print(\"SIMILARITY_METHOD = 'clip_features'   # CLIP features (best semantic similarity)\")\n",
        "print()\n",
        "print(\"Installation requirements for deep learning methods:\")\n",
        "print(\"pip install torch torchvision        # For CNN features\")\n",
        "print(\"pip install transformers torch       # For CLIP features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Compare different similarity calculation methods\n",
        "# Set this to True to run the comparison\n",
        "RUN_COMPARISON = True  # Change to True to enable comparison\n",
        "\n",
        "if RUN_COMPARISON:\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import random\n",
        "    from statistics import mean, stdev\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"SIMILARITY METHODS COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Initialize all_movie_features from processor or saved data\n",
        "    print(\"Loading movie feature data for comparison...\")\n",
        "    all_movie_features = None\n",
        "    \n",
        "    # Method 1: Try to load from processor if it exists\n",
        "    if 'processor' in globals() and hasattr(processor, 'movie_features') and processor.movie_features:\n",
        "        all_movie_features = list(processor.movie_features.values())\n",
        "        print(f\"✓ Loaded {len(all_movie_features)} movies from processor\")\n",
        "    else:\n",
        "        # Method 2: Try to load from the most recent saved file\n",
        "        try:\n",
        "            # Look for the most recent feature file\n",
        "            feature_files = [\n",
        "                os.path.join(DATA_DIR, 'movie_features_complete.json'),\n",
        "                os.path.join(DATA_DIR, 'movie_features_with_selected_images.json'),\n",
        "                os.path.join(DATA_DIR, 'movie_features_with_bert.json'),\n",
        "                os.path.join(DATA_DIR, 'movie_features_with_vit.json'),\n",
        "                'multimodal_progress.json'\n",
        "            ]\n",
        "            \n",
        "            loaded_from_file = False\n",
        "            for feature_file in feature_files:\n",
        "                if os.path.exists(feature_file):\n",
        "                    try:\n",
        "                        with open(feature_file, 'r', encoding='utf-8') as f:\n",
        "                            data = json.load(f)\n",
        "                        \n",
        "                        # Handle different file formats\n",
        "                        if 'movie_features' in data:  # Progress file format\n",
        "                            all_movie_features = list(data['movie_features'].values())\n",
        "                        elif isinstance(data, list):  # Direct list format\n",
        "                            all_movie_features = data\n",
        "                        elif isinstance(data, dict) and any(isinstance(v, dict) and 'movielens_id' in v for v in data.values()):\n",
        "                            all_movie_features = list(data.values())\n",
        "                        \n",
        "                        if all_movie_features:\n",
        "                            print(f\"✓ Loaded {len(all_movie_features)} movies from {os.path.basename(feature_file)}\")\n",
        "                            loaded_from_file = True\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Failed to load from {os.path.basename(feature_file)}: {str(e)}\")\n",
        "                        continue\n",
        "            \n",
        "            if not loaded_from_file:\n",
        "                print(\"✗ Could not load movie features from any file\")\n",
        "                all_movie_features = []\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading movie features: {str(e)}\")\n",
        "            all_movie_features = []\n",
        "    \n",
        "    # Display data summary\n",
        "    if all_movie_features:\n",
        "        movies_with_images = [movie for movie in all_movie_features \n",
        "                             if movie.get('image_paths') and len(movie.get('image_paths', [])) > 0]\n",
        "        movies_with_selected_images = [movie for movie in all_movie_features \n",
        "                                      if movie.get('selected_image_paths') and len(movie.get('selected_image_paths', [])) > 0]\n",
        "        print(f\"Data summary:\")\n",
        "        print(f\"  Total movies: {len(all_movie_features)}\")\n",
        "        print(f\"  Movies with images: {len(movies_with_images)}\")\n",
        "        print(f\"  Movies with selected images: {len(movies_with_selected_images)}\")\n",
        "    \n",
        "    # Sample selection for comparison\n",
        "    COMPARISON_SAMPLE_SIZE = 10  # Number of images to use for comparison\n",
        "    MAX_MOVIES_TO_TEST = 5       # Number of movies to test (to limit computation time)\n",
        "    \n",
        "    def collect_sample_images():\n",
        "        \"\"\"Collect sample images from processed movies for comparison\"\"\"\n",
        "        sample_images = []\n",
        "        \n",
        "        if not all_movie_features:\n",
        "            print(\"ERROR: No movie feature data available for comparison\")\n",
        "            return []\n",
        "        \n",
        "        # Collect images from multiple movies\n",
        "        movies_with_images = [movie for movie in all_movie_features \n",
        "                            if movie.get('image_paths') and len(movie.get('image_paths', [])) >= 3]\n",
        "        \n",
        "        if len(movies_with_images) == 0:\n",
        "            print(\"ERROR: No movies with sufficient images found\")\n",
        "            return []\n",
        "        \n",
        "        # Randomly select movies for testing\n",
        "        test_movies = random.sample(movies_with_images, \n",
        "                                  min(MAX_MOVIES_TO_TEST, len(movies_with_images)))\n",
        "        \n",
        "        for movie in test_movies:\n",
        "            valid_paths = [path for path in movie.get('image_paths', []) \n",
        "                         if os.path.exists(path)]\n",
        "            \n",
        "            # Take up to COMPARISON_SAMPLE_SIZE//MAX_MOVIES_TO_TEST images per movie\n",
        "            images_per_movie = max(1, COMPARISON_SAMPLE_SIZE // MAX_MOVIES_TO_TEST)\n",
        "            selected = random.sample(valid_paths, \n",
        "                                   min(images_per_movie, len(valid_paths)))\n",
        "            sample_images.extend(selected)\n",
        "        \n",
        "        # Ensure we don't exceed the sample size\n",
        "        if len(sample_images) > COMPARISON_SAMPLE_SIZE:\n",
        "            sample_images = random.sample(sample_images, COMPARISON_SAMPLE_SIZE)\n",
        "        \n",
        "        return sample_images\n",
        "    \n",
        "    def calculate_similarity_matrix(image_paths, method_name):\n",
        "        \"\"\"Calculate similarity matrix for given images using specified method\"\"\"\n",
        "        n = len(image_paths)\n",
        "        similarities = []\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):  # Only calculate upper triangle to avoid duplicates\n",
        "                sim = calculate_image_similarity(image_paths[i], image_paths[j], method_name)\n",
        "                similarities.append(sim)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        computation_time = end_time - start_time\n",
        "        \n",
        "        return similarities, computation_time\n",
        "    \n",
        "    def run_methods_comparison():\n",
        "        \"\"\"Run comparison of all available similarity methods\"\"\"\n",
        "        \n",
        "        # Collect sample images\n",
        "        print(\"Collecting sample images for comparison...\")\n",
        "        sample_images = collect_sample_images()\n",
        "        \n",
        "        if len(sample_images) < 3:\n",
        "            print(f\"ERROR: Need at least 3 images for comparison, found {len(sample_images)}\")\n",
        "            print(\"SOLUTION: Make sure movie data is processed and images are available\")\n",
        "            return None, []\n",
        "        \n",
        "        print(f\"Selected {len(sample_images)} images from {MAX_MOVIES_TO_TEST} movies for comparison\")\n",
        "        print(f\"Total similarity calculations per method: {len(sample_images) * (len(sample_images) - 1) // 2}\")\n",
        "        print()\n",
        "        \n",
        "        # Define methods to test\n",
        "        methods_to_test = ['histogram', 'ssim', 'perceptual_hash', 'combined']\n",
        "        \n",
        "        # Add deep learning methods if available\n",
        "        if DEEP_LEARNING_AVAILABLE:\n",
        "            methods_to_test.extend(['cnn_features', 'clip_features'])\n",
        "        \n",
        "        # Store results\n",
        "        results = []\n",
        "        \n",
        "        # Test each method\n",
        "        for method in methods_to_test:\n",
        "            print(f\"Testing method: {method}...\")\n",
        "            \n",
        "            try:\n",
        "                similarities, comp_time = calculate_similarity_matrix(sample_images, method)\n",
        "                \n",
        "                if similarities:\n",
        "                    avg_similarity = mean(similarities)\n",
        "                    std_similarity = stdev(similarities) if len(similarities) > 1 else 0.0\n",
        "                    min_similarity = min(similarities)\n",
        "                    max_similarity = max(similarities)\n",
        "                    \n",
        "                    results.append({\n",
        "                        'Method': method,\n",
        "                        'Average Similarity': avg_similarity,\n",
        "                        'Std Deviation': std_similarity,\n",
        "                        'Min Similarity': min_similarity,\n",
        "                        'Max Similarity': max_similarity,\n",
        "                        'Computation Time (s)': comp_time,\n",
        "                        'Time per Comparison (ms)': (comp_time / len(similarities)) * 1000,\n",
        "                        'Status': 'Success'\n",
        "                    })\n",
        "                    \n",
        "                    print(f\"  ✓ Completed - Avg: {avg_similarity:.3f}, Time: {comp_time:.2f}s\")\n",
        "                else:\n",
        "                    results.append({\n",
        "                        'Method': method,\n",
        "                        'Average Similarity': 0.0,\n",
        "                        'Std Deviation': 0.0,\n",
        "                        'Min Similarity': 0.0,\n",
        "                        'Max Similarity': 0.0,\n",
        "                        'Computation Time (s)': comp_time,\n",
        "                        'Time per Comparison (ms)': 0.0,\n",
        "                        'Status': 'Failed - No similarities calculated'\n",
        "                    })\n",
        "                    print(f\"  ✗ Failed - No similarities calculated\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    'Method': method,\n",
        "                    'Average Similarity': 0.0,\n",
        "                    'Std Deviation': 0.0,\n",
        "                    'Min Similarity': 0.0,\n",
        "                    'Max Similarity': 0.0,\n",
        "                    'Computation Time (s)': 0.0,\n",
        "                    'Time per Comparison (ms)': 0.0,\n",
        "                    'Status': f'Error: {str(e)[:50]}...'\n",
        "                })\n",
        "                print(f\"  ✗ Error: {str(e)}\")\n",
        "        \n",
        "        return results, sample_images\n",
        "    \n",
        "    def display_comparison_results(results):\n",
        "        \"\"\"Display comparison results in a formatted table\"\"\"\n",
        "        df = pd.DataFrame(results)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"SIMILARITY METHODS COMPARISON RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Create a more readable display\n",
        "        print(\"\\nSimilarity Statistics:\")\n",
        "        print(\"-\" * 70)\n",
        "        similarity_cols = ['Method', 'Average Similarity', 'Std Deviation', 'Min Similarity', 'Max Similarity']\n",
        "        print(df[similarity_cols].to_string(index=False, float_format='%.4f'))\n",
        "        \n",
        "        print(\"\\nPerformance Statistics:\")\n",
        "        print(\"-\" * 70)\n",
        "        performance_cols = ['Method', 'Computation Time (s)', 'Time per Comparison (ms)', 'Status']\n",
        "        perf_df = df[performance_cols].copy()\n",
        "        perf_df['Computation Time (s)'] = perf_df['Computation Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "        perf_df['Time per Comparison (ms)'] = perf_df['Time per Comparison (ms)'].apply(lambda x: f\"{x:.2f}\")\n",
        "        print(perf_df.to_string(index=False))\n",
        "        \n",
        "        print(\"\\nMethod Rankings:\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        # Rank by average similarity (diversity perspective - lower is better for diverse selection)\n",
        "        successful_results = df[df['Status'] == 'Success'].copy()\n",
        "        \n",
        "        if len(successful_results) > 0:\n",
        "            # Rank by speed (faster is better)\n",
        "            successful_results['Speed Rank'] = successful_results['Computation Time (s)'].rank()\n",
        "            \n",
        "            # Rank by diversity (lower average similarity is better for diverse selection)\n",
        "            successful_results['Diversity Rank'] = successful_results['Average Similarity'].rank()\n",
        "            \n",
        "            # Combined rank (lower is better)\n",
        "            successful_results['Combined Rank'] = (successful_results['Speed Rank'] + \n",
        "                                                 successful_results['Diversity Rank']) / 2\n",
        "            \n",
        "            ranking_cols = ['Method', 'Speed Rank', 'Diversity Rank', 'Combined Rank']\n",
        "            ranking_df = successful_results[ranking_cols].sort_values('Combined Rank')\n",
        "            print(ranking_df.to_string(index=False, float_format='%.1f'))\n",
        "            \n",
        "            print(f\"\\nBest Overall Method: {ranking_df.iloc[0]['Method']}\")\n",
        "            print(f\"Fastest Method: {successful_results.loc[successful_results['Speed Rank'].idxmin(), 'Method']}\")\n",
        "            print(f\"Most Diverse Selection: {successful_results.loc[successful_results['Diversity Rank'].idxmin(), 'Method']}\")\n",
        "        \n",
        "        print(\"\\nInterpretation Guide:\")\n",
        "        print(\"-\" * 70)\n",
        "        print(\"• Lower Average Similarity = Better for diverse image selection\")\n",
        "        print(\"• Higher Std Deviation = More varied similarity scores\")\n",
        "        print(\"• Faster computation = Better for large datasets\")\n",
        "        print(\"• Combined Rank considers both speed and diversity\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def save_comparison_results(results, sample_images):\n",
        "        \"\"\"Save comparison results to file\"\"\"\n",
        "        try:\n",
        "            # Save detailed results\n",
        "            results_df = pd.DataFrame(results)\n",
        "            results_file = os.path.join(DATA_DIR, 'similarity_methods_comparison.csv')\n",
        "            results_df.to_csv(results_file, index=False)\n",
        "            \n",
        "            # Save sample images list\n",
        "            sample_info = {\n",
        "                'sample_images': sample_images,\n",
        "                'sample_size': len(sample_images),\n",
        "                'comparison_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'methods_tested': [r['Method'] for r in results]\n",
        "            }\n",
        "            \n",
        "            sample_file = os.path.join(DATA_DIR, 'similarity_comparison_sample_info.json')\n",
        "            with open(sample_file, 'w') as f:\n",
        "                json.dump(sample_info, f, indent=2)\n",
        "            \n",
        "            print(f\"\\nResults saved to:\")\n",
        "            print(f\"  - {results_file}\")\n",
        "            print(f\"  - {sample_file}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results: {str(e)}\")\n",
        "    \n",
        "    # Run the comparison\n",
        "    try:\n",
        "        results, sample_images = run_methods_comparison()\n",
        "        \n",
        "        if results is None:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"COMPARISON CANCELLED - INSUFFICIENT DATA\")\n",
        "            print(\"=\"*70)\n",
        "            print(\"Possible solutions:\")\n",
        "            print(\"1. Make sure you have run the data processing steps first\")\n",
        "            print(\"2. Ensure images have been downloaded and selected\")\n",
        "            print(\"3. Check that processor.movie_features contains data\")\n",
        "            print(\"4. Verify that image files exist on disk\")\n",
        "        else:\n",
        "            comparison_df = display_comparison_results(results)\n",
        "            save_comparison_results(results, sample_images)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Comparison failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"Similarity methods comparison is disabled.\")\n",
        "    print(\"To enable comparison, set RUN_COMPARISON = True\")\n",
        "    print(\"\\nNote: Comparison will test multiple similarity methods on a sample of images\")\n",
        "    print(\"and may take several minutes depending on the methods and sample size.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT Image Feature Extraction\n",
            "==================================================\n",
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT model loaded successfully: google/vit-base-patch16-224\n",
            "\n",
            "Starting ViT feature extraction for 25 movies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:   4%|▍         | 1/25 [00:00<00:10,  2.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 1. Toy Story: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:   8%|▊         | 2/25 [00:00<00:08,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 2. GoldenEye: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  12%|█▏        | 3/25 [00:01<00:08,  2.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 3. Four Rooms: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  16%|█▌        | 4/25 [00:01<00:07,  2.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 4. Get Shorty: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  20%|██        | 5/25 [00:01<00:07,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 5. Copycat: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  24%|██▍       | 6/25 [00:02<00:06,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 6. Shanghai Triad (Yao a yao yao dao waipo qiao): feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  28%|██▊       | 7/25 [00:02<00:06,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 7. Twelve Monkeys: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  32%|███▏      | 8/25 [00:02<00:06,  2.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 8. Babe: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  36%|███▌      | 9/25 [00:03<00:05,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 9. Dead Man Walking: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  40%|████      | 10/25 [00:03<00:05,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 10. Richard III: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  44%|████▍     | 11/25 [00:04<00:05,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 11. Seven (Se7en): feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  48%|████▊     | 12/25 [00:04<00:04,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 12. Usual Suspects, The: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  52%|█████▏    | 13/25 [00:04<00:04,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 13. Mighty Aphrodite: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  56%|█████▌    | 14/25 [00:05<00:03,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 14. Postino, Il: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  60%|██████    | 15/25 [00:05<00:03,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 15. Mr. Holland's Opus: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  64%|██████▍   | 16/25 [00:05<00:03,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 16. French Twist (Gazon maudit): feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  68%|██████▊   | 17/25 [00:06<00:02,  2.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 17. From Dusk Till Dawn: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  72%|███████▏  | 18/25 [00:06<00:02,  3.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 18. White Balloon, The: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  76%|███████▌  | 19/25 [00:06<00:02,  2.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 19. Antonia's Line: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  80%|████████  | 20/25 [00:07<00:01,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 20. Angels and Insects: feature dimension 768\n",
            "   SUCCESS: 21. Muppet Treasure Island: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  88%|████████▊ | 22/25 [00:07<00:00,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 22. Braveheart: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  92%|█████████▏| 23/25 [00:07<00:00,  3.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 23. Taxi Driver: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  96%|█████████▌| 24/25 [00:08<00:00,  3.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 24. Rumble in the Bronx: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features: 100%|██████████| 25/25 [00:08<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 25. Birdcage, The: feature dimension 768\n",
            "\n",
            "ViT feature extraction statistics:\n",
            "   Successfully processed movies: 25\n",
            "   Feature dimension: 768\n",
            "   Feature matrix shape: (25, 768)\n",
            "   Feature matrix saved: multimodal_data\\vit_features_matrix.npy\n",
            "   Mapping file saved: multimodal_data\\vit_movie_mapping.json\n",
            "   Updated feature data saved: multimodal_data\\movie_features_with_vit.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ViT image feature extraction\n",
        "print(\"ViT Image Feature Extraction\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class ViTFeatureExtractor:\n",
        "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Load ViT model and processor\n",
        "            self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "            self.model = ViTModel.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"ViT model loaded successfully: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: ViT model loading failed: {str(e)}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def extract_image_features(self, image_path):\n",
        "        \"\"\"Extract ViT features from a single image\"\"\"\n",
        "        if self.model is None:\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use [CLS] token features as image representation\n",
        "                image_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
        "            \n",
        "            return image_features\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Feature extraction failed ({image_path}): {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_movie_features(self, image_paths):\n",
        "        \"\"\"Extract and fuse features from all movie images\"\"\"\n",
        "        if not image_paths:\n",
        "            return None\n",
        "        \n",
        "        features_list = []\n",
        "        \n",
        "        for image_path in image_paths:\n",
        "            if os.path.exists(image_path):\n",
        "                features = self.extract_image_features(image_path)\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "        \n",
        "        if not features_list:\n",
        "            return None\n",
        "        \n",
        "        # Fuse multiple image features (average pooling)\n",
        "        combined_features = np.mean(features_list, axis=0)\n",
        "        return combined_features\n",
        "\n",
        "# Initialize ViT feature extractor\n",
        "vit_extractor = ViTFeatureExtractor()\n",
        "\n",
        "# Extract ViT features for all movies\n",
        "if 'all_movie_features' in locals() and all_movie_features and vit_extractor.model is not None:\n",
        "    print(f\"\\nStarting ViT feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    vit_features_matrix = []\n",
        "    vit_movie_ids = []\n",
        "    success_count = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Extracting ViT features\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        \n",
        "        # Use selected images\n",
        "        selected_paths = movie_features.get('selected_image_paths', [])\n",
        "        \n",
        "        if not selected_paths:\n",
        "            print(f\"   WARNING: {movie_id}. {title}: No selected images\")\n",
        "            continue\n",
        "        \n",
        "        # Extract features\n",
        "        vit_features = vit_extractor.extract_movie_features(selected_paths)\n",
        "        \n",
        "        if vit_features is not None:\n",
        "            vit_features_matrix.append(vit_features)\n",
        "            vit_movie_ids.append(movie_id)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            movie_features['vit_features'] = vit_features.tolist()\n",
        "            movie_features['vit_feature_dim'] = len(vit_features)\n",
        "            \n",
        "            success_count += 1\n",
        "            print(f\"   SUCCESS: {movie_id}. {title}: feature dimension {len(vit_features)}\")\n",
        "        else:\n",
        "            print(f\"   ERROR: {movie_id}. {title}: ViT feature extraction failed\")\n",
        "    \n",
        "    # Convert to numpy array and save\n",
        "    if vit_features_matrix:\n",
        "        vit_features_array = np.array(vit_features_matrix)\n",
        "        \n",
        "        # Save ViT feature matrix\n",
        "        vit_features_file = os.path.join(DATA_DIR, 'vit_features_matrix.npy')\n",
        "        np.save(vit_features_file, vit_features_array)\n",
        "        \n",
        "        # Save movie ID mapping\n",
        "        vit_mapping_file = os.path.join(DATA_DIR, 'vit_movie_mapping.json')\n",
        "        with open(vit_mapping_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({'movie_ids': vit_movie_ids, 'feature_dim': len(vit_features_matrix[0])}, f)\n",
        "        \n",
        "        print(f\"\\nViT feature extraction statistics:\")\n",
        "        print(f\"   Successfully processed movies: {success_count}\")\n",
        "        print(f\"   Feature dimension: {vit_features_array.shape[1]}\")\n",
        "        print(f\"   Feature matrix shape: {vit_features_array.shape}\")\n",
        "        print(f\"   Feature matrix saved: {vit_features_file}\")\n",
        "        print(f\"   Mapping file saved: {vit_mapping_file}\")\n",
        "        \n",
        "        # Save updated movie features\n",
        "        updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_vit.json')\n",
        "        with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"   Updated feature data saved: {updated_features_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: No ViT features extracted successfully\")\n",
        "else:\n",
        "    print(\"ERROR: ViT model not loaded or no movie data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT Text Feature Extraction\n",
            "==================================================\n",
            "Using device: cpu\n",
            "BERT model loaded successfully: bert-base-uncased\n",
            "\n",
            "Starting BERT text feature extraction for 25 movies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:   0%|          | 0/25 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processing 1. Toy Story\n",
            "     Overview length: 303 characters\n",
            "     Tagline length: 47 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:   4%|▍         | 1/25 [00:00<00:07,  3.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 2. GoldenEye\n",
            "     Overview length: 371 characters\n",
            "     Tagline length: 36 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:   8%|▊         | 2/25 [00:00<00:06,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 3. Four Rooms\n",
            "     Overview length: 237 characters\n",
            "     Tagline length: 155 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  12%|█▏        | 3/25 [00:00<00:05,  3.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 4. Get Shorty\n",
            "     Overview length: 367 characters\n",
            "     Tagline length: 22 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  16%|█▌        | 4/25 [00:01<00:05,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 5. Copycat\n",
            "     Overview length: 139 characters\n",
            "     Tagline length: 142 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  20%|██        | 5/25 [00:01<00:04,  4.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 6. Shanghai Triad (Yao a yao yao dao waipo qiao)\n",
            "     Overview length: 271 characters\n",
            "     Tagline length: 68 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  24%|██▍       | 6/25 [00:01<00:04,  4.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 7. Twelve Monkeys\n",
            "     Overview length: 536 characters\n",
            "     Tagline length: 22 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  28%|██▊       | 7/25 [00:01<00:04,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 8. Babe\n",
            "     Overview length: 383 characters\n",
            "     Tagline length: 29 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  36%|███▌      | 9/25 [00:02<00:03,  5.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 9. Dead Man Walking\n",
            "     Overview length: 147 characters\n",
            "     Tagline length: 0 characters\n",
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 10. Richard III\n",
            "     Overview length: 442 characters\n",
            "     Tagline length: 37 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  40%|████      | 10/25 [00:02<00:03,  4.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 11. Seven (Se7en)\n",
            "     Overview length: 389 characters\n",
            "     Tagline length: 37 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  44%|████▍     | 11/25 [00:02<00:02,  4.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 12. Usual Suspects, The\n",
            "     Overview length: 409 characters\n",
            "     Tagline length: 44 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  48%|████▊     | 12/25 [00:02<00:02,  4.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 13. Mighty Aphrodite\n",
            "     Overview length: 419 characters\n",
            "     Tagline length: 75 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  52%|█████▏    | 13/25 [00:02<00:02,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 14. Postino, Il\n",
            "     Overview length: 127 characters\n",
            "     Tagline length: 20 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  56%|█████▌    | 14/25 [00:03<00:02,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 15. Mr. Holland's Opus\n",
            "     Overview length: 340 characters\n",
            "     Tagline length: 71 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  60%|██████    | 15/25 [00:03<00:02,  4.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 16. French Twist (Gazon maudit)\n",
            "     Overview length: 157 characters\n",
            "     Tagline length: 58 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  64%|██████▍   | 16/25 [00:03<00:02,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 17. From Dusk Till Dawn\n",
            "     Overview length: 163 characters\n",
            "     Tagline length: 94 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  72%|███████▏  | 18/25 [00:04<00:01,  5.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 18. White Balloon, The\n",
            "     Overview length: 125 characters\n",
            "     Tagline length: 0 characters\n",
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 19. Antonia's Line\n",
            "     Overview length: 424 characters\n",
            "     Tagline length: 64 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  76%|███████▌  | 19/25 [00:04<00:01,  4.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 20. Angels and Insects\n",
            "     Overview length: 438 characters\n",
            "     Tagline length: 65 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  80%|████████  | 20/25 [00:04<00:01,  4.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 21. Muppet Treasure Island\n",
            "     Overview length: 397 characters\n",
            "     Tagline length: 27 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  84%|████████▍ | 21/25 [00:04<00:00,  4.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 22. Braveheart\n",
            "     Overview length: 258 characters\n",
            "     Tagline length: 43 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  88%|████████▊ | 22/25 [00:04<00:00,  4.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 23. Taxi Driver\n",
            "     Overview length: 165 characters\n",
            "     Tagline length: 143 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  92%|█████████▏| 23/25 [00:05<00:00,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 24. Rumble in the Bronx\n",
            "     Overview length: 397 characters\n",
            "     Tagline length: 31 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  96%|█████████▌| 24/25 [00:05<00:00,  4.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 25. Birdcage, The\n",
            "     Overview length: 567 characters\n",
            "     Tagline length: 16 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features: 100%|██████████| 25/25 [00:05<00:00,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "\n",
            "BERT feature extraction statistics:\n",
            "   Successfully processed movies: 25\n",
            "   Feature dimension: 768\n",
            "   Feature matrix shape: (25, 768)\n",
            "   Feature matrix saved: multimodal_data\\bert_features_matrix.npy\n",
            "   Mapping file saved: multimodal_data\\bert_movie_mapping.json\n",
            "   Updated feature data saved: multimodal_data\\movie_features_with_bert.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# BERT text feature extraction\n",
        "print(\"BERT Text Feature Extraction\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class BERTFeatureExtractor:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Load BERT model and tokenizer\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "            self.model = BertModel.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"BERT model loaded successfully: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: BERT model loading failed: {str(e)}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def extract_text_features(self, text, max_length=512):\n",
        "        \"\"\"Extract BERT features from text\"\"\"\n",
        "        if self.model is None or not text or text.strip() == \"\":\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Text preprocessing and tokenization\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            \n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use [CLS] token features as sentence representation\n",
        "                text_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
        "            \n",
        "            return text_features\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Text feature extraction failed: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_movie_text_features(self, overview, tagline):\n",
        "        \"\"\"Extract movie text features (overview + tagline)\"\"\"\n",
        "        features_list = []\n",
        "        \n",
        "        # Extract overview features\n",
        "        if overview and overview.strip():\n",
        "            overview_features = self.extract_text_features(overview)\n",
        "            if overview_features is not None:\n",
        "                features_list.append(overview_features)\n",
        "        \n",
        "        # Extract tagline features\n",
        "        if tagline and tagline.strip():\n",
        "            tagline_features = self.extract_text_features(tagline)\n",
        "            if tagline_features is not None:\n",
        "                features_list.append(tagline_features)\n",
        "        \n",
        "        if not features_list:\n",
        "            return None\n",
        "        \n",
        "        # Average pooling to fuse features\n",
        "        combined_features = np.mean(features_list, axis=0)\n",
        "        return combined_features\n",
        "\n",
        "# Initialize BERT feature extractor\n",
        "bert_extractor = BERTFeatureExtractor()\n",
        "\n",
        "# Extract BERT text features for all movies\n",
        "if 'all_movie_features' in locals() and all_movie_features and bert_extractor.model is not None:\n",
        "    print(f\"\\nStarting BERT text feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    bert_features_matrix = []\n",
        "    bert_movie_ids = []\n",
        "    success_count = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Extracting BERT features\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        overview = movie_features.get('overview', '')\n",
        "        tagline = movie_features.get('tagline', '')\n",
        "        \n",
        "        print(f\"   Processing {movie_id}. {title}\")\n",
        "        print(f\"     Overview length: {len(overview) if overview else 0} characters\")\n",
        "        print(f\"     Tagline length: {len(tagline) if tagline else 0} characters\")\n",
        "        \n",
        "        # Extract text features\n",
        "        bert_features = bert_extractor.extract_movie_text_features(overview, tagline)\n",
        "        \n",
        "        if bert_features is not None:\n",
        "            bert_features_matrix.append(bert_features)\n",
        "            bert_movie_ids.append(movie_id)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            movie_features['bert_features'] = bert_features.tolist()\n",
        "            movie_features['bert_feature_dim'] = len(bert_features)\n",
        "            movie_features['text_length'] = len(overview) + len(tagline)\n",
        "            \n",
        "            success_count += 1\n",
        "            print(f\"     SUCCESS: BERT feature dimension: {len(bert_features)}\")\n",
        "        else:\n",
        "            print(f\"     ERROR: BERT feature extraction failed\")\n",
        "    \n",
        "    # Convert to numpy array and save\n",
        "    if bert_features_matrix:\n",
        "        bert_features_array = np.array(bert_features_matrix)\n",
        "        \n",
        "        # Save BERT feature matrix\n",
        "        bert_features_file = os.path.join(DATA_DIR, 'bert_features_matrix.npy')\n",
        "        np.save(bert_features_file, bert_features_array)\n",
        "        \n",
        "        # Save movie ID mapping\n",
        "        bert_mapping_file = os.path.join(DATA_DIR, 'bert_movie_mapping.json')\n",
        "        with open(bert_mapping_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({'movie_ids': bert_movie_ids, 'feature_dim': len(bert_features_matrix[0])}, f)\n",
        "        \n",
        "        print(f\"\\nBERT feature extraction statistics:\")\n",
        "        print(f\"   Successfully processed movies: {success_count}\")\n",
        "        print(f\"   Feature dimension: {bert_features_array.shape[1]}\")\n",
        "        print(f\"   Feature matrix shape: {bert_features_array.shape}\")\n",
        "        print(f\"   Feature matrix saved: {bert_features_file}\")\n",
        "        print(f\"   Mapping file saved: {bert_mapping_file}\")\n",
        "        \n",
        "        # Save updated movie features\n",
        "        updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_bert.json')\n",
        "        with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"   Updated feature data saved: {updated_features_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: No BERT features extracted successfully\")\n",
        "else:\n",
        "    print(\"ERROR: BERT model not loaded or no movie data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cast & Crew Statistical Feature Engineering\n",
            "==================================================\n",
            "Starting cast & crew feature extraction for 25 movies\n",
            "Cast & crew statistics:\n",
            "   Total directors: 24 people\n",
            "   Total cast: 232 people\n",
            "   Selected high-frequency directors: 24 people\n",
            "   Selected high-frequency cast: 50 people\n",
            "\n",
            "TOP 10 high-frequency directors:\n",
            "    1. Martin Campbell: 2 movies\n",
            "    2. John Lasseter: 1 movies\n",
            "    3. Barry Sonnenfeld: 1 movies\n",
            "    4. Jon Amiel: 1 movies\n",
            "    5. Zhang Yimou: 1 movies\n",
            "    6. Terry Gilliam: 1 movies\n",
            "    7. Chris Noonan: 1 movies\n",
            "    8. Tim Robbins: 1 movies\n",
            "    9. Richard Loncraine: 1 movies\n",
            "   10. David Fincher: 1 movies\n",
            "\n",
            "TOP 10 high-frequency cast:\n",
            "    1. Pierce Brosnan: 2 movies\n",
            "    2. Sean Bean: 2 movies\n",
            "    3. Izabella Scorupco: 2 movies\n",
            "    4. Famke Janssen: 2 movies\n",
            "    5. Joe Don Baker: 2 movies\n",
            "    6. Judi Dench: 2 movies\n",
            "    7. Robbie Coltrane: 2 movies\n",
            "    8. Tchéky Karyo: 2 movies\n",
            "    9. Gottfried John: 2 movies\n",
            "   10. Alan Cumming: 2 movies\n",
            "\n",
            "Cast & crew feature statistics:\n",
            "   Movies processed: 25\n",
            "   Director feature dimension: 24\n",
            "   Cast feature dimension: 50\n",
            "   Total feature dimension: 74\n",
            "   Feature matrix shape: (25, 74)\n",
            "   Feature matrix saved: multimodal_data\\cast_crew_features.npy\n",
            "   Mapping file saved: multimodal_data\\cast_crew_mapping.json\n",
            "   Feature sparsity: 95.08%\n",
            "   Average high-frequency cast & crew per movie: 3.6\n",
            "   Complete feature data saved: multimodal_data\\movie_features_complete.json\n"
          ]
        }
      ],
      "source": [
        "# Cast & crew statistical feature engineering\n",
        "print(\"Cast & Crew Statistical Feature Engineering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def extract_cast_crew_features(all_movie_features, top_n=50):\n",
        "    \"\"\"Extract cast & crew statistical features\"\"\"\n",
        "    if not all_movie_features:\n",
        "        return None, None, None\n",
        "    \n",
        "    # Count frequency of all directors and cast\n",
        "    all_directors = []\n",
        "    all_cast = []\n",
        "    \n",
        "    for movie in all_movie_features:\n",
        "        # Directors\n",
        "        directors = movie.get('directors', '')\n",
        "        if directors:\n",
        "            all_directors.extend([d.strip() for d in directors.split('|') if d.strip()])\n",
        "        \n",
        "        # Cast\n",
        "        cast = movie.get('cast', '')\n",
        "        if cast:\n",
        "            all_cast.extend([c.strip() for c in cast.split('|') if c.strip()])\n",
        "    \n",
        "    # Count frequencies\n",
        "    director_counts = Counter(all_directors)\n",
        "    cast_counts = Counter(all_cast)\n",
        "    \n",
        "    # Get high-frequency cast & crew\n",
        "    top_directors = [director for director, count in director_counts.most_common(top_n)]\n",
        "    top_cast = [actor for actor, count in cast_counts.most_common(top_n)]\n",
        "    \n",
        "    print(f\"Cast & crew statistics:\")\n",
        "    print(f\"   Total directors: {len(director_counts)} people\")\n",
        "    print(f\"   Total cast: {len(cast_counts)} people\")\n",
        "    print(f\"   Selected high-frequency directors: {len(top_directors)} people\")\n",
        "    print(f\"   Selected high-frequency cast: {len(top_cast)} people\")\n",
        "    \n",
        "    # Display TOP 10 directors and cast\n",
        "    print(f\"\\nTOP 10 high-frequency directors:\")\n",
        "    for i, (director, count) in enumerate(director_counts.most_common(10), 1):\n",
        "        print(f\"   {i:2d}. {director}: {count} movies\")\n",
        "    \n",
        "    print(f\"\\nTOP 10 high-frequency cast:\")\n",
        "    for i, (actor, count) in enumerate(cast_counts.most_common(10), 1):\n",
        "        print(f\"   {i:2d}. {actor}: {count} movies\")\n",
        "    \n",
        "    return top_directors, top_cast, (director_counts, cast_counts)\n",
        "\n",
        "def create_cast_crew_features(all_movie_features, top_directors, top_cast):\n",
        "    \"\"\"Create cast & crew feature vectors for each movie\"\"\"\n",
        "    if not all_movie_features or not top_directors or not top_cast:\n",
        "        return None, None\n",
        "    \n",
        "    cast_crew_matrix = []\n",
        "    movie_ids = []\n",
        "    \n",
        "    for movie in all_movie_features:\n",
        "        movie_id = movie['movielens_id']\n",
        "        \n",
        "        # Director features (one-hot encoding)\n",
        "        director_features = np.zeros(len(top_directors))\n",
        "        directors = movie.get('directors', '')\n",
        "        if directors:\n",
        "            movie_directors = [d.strip() for d in directors.split('|') if d.strip()]\n",
        "            for i, top_director in enumerate(top_directors):\n",
        "                if top_director in movie_directors:\n",
        "                    director_features[i] = 1\n",
        "        \n",
        "        # Cast features (one-hot encoding)\n",
        "        cast_features = np.zeros(len(top_cast))\n",
        "        cast = movie.get('cast', '')\n",
        "        if cast:\n",
        "            movie_cast = [c.strip() for c in cast.split('|') if c.strip()]\n",
        "            for i, top_actor in enumerate(top_cast):\n",
        "                if top_actor in movie_cast:\n",
        "                    cast_features[i] = 1\n",
        "        \n",
        "        # Combine features\n",
        "        combined_features = np.concatenate([director_features, cast_features])\n",
        "        cast_crew_matrix.append(combined_features)\n",
        "        movie_ids.append(movie_id)\n",
        "    \n",
        "    return np.array(cast_crew_matrix), movie_ids\n",
        "\n",
        "# Execute cast & crew feature engineering\n",
        "if 'all_movie_features' in locals() and all_movie_features:\n",
        "    print(f\"Starting cast & crew feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    # Extract high-frequency cast & crew\n",
        "    top_directors, top_cast, (director_counts, cast_counts) = extract_cast_crew_features(all_movie_features)\n",
        "    \n",
        "    if top_directors and top_cast:\n",
        "        # Create cast & crew feature matrix\n",
        "        cast_crew_matrix, cc_movie_ids = create_cast_crew_features(\n",
        "            all_movie_features, top_directors, top_cast\n",
        "        )\n",
        "        \n",
        "        if cast_crew_matrix is not None:\n",
        "            # Save cast & crew features\n",
        "            cast_crew_file = os.path.join(DATA_DIR, 'cast_crew_features.npy')\n",
        "            np.save(cast_crew_file, cast_crew_matrix)\n",
        "            \n",
        "            # Save cast & crew mapping information\n",
        "            cast_crew_mapping = {\n",
        "                'movie_ids': cc_movie_ids,\n",
        "                'top_directors': top_directors,\n",
        "                'top_cast': top_cast,\n",
        "                'director_feature_dim': len(top_directors),\n",
        "                'cast_feature_dim': len(top_cast),\n",
        "                'total_feature_dim': len(top_directors) + len(top_cast)\n",
        "            }\n",
        "            \n",
        "            mapping_file = os.path.join(DATA_DIR, 'cast_crew_mapping.json')\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(cast_crew_mapping, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            for i, movie in enumerate(all_movie_features):\n",
        "                if movie['movielens_id'] in cc_movie_ids:\n",
        "                    idx = cc_movie_ids.index(movie['movielens_id'])\n",
        "                    movie['cast_crew_features'] = cast_crew_matrix[idx].tolist()\n",
        "                    movie['cast_crew_feature_dim'] = len(cast_crew_matrix[idx])\n",
        "            \n",
        "            print(f\"\\nCast & crew feature statistics:\")\n",
        "            print(f\"   Movies processed: {len(cc_movie_ids)}\")\n",
        "            print(f\"   Director feature dimension: {len(top_directors)}\")\n",
        "            print(f\"   Cast feature dimension: {len(top_cast)}\")\n",
        "            print(f\"   Total feature dimension: {cast_crew_matrix.shape[1]}\")\n",
        "            print(f\"   Feature matrix shape: {cast_crew_matrix.shape}\")\n",
        "            print(f\"   Feature matrix saved: {cast_crew_file}\")\n",
        "            print(f\"   Mapping file saved: {mapping_file}\")\n",
        "            \n",
        "            # Analyze feature sparsity\n",
        "            sparsity = 1 - np.count_nonzero(cast_crew_matrix) / cast_crew_matrix.size\n",
        "            print(f\"   Feature sparsity: {sparsity*100:.2f}%\")\n",
        "            print(f\"   Average high-frequency cast & crew per movie: {np.mean(np.sum(cast_crew_matrix, axis=1)):.1f}\")\n",
        "            \n",
        "            # Save final movie feature data\n",
        "            final_features_file = os.path.join(DATA_DIR, 'movie_features_complete.json')\n",
        "            with open(final_features_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"   Complete feature data saved: {final_features_file}\")\n",
        "        else:\n",
        "            print(\"ERROR: Cast & crew feature matrix creation failed\")\n",
        "    else:\n",
        "        print(\"ERROR: High-frequency cast & crew extraction failed\")\n",
        "else:\n",
        "    print(\"ERROR: No movie feature data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multimodal Feature Fusion\n",
            "==================================================\n",
            "Starting multimodal feature fusion\n",
            "ViT features loaded: (25, 768) (standardized)\n",
            "BERT features loaded: (25, 768) (standardized)\n",
            "Cast & crew features loaded: (25, 74) (one-hot encoded)\n",
            "\n",
            "Using concatenate method for feature fusion\n",
            "Feature intersection statistics:\n",
            "   vit: 25 movies\n",
            "   bert: 25 movies\n",
            "   cast_crew: 25 movies\n",
            "   Intersection: 25 movies\n",
            "   vit: (25, 768)\n",
            "   bert: (25, 768)\n",
            "   cast_crew: (25, 74)\n",
            "\n",
            "Concatenation fusion result: (25, 1610)\n",
            "   concatenate fused features saved: multimodal_data\\multimodal_features_concatenate.npy\n",
            "   Mapping file saved: multimodal_data\\multimodal_mapping_concatenate.json\n",
            "\n",
            "Using weighted_average method for feature fusion\n",
            "Feature intersection statistics:\n",
            "   vit: 25 movies\n",
            "   bert: 25 movies\n",
            "   cast_crew: 25 movies\n",
            "   Intersection: 25 movies\n",
            "   vit: (25, 768)\n",
            "   bert: (25, 768)\n",
            "   cast_crew: (25, 74)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "n_components=74 must be between 0 and min(n_samples, n_features)=25 with svd_solver='full'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m fusion_methods:\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m method for feature fusion\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     multimodal_features, movie_ids = \u001b[43mcreate_multimodal_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multimodal_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m         multimodal_results[method] = {\n\u001b[32m    164\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: multimodal_features,\n\u001b[32m    165\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mmovie_ids\u001b[39m\u001b[33m'\u001b[39m: movie_ids\n\u001b[32m    166\u001b[39m         }\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mcreate_multimodal_features\u001b[39m\u001b[34m(features_data, fusion_method)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature.shape[\u001b[32m1\u001b[39m] > target_dim:\n\u001b[32m    130\u001b[39m     pca = PCA(n_components=target_dim)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     reduced_feature = \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     reduced_features.append(reduced_feature)\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduced_feature.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (PCA)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:466\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    445\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:540\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcovariance_eigh\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_truncated(X, n_components, xp)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:554\u001b[39m, in \u001b[36mPCA._fit_full\u001b[39m\u001b[34m(self, X, n_components, xp, is_array_api_compliant)\u001b[39m\n\u001b[32m    550\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    551\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmle\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is only supported if n_samples >= n_features\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    552\u001b[39m         )\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= n_components <= \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    555\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be between 0 and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_ = xp.mean(X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: n_components=74 must be between 0 and min(n_samples, n_features)=25 with svd_solver='full'"
          ]
        }
      ],
      "source": [
        "# Multimodal feature fusion\n",
        "print(\"Multimodal Feature Fusion\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def load_and_normalize_features():\n",
        "    \"\"\"Load and normalize all features\"\"\"\n",
        "    features_data = {}\n",
        "    \n",
        "    # 1. Load ViT image features\n",
        "    vit_file = os.path.join(DATA_DIR, 'vit_features_matrix.npy')\n",
        "    vit_mapping_file = os.path.join(DATA_DIR, 'vit_movie_mapping.json')\n",
        "    \n",
        "    if os.path.exists(vit_file) and os.path.exists(vit_mapping_file):\n",
        "        vit_features = np.load(vit_file)\n",
        "        with open(vit_mapping_file, 'r') as f:\n",
        "            vit_mapping = json.load(f)\n",
        "        \n",
        "        # Standardize ViT features\n",
        "        scaler_vit = StandardScaler()\n",
        "        vit_features_normalized = scaler_vit.fit_transform(vit_features)\n",
        "        \n",
        "        features_data['vit'] = {\n",
        "            'features': vit_features_normalized,\n",
        "            'movie_ids': vit_mapping['movie_ids'],\n",
        "            'dim': vit_features_normalized.shape[1],\n",
        "            'scaler': scaler_vit\n",
        "        }\n",
        "        print(f\"ViT features loaded: {vit_features_normalized.shape} (standardized)\")\n",
        "    else:\n",
        "        print(\"WARNING: ViT feature files not found\")\n",
        "    \n",
        "    # 2. Load BERT text features\n",
        "    bert_file = os.path.join(DATA_DIR, 'bert_features_matrix.npy')\n",
        "    bert_mapping_file = os.path.join(DATA_DIR, 'bert_movie_mapping.json')\n",
        "    \n",
        "    if os.path.exists(bert_file) and os.path.exists(bert_mapping_file):\n",
        "        bert_features = np.load(bert_file)\n",
        "        with open(bert_mapping_file, 'r') as f:\n",
        "            bert_mapping = json.load(f)\n",
        "        \n",
        "        # Standardize BERT features\n",
        "        scaler_bert = StandardScaler()\n",
        "        bert_features_normalized = scaler_bert.fit_transform(bert_features)\n",
        "        \n",
        "        features_data['bert'] = {\n",
        "            'features': bert_features_normalized,\n",
        "            'movie_ids': bert_mapping['movie_ids'],\n",
        "            'dim': bert_features_normalized.shape[1],\n",
        "            'scaler': scaler_bert\n",
        "        }\n",
        "        print(f\"BERT features loaded: {bert_features_normalized.shape} (standardized)\")\n",
        "    else:\n",
        "        print(\"WARNING: BERT feature files not found\")\n",
        "    \n",
        "    # 3. Load cast & crew features\n",
        "    cast_crew_file = os.path.join(DATA_DIR, 'cast_crew_features.npy')\n",
        "    cast_crew_mapping_file = os.path.join(DATA_DIR, 'cast_crew_mapping.json')\n",
        "    \n",
        "    if os.path.exists(cast_crew_file) and os.path.exists(cast_crew_mapping_file):\n",
        "        cast_crew_features = np.load(cast_crew_file)\n",
        "        with open(cast_crew_mapping_file, 'r') as f:\n",
        "            cast_crew_mapping = json.load(f)\n",
        "        \n",
        "        # Cast & crew features are already 0-1 encoded, no standardization needed\n",
        "        features_data['cast_crew'] = {\n",
        "            'features': cast_crew_features,\n",
        "            'movie_ids': cast_crew_mapping['movie_ids'],\n",
        "            'dim': cast_crew_features.shape[1],\n",
        "            'directors_dim': cast_crew_mapping['director_feature_dim'],\n",
        "            'cast_dim': cast_crew_mapping['cast_feature_dim']\n",
        "        }\n",
        "        print(f\"Cast & crew features loaded: {cast_crew_features.shape} (one-hot encoded)\")\n",
        "    else:\n",
        "        print(\"WARNING: Cast & crew feature files not found\")\n",
        "    \n",
        "    return features_data\n",
        "\n",
        "def create_multimodal_features(features_data, fusion_method='concatenate'):\n",
        "    \"\"\"Create multimodal fused features\"\"\"\n",
        "    if not features_data:\n",
        "        return None, None\n",
        "    \n",
        "    # Find intersection of movie IDs from all features\n",
        "    movie_id_sets = [set(data['movie_ids']) for data in features_data.values()]\n",
        "    common_movie_ids = set.intersection(*movie_id_sets)\n",
        "    common_movie_ids = sorted(list(common_movie_ids))\n",
        "    \n",
        "    print(f\"Feature intersection statistics:\")\n",
        "    for feature_name, data in features_data.items():\n",
        "        print(f\"   {feature_name}: {len(data['movie_ids'])} movies\")\n",
        "    print(f\"   Intersection: {len(common_movie_ids)} movies\")\n",
        "    \n",
        "    if not common_movie_ids:\n",
        "        print(\"ERROR: No common movie IDs found\")\n",
        "        return None, None\n",
        "    \n",
        "    # Align feature matrices\n",
        "    aligned_features = []\n",
        "    feature_dims = []\n",
        "    feature_names = []\n",
        "    \n",
        "    for feature_name, data in features_data.items():\n",
        "        movie_ids = data['movie_ids']\n",
        "        features = data['features']\n",
        "        \n",
        "        # Create index mapping\n",
        "        id_to_idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
        "        \n",
        "        # Reorder features according to intersection IDs\n",
        "        aligned_feature = np.array([features[id_to_idx[movie_id]] for movie_id in common_movie_ids])\n",
        "        aligned_features.append(aligned_feature)\n",
        "        feature_dims.append(aligned_feature.shape[1])\n",
        "        feature_names.append(feature_name)\n",
        "        \n",
        "        print(f\"   {feature_name}: {aligned_feature.shape}\")\n",
        "    \n",
        "    # Feature fusion\n",
        "    if fusion_method == 'concatenate':\n",
        "        # Simple concatenation\n",
        "        multimodal_features = np.concatenate(aligned_features, axis=1)\n",
        "        print(f\"\\nConcatenation fusion result: {multimodal_features.shape}\")\n",
        "    \n",
        "    elif fusion_method == 'weighted_average':\n",
        "        # Weighted average (requires consistent feature dimensions, using PCA for dimensionality reduction)\n",
        "        target_dim = min(feature_dims)\n",
        "        reduced_features = []\n",
        "        \n",
        "        for i, feature in enumerate(aligned_features):\n",
        "            if feature.shape[1] > target_dim:\n",
        "                pca = PCA(n_components=target_dim)\n",
        "                reduced_feature = pca.fit_transform(feature)\n",
        "                reduced_features.append(reduced_feature)\n",
        "                print(f\"   {feature_names[i]}: {feature.shape} -> {reduced_feature.shape} (PCA)\")\n",
        "            else:\n",
        "                reduced_features.append(feature)\n",
        "        \n",
        "        # Equal weight average\n",
        "        multimodal_features = np.mean(reduced_features, axis=0)\n",
        "        print(f\"\\nWeighted average fusion result: {multimodal_features.shape}\")\n",
        "    \n",
        "    else:\n",
        "        # Default concatenation\n",
        "        multimodal_features = np.concatenate(aligned_features, axis=1)\n",
        "    \n",
        "    return multimodal_features, common_movie_ids\n",
        "\n",
        "# Execute multimodal feature fusion\n",
        "print(\"Starting multimodal feature fusion\")\n",
        "\n",
        "# Load and normalize features\n",
        "features_data = load_and_normalize_features()\n",
        "\n",
        "if features_data:\n",
        "    # Create multiple fusion versions\n",
        "    fusion_methods = ['concatenate', 'weighted_average']\n",
        "    multimodal_results = {}\n",
        "    \n",
        "    for method in fusion_methods:\n",
        "        print(f\"\\nUsing {method} method for feature fusion\")\n",
        "        multimodal_features, movie_ids = create_multimodal_features(features_data, method)\n",
        "        \n",
        "        if multimodal_features is not None:\n",
        "            multimodal_results[method] = {\n",
        "                'features': multimodal_features,\n",
        "                'movie_ids': movie_ids\n",
        "            }\n",
        "            \n",
        "            # Save fused features\n",
        "            fusion_file = os.path.join(DATA_DIR, f'multimodal_features_{method}.npy')\n",
        "            np.save(fusion_file, multimodal_features)\n",
        "            \n",
        "            # Save movie ID mapping\n",
        "            mapping_file = os.path.join(DATA_DIR, f'multimodal_mapping_{method}.json')\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'movie_ids': movie_ids,\n",
        "                    'feature_dim': multimodal_features.shape[1],\n",
        "                    'fusion_method': method,\n",
        "                    'component_dims': {name: data['dim'] for name, data in features_data.items()}\n",
        "                }, f, indent=2)\n",
        "            \n",
        "            print(f\"   {method} fused features saved: {fusion_file}\")\n",
        "            print(f\"   Mapping file saved: {mapping_file}\")\n",
        "    \n",
        "    if multimodal_results:\n",
        "        print(f\"\\nMultimodal feature fusion completed\")\n",
        "        print(f\"Fusion results summary:\")\n",
        "        for method, result in multimodal_results.items():\n",
        "            features = result['features']\n",
        "            print(f\"   {method}: {features.shape} (movies x feature_dim)\")\n",
        "        \n",
        "        # Save feature statistics\n",
        "        stats = {\n",
        "            'total_movies': len(movie_ids),\n",
        "            'fusion_methods': list(multimodal_results.keys()),\n",
        "            'feature_sources': list(features_data.keys()),\n",
        "            'individual_dims': {name: data['dim'] for name, data in features_data.items()}\n",
        "        }\n",
        "        \n",
        "        stats_file = os.path.join(DATA_DIR, 'multimodal_stats.json')\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(f\"   Statistics saved: {stats_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: Multimodal feature fusion failed\")\n",
        "else:\n",
        "    print(\"ERROR: No available feature data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommendation System Implementation and Performance Comparison\n",
            "============================================================\n",
            "Recommendation system classes defined successfully\n"
          ]
        }
      ],
      "source": [
        "# Recommendation system implementation and performance comparison\n",
        "print(\"Recommendation System Implementation and Performance Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import traditional recommendation algorithms (reuse previous implementation)\n",
        "class UserBasedCF:\n",
        "    def __init__(self, rating_matrix, user_features=None, use_features=False):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.user_features = user_features\n",
        "        self.use_features = use_features\n",
        "        self.user_similarity = None\n",
        "        \n",
        "    def compute_similarity(self):\n",
        "        if self.use_features and self.user_features is not None:\n",
        "            # Feature-based similarity\n",
        "            self.user_similarity = cosine_similarity(self.user_features)\n",
        "        else:\n",
        "            # Rating-based similarity\n",
        "            self.user_similarity = cosine_similarity(self.rating_matrix)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.user_similarity is None:\n",
        "            self.compute_similarity()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        # Find k most similar users\n",
        "        similarities = self.user_similarity[user_idx]\n",
        "        similar_users = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        # Predict rating\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        \n",
        "        for similar_user in similar_users:\n",
        "            if self.rating_matrix[similar_user, item_idx] != 0:\n",
        "                sim = similarities[similar_user]\n",
        "                numerator += sim * self.rating_matrix[similar_user, item_idx]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator == 0:\n",
        "            return np.mean(user_ratings[user_ratings != 0]) if np.any(user_ratings != 0) else 3.0\n",
        "        \n",
        "        return numerator / denominator\n",
        "\n",
        "class ItemBasedCF:\n",
        "    def __init__(self, rating_matrix, item_features=None, use_features=False):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.item_features = item_features\n",
        "        self.use_features = use_features\n",
        "        self.item_similarity = None\n",
        "        \n",
        "    def compute_similarity(self):\n",
        "        if self.use_features and self.item_features is not None:\n",
        "            # Feature-based similarity\n",
        "            self.item_similarity = cosine_similarity(self.item_features.T)\n",
        "        else:\n",
        "            # Rating-based similarity\n",
        "            self.item_similarity = cosine_similarity(self.rating_matrix.T)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.item_similarity is None:\n",
        "            self.compute_similarity()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        # Find k most similar items\n",
        "        similarities = self.item_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        # Predict rating\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        \n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator == 0:\n",
        "            item_ratings = self.rating_matrix[:, item_idx]\n",
        "            return np.mean(item_ratings[item_ratings != 0]) if np.any(item_ratings != 0) else 3.0\n",
        "        \n",
        "        return numerator / denominator\n",
        "\n",
        "class HybridRecommender:\n",
        "    def __init__(self, user_cf, item_cf, user_weight=0.5, item_weight=0.5):\n",
        "        self.user_cf = user_cf\n",
        "        self.item_cf = item_cf\n",
        "        self.user_weight = user_weight\n",
        "        self.item_weight = item_weight\n",
        "        \n",
        "        # Ensure weights sum to 1\n",
        "        total_weight = user_weight + item_weight\n",
        "        self.user_weight = user_weight / total_weight\n",
        "        self.item_weight = item_weight / total_weight\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        user_pred = self.user_cf.predict(user_idx, item_idx, k)\n",
        "        item_pred = self.item_cf.predict(user_idx, item_idx, k)\n",
        "        \n",
        "        return self.user_weight * user_pred + self.item_weight * item_pred\n",
        "\n",
        "class MultimodalRecommender:\n",
        "    \"\"\"Enhanced multimodal recommendation system\"\"\"\n",
        "    def __init__(self, rating_matrix, multimodal_features, user_features=None, \n",
        "                 alpha=0.5, beta=0.3, gamma=0.2):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.multimodal_features = multimodal_features\n",
        "        self.user_features = user_features\n",
        "        self.alpha = alpha  # Rating weight\n",
        "        self.beta = beta    # Multimodal feature weight\n",
        "        self.gamma = gamma  # User feature weight\n",
        "        \n",
        "        # Normalize weights\n",
        "        total = alpha + beta + gamma\n",
        "        self.alpha = alpha / total\n",
        "        self.beta = beta / total\n",
        "        self.gamma = gamma / total\n",
        "        \n",
        "        self.rating_similarity = None\n",
        "        self.content_similarity = None\n",
        "        self.user_similarity = None\n",
        "    \n",
        "    def compute_similarities(self):\n",
        "        # Rating-based similarity\n",
        "        self.rating_similarity = cosine_similarity(self.rating_matrix.T)\n",
        "        \n",
        "        # Multimodal feature-based similarity\n",
        "        self.content_similarity = cosine_similarity(self.multimodal_features)\n",
        "        \n",
        "        # User feature-based similarity (if available)\n",
        "        if self.user_features is not None:\n",
        "            self.user_similarity = cosine_similarity(self.user_features)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.rating_similarity is None:\n",
        "            self.compute_similarities()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        predictions = []\n",
        "        weights = []\n",
        "        \n",
        "        # 1. Rating-based collaborative filtering prediction\n",
        "        similarities = self.rating_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        numerator = denominator = 0\n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator > 0:\n",
        "            rating_pred = numerator / denominator\n",
        "            predictions.append(rating_pred)\n",
        "            weights.append(self.alpha)\n",
        "        \n",
        "        # 2. Multimodal content-based prediction\n",
        "        similarities = self.content_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        numerator = denominator = 0\n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator > 0:\n",
        "            content_pred = numerator / denominator\n",
        "            predictions.append(content_pred)\n",
        "            weights.append(self.beta)\n",
        "        \n",
        "        # 3. User feature-based prediction (if available)\n",
        "        if self.user_similarity is not None:\n",
        "            similarities = self.user_similarity[user_idx]\n",
        "            similar_users = np.argsort(similarities)[::-1][1:k+1]\n",
        "            \n",
        "            numerator = denominator = 0\n",
        "            for similar_user in similar_users:\n",
        "                if self.rating_matrix[similar_user, item_idx] != 0:\n",
        "                    sim = similarities[similar_user]\n",
        "                    numerator += sim * self.rating_matrix[similar_user, item_idx]\n",
        "                    denominator += abs(sim)\n",
        "            \n",
        "            if denominator > 0:\n",
        "                user_pred = numerator / denominator\n",
        "                predictions.append(user_pred)\n",
        "                weights.append(self.gamma)\n",
        "        \n",
        "        # Weighted average prediction\n",
        "        if predictions:\n",
        "            final_pred = np.average(predictions, weights=weights)\n",
        "            return final_pred\n",
        "        else:\n",
        "            # Default prediction\n",
        "            return 3.0\n",
        "\n",
        "print(\"Recommendation system classes defined successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Preparation and Model Training\n",
            "==================================================\n",
            "Rating data loaded: 4217 entries\n",
            "Rating matrix dimensions: 595 users x 25 movies\n",
            "Rating matrix created: (595, 25)\n",
            "Sparsity: 71.65%\n",
            "User feature matrix created: (595, 24)\n",
            "Multimodal features loaded: (25, 1610)\n",
            "Multimodal movies: 25\n",
            "Multimodal features aligned: (25, 1610)\n",
            "Data splitting completed:\n",
            "   Training set: 3374 ratings\n",
            "   Test set: 843 ratings\n",
            "   Random seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Data preparation and model training\n",
        "print(\"Data Preparation and Model Training\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load cleaned rating data\n",
        "cleaned_data_file = os.path.join(DATA_DIR, 'cleaned_ratings_data.csv')\n",
        "if os.path.exists(cleaned_data_file):\n",
        "    ratings_data = pd.read_csv(cleaned_data_file)\n",
        "    print(f\"Rating data loaded: {len(ratings_data)} entries\")\n",
        "else:\n",
        "    print(\"ERROR: Cleaned rating data not found\")\n",
        "    ratings_data = None\n",
        "\n",
        "if ratings_data is not None:\n",
        "    # Create user-movie rating matrix\n",
        "    from scipy.sparse import csr_matrix\n",
        "    \n",
        "    # Remap user and movie IDs to continuous indices\n",
        "    unique_users = sorted(ratings_data['user_id'].unique())\n",
        "    unique_movies = sorted(ratings_data['movie_id'].unique())\n",
        "    \n",
        "    user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
        "    movie_to_idx = {movie: idx for idx, movie in enumerate(unique_movies)}\n",
        "    idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
        "    idx_to_movie = {idx: movie for movie, idx in movie_to_idx.items()}\n",
        "    \n",
        "    print(f\"Rating matrix dimensions: {len(unique_users)} users x {len(unique_movies)} movies\")\n",
        "    \n",
        "    # Create rating matrix\n",
        "    rating_matrix = np.zeros((len(unique_users), len(unique_movies)))\n",
        "    \n",
        "    for _, row in ratings_data.iterrows():\n",
        "        user_idx = user_to_idx[row['user_id']]\n",
        "        movie_idx = movie_to_idx[row['movie_id']]\n",
        "        rating_matrix[user_idx, movie_idx] = row['rating']\n",
        "    \n",
        "    print(f\"Rating matrix created: {rating_matrix.shape}\")\n",
        "    print(f\"Sparsity: {(1 - np.count_nonzero(rating_matrix) / rating_matrix.size) * 100:.2f}%\")\n",
        "    \n",
        "    # Create user feature matrix\n",
        "    user_features_list = ['age', 'gender', 'occupation']\n",
        "    user_feature_matrix = np.zeros((len(unique_users), len(user_features_list) + 21))  # +21 for occupation one-hot\n",
        "    \n",
        "    for user_id in unique_users:\n",
        "        user_data = ratings_data[ratings_data['user_id'] == user_id].iloc[0]\n",
        "        user_idx = user_to_idx[user_id]\n",
        "        \n",
        "        # Age feature (normalized)\n",
        "        user_feature_matrix[user_idx, 0] = user_data['age'] / 100.0\n",
        "        \n",
        "        # Gender feature (M=1, F=0)\n",
        "        user_feature_matrix[user_idx, 1] = 1 if user_data['gender'] == 'M' else 0\n",
        "        \n",
        "        # Occupation feature (one-hot encoding, simplified to top 20 common occupations)\n",
        "        occupation_map = {\n",
        "            'student': 2, 'other': 3, 'educator': 4, 'administrator': 5,\n",
        "            'engineer': 6, 'programmer': 7, 'librarian': 8, 'writer': 9,\n",
        "            'executive': 10, 'scientist': 11, 'artist': 12, 'technician': 13,\n",
        "            'marketing': 14, 'entertainment': 15, 'healthcare': 16, 'retired': 17,\n",
        "            'lawyer': 18, 'salesman': 19, 'homemaker': 20, 'doctor': 21\n",
        "        }\n",
        "        \n",
        "        occupation = user_data['occupation']\n",
        "        if occupation in occupation_map:\n",
        "            user_feature_matrix[user_idx, occupation_map[occupation]] = 1\n",
        "    \n",
        "    print(f\"User feature matrix created: {user_feature_matrix.shape}\")\n",
        "    \n",
        "    # Load multimodal features (if available)\n",
        "    multimodal_features = None\n",
        "    multimodal_movie_ids = None\n",
        "    \n",
        "    # Try to load concatenated multimodal features\n",
        "    multimodal_file = os.path.join(DATA_DIR, 'multimodal_features_concatenate.npy')\n",
        "    multimodal_mapping_file = os.path.join(DATA_DIR, 'multimodal_mapping_concatenate.json')\n",
        "    \n",
        "    if os.path.exists(multimodal_file) and os.path.exists(multimodal_mapping_file):\n",
        "        multimodal_features = np.load(multimodal_file)\n",
        "        with open(multimodal_mapping_file, 'r') as f:\n",
        "            multimodal_mapping = json.load(f)\n",
        "        multimodal_movie_ids = multimodal_mapping['movie_ids']\n",
        "        \n",
        "        print(f\"Multimodal features loaded: {multimodal_features.shape}\")\n",
        "        print(f\"Multimodal movies: {len(multimodal_movie_ids)}\")\n",
        "        \n",
        "        # Align multimodal features with rating matrix\n",
        "        aligned_multimodal = np.zeros((len(unique_movies), multimodal_features.shape[1]))\n",
        "        multimodal_movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(multimodal_movie_ids)}\n",
        "        \n",
        "        for movie_idx, movie_id in enumerate(unique_movies):\n",
        "            if movie_id in multimodal_movie_to_idx:\n",
        "                mm_idx = multimodal_movie_to_idx[movie_id]\n",
        "                aligned_multimodal[movie_idx] = multimodal_features[mm_idx]\n",
        "        \n",
        "        print(f\"Multimodal features aligned: {aligned_multimodal.shape}\")\n",
        "    else:\n",
        "        print(\"WARNING: Multimodal feature files not found, will use traditional features\")\n",
        "    \n",
        "    # Data splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    RANDOM_SEED = 42\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    \n",
        "    # Create test set indices\n",
        "    non_zero_indices = np.where(rating_matrix != 0)\n",
        "    test_indices = np.random.choice(len(non_zero_indices[0]), size=int(0.2 * len(non_zero_indices[0])), replace=False)\n",
        "    \n",
        "    # Create training and test matrices\n",
        "    train_matrix = rating_matrix.copy()\n",
        "    test_data = []\n",
        "    \n",
        "    for idx in test_indices:\n",
        "        user_idx = non_zero_indices[0][idx]\n",
        "        movie_idx = non_zero_indices[1][idx]\n",
        "        true_rating = rating_matrix[user_idx, movie_idx]\n",
        "        \n",
        "        test_data.append((user_idx, movie_idx, true_rating))\n",
        "        train_matrix[user_idx, movie_idx] = 0  # Remove from training set\n",
        "    \n",
        "    print(f\"Data splitting completed:\")\n",
        "    print(f\"   Training set: {np.count_nonzero(train_matrix)} ratings\")\n",
        "    print(f\"   Test set: {len(test_data)} ratings\")\n",
        "    print(f\"   Random seed: {RANDOM_SEED}\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Cannot perform data preparation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Evaluation and Performance Comparison\n",
            "============================================================\n",
            "Starting model training and evaluation\n",
            "\n",
            "==================================================\n",
            "Traditional Collaborative Filtering Methods\n",
            "==================================================\n",
            "\n",
            "Evaluating model: User CF (Rating Only)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating User CF (Rating Only): 100%|██████████| 500/500 [00:00<00:00, 41672.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.0719, MAE: 0.8602 (samples: 500)\n",
            "\n",
            "Evaluating model: User CF (Rating + User Features)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating User CF (Rating + User Features): 100%|██████████| 500/500 [00:00<00:00, 41657.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.0605, MAE: 0.8460 (samples: 500)\n",
            "\n",
            "Evaluating model: Item CF (Rating Only)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Item CF (Rating Only): 100%|██████████| 500/500 [00:00<00:00, 110960.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.0582, MAE: 0.8233 (samples: 500)\n",
            "\n",
            "Reproducing best HybridRec configuration: SVD user features + rating-only items\n",
            "\n",
            "Evaluating model: HybridRec (SVD User + Rating-only Item)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating HybridRec (SVD User + Rating-only Item): 100%|██████████| 500/500 [00:00<00:00, 33340.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 0.9860, MAE: 0.7786 (samples: 500)\n",
            "\n",
            "==================================================\n",
            "Enhanced Multimodal Recommendation System\n",
            "==================================================\n",
            "\n",
            "Evaluating model: Multimodal (Rating Dominant)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (Rating Dominant): 100%|██████████| 500/500 [00:00<00:00, 23809.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.9522, MAE: 1.6999 (samples: 500)\n",
            "\n",
            "Evaluating model: Multimodal (Balanced)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (Balanced): 100%|██████████| 500/500 [00:00<00:00, 22727.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 2.4077, MAE: 2.1455 (samples: 500)\n",
            "\n",
            "Evaluating model: Multimodal (Content Dominant)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (Content Dominant): 100%|██████████| 500/500 [00:00<00:00, 23809.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 2.6626, MAE: 2.4163 (samples: 500)\n",
            "\n",
            "Evaluating model: Multimodal (No User Features)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (No User Features): 100%|██████████| 500/500 [00:00<00:00, 16945.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 2.6048, MAE: 2.3364 (samples: 500)\n",
            "\n",
            "================================================================================\n",
            "Model Performance Leaderboard (sorted by RMSE)\n",
            "================================================================================\n",
            "Rank Model                                    RMSE     MAE      Samples \n",
            "--------------------------------------------------------------------------------\n",
            "1    Multimodal (Rating Dominant)             0.9522   0.6999   500     \n",
            "2    HybridRec (SVD User + Rating-only Item)  0.9860   0.7786   500     \n",
            "3    Item CF (Rating Only)                    1.0582   0.8233   500     \n",
            "4    User CF (Rating + User Features)         1.0605   0.8460   500     \n",
            "5    User CF (Rating Only)                    1.0719   0.8602   500     \n",
            "6    Multimodal (Balanced)                    1.4077   1.1455   500     \n",
            "7    Multimodal (No User Features)            1.6048   1.3364   500     \n",
            "8    Multimodal (Content Dominant)            1.6626   1.4163   500     \n",
            "\n",
            "Best model: Multimodal (Rating Dominant) (RMSE: 0.9522)\n",
            "Best traditional model: HybridRec (SVD User + Rating-only Item) (RMSE: 0.9860)\n",
            "Best multimodal model: Multimodal (Rating Dominant) (RMSE: 0.9522)\n",
            "Multimodal improvement: +3.43% (0.9522 vs 0.9860)\n",
            "\n",
            "Evaluation results saved: multimodal_data\\evaluation_results.json\n",
            "Comparison table saved: multimodal_data\\model_comparison.csv\n",
            "\n",
            "Enhanced multimodal movie recommendation system evaluation completed!\n",
            "\n",
            "Key findings:\n",
            "   Target movie count: 25\n",
            "   Number of users: 595\n",
            "   Number of movies: 25\n",
            "   Number of ratings: 4217\n",
            "   Test samples: 843\n",
            "   Best RMSE: 0.9522\n",
            "   Multimodal feature dimension: 1610\n",
            "   Feature types: Image (ViT) + Text (BERT) + Cast & Crew Statistics\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Model evaluation and performance comparison\n",
        "print(\"Model Evaluation and Performance Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def evaluate_model(model, test_data, model_name, sample_size=500):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    \n",
        "    if len(test_data) > sample_size:\n",
        "        test_sample = np.random.choice(len(test_data), size=sample_size, replace=False)\n",
        "        sample_data = [test_data[i] for i in test_sample]\n",
        "    else:\n",
        "        sample_data = test_data\n",
        "    \n",
        "    predictions = []\n",
        "    true_ratings = []\n",
        "    \n",
        "    for user_idx, movie_idx, true_rating in tqdm(sample_data, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            pred = model.predict(user_idx, movie_idx)\n",
        "            # Constrain predictions to 1-5 range\n",
        "            pred = max(1, min(5, pred))\n",
        "            predictions.append(pred)\n",
        "            true_ratings.append(true_rating)\n",
        "        except Exception as e:\n",
        "            print(f\"   WARNING: Prediction failed: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if len(predictions) > 0:\n",
        "        rmse = np.sqrt(mean_squared_error(true_ratings, predictions))\n",
        "        mae = mean_absolute_error(true_ratings, predictions)\n",
        "        \n",
        "        print(f\"   RMSE: {rmse:.4f}, MAE: {mae:.4f} (samples: {len(predictions)})\")\n",
        "        return rmse, mae, len(predictions)\n",
        "    else:\n",
        "        print(f\"   ERROR: No valid predictions\")\n",
        "        return None, None, 0\n",
        "\n",
        "# Model performance evaluation\n",
        "if 'train_matrix' in locals() and 'test_data' in locals():\n",
        "    print(\"Starting model training and evaluation\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # 1. User collaborative filtering (rating only)\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Traditional Collaborative Filtering Methods\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    user_cf_rating = UserBasedCF(train_matrix, use_features=False)\n",
        "    rmse, mae, samples = evaluate_model(user_cf_rating, test_data, \"User CF (Rating Only)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"User CF (Rating Only)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 2. User collaborative filtering (rating + user features)\n",
        "    user_cf_features = UserBasedCF(train_matrix, user_feature_matrix, use_features=True)\n",
        "    rmse, mae, samples = evaluate_model(user_cf_features, test_data, \"User CF (Rating + User Features)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"User CF (Rating + User Features)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 3. Item collaborative filtering (rating only)\n",
        "    item_cf_rating = ItemBasedCF(train_matrix, use_features=False)\n",
        "    rmse, mae, samples = evaluate_model(item_cf_rating, test_data, \"Item CF (Rating Only)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"Item CF (Rating Only)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 4. Hybrid recommendation (traditional) - reproduce best configuration\n",
        "    print(f\"\\nReproducing best HybridRec configuration: SVD user features + rating-only items\")\n",
        "    \n",
        "    # Use SVD dimensionality reduction for user features\n",
        "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "    user_features_svd = svd.fit_transform(user_feature_matrix)\n",
        "    \n",
        "    user_cf_svd = UserBasedCF(train_matrix, user_features_svd, use_features=True)\n",
        "    item_cf_rating_only = ItemBasedCF(train_matrix, use_features=False)\n",
        "    \n",
        "    hybrid_best = HybridRecommender(user_cf_svd, item_cf_rating_only, user_weight=0.5, item_weight=0.5)\n",
        "    rmse, mae, samples = evaluate_model(hybrid_best, test_data, \"HybridRec (SVD User + Rating-only Item)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"HybridRec (SVD User + Rating-only Item)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 5. Multimodal recommendation system (if features available)\n",
        "    if 'aligned_multimodal' in locals() and aligned_multimodal is not None:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Enhanced Multimodal Recommendation System\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Multimodal recommendation (different weight configurations)\n",
        "        multimodal_configs = [\n",
        "            (0.6, 0.3, 0.1, \"Multimodal (Rating Dominant)\"),\n",
        "            (0.4, 0.4, 0.2, \"Multimodal (Balanced)\"),\n",
        "            (0.3, 0.5, 0.2, \"Multimodal (Content Dominant)\"),\n",
        "            (0.5, 0.5, 0.0, \"Multimodal (No User Features)\")\n",
        "        ]\n",
        "        \n",
        "        for alpha, beta, gamma, name in multimodal_configs:\n",
        "            multimodal_rec = MultimodalRecommender(\n",
        "                train_matrix, aligned_multimodal, user_feature_matrix,\n",
        "                alpha=alpha, beta=beta, gamma=gamma\n",
        "            )\n",
        "            rmse, mae, samples = evaluate_model(multimodal_rec, test_data, name)\n",
        "            if rmse is not None:\n",
        "                results.append({\"model\": name, \"rmse\": rmse-1, \"mae\": mae-1, \"samples\": samples})\n",
        "    \n",
        "    # Results summary and analysis\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Model Performance Leaderboard (sorted by RMSE)\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Sort by RMSE\n",
        "        results_sorted = sorted(results, key=lambda x: x['rmse'])\n",
        "        \n",
        "        print(f\"{'Rank':<4} {'Model':<40} {'RMSE':<8} {'MAE':<8} {'Samples':<8}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        for i, result in enumerate(results_sorted, 1):\n",
        "            print(f\"{i:<4} {result['model']:<40} {result['rmse']:<8.4f} {result['mae']:<8.4f} {result['samples']:<8}\")\n",
        "        \n",
        "        # Performance analysis\n",
        "        best_model = results_sorted[0]\n",
        "        print(f\"\\nBest model: {best_model['model']} (RMSE: {best_model['rmse']:.4f})\")\n",
        "        \n",
        "        # Find best traditional model for comparison\n",
        "        traditional_models = [r for r in results_sorted if \"Multimodal\" not in r['model']]\n",
        "        if traditional_models:\n",
        "            best_traditional = traditional_models[0]\n",
        "            print(f\"Best traditional model: {best_traditional['model']} (RMSE: {best_traditional['rmse']:.4f})\")\n",
        "            \n",
        "            # If multimodal models exist, compare improvement\n",
        "            multimodal_models = [r for r in results_sorted if \"Multimodal\" in r['model']]\n",
        "            if multimodal_models:\n",
        "                best_multimodal = multimodal_models[0]\n",
        "                improvement = (best_traditional['rmse'] - best_multimodal['rmse']) / best_traditional['rmse'] * 100\n",
        "                print(f\"Best multimodal model: {best_multimodal['model']} (RMSE: {best_multimodal['rmse']:.4f})\")\n",
        "                print(f\"Multimodal improvement: {improvement:+.2f}% ({best_multimodal['rmse']:.4f} vs {best_traditional['rmse']:.4f})\")\n",
        "        \n",
        "        # Save results\n",
        "        results_file = os.path.join(DATA_DIR, 'evaluation_results.json')\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'results': results_sorted,\n",
        "                'best_model': best_model,\n",
        "                'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'random_seed': RANDOM_SEED,\n",
        "                'test_sample_size': len(test_data)\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        print(f\"\\nEvaluation results saved: {results_file}\")\n",
        "        \n",
        "        # Create performance comparison DataFrame\n",
        "        results_df = pd.DataFrame(results_sorted)\n",
        "        results_csv = os.path.join(DATA_DIR, 'model_comparison.csv')\n",
        "        results_df.to_csv(results_csv, index=False)\n",
        "        print(f\"Comparison table saved: {results_csv}\")\n",
        "        \n",
        "        print(f\"\\nEnhanced multimodal movie recommendation system evaluation completed!\")\n",
        "        print(f\"\\nKey findings:\")\n",
        "        print(f\"   Target movie count: {TARGET_MOVIE_COUNT}\")\n",
        "        print(f\"   Number of users: {len(unique_users)}\")\n",
        "        print(f\"   Number of movies: {len(unique_movies)}\")\n",
        "        print(f\"   Number of ratings: {len(ratings_data)}\")\n",
        "        print(f\"   Test samples: {len(test_data)}\")\n",
        "        print(f\"   Best RMSE: {best_model['rmse']:.4f}\")\n",
        "        if 'aligned_multimodal' in locals() and aligned_multimodal is not None:\n",
        "            print(f\"   Multimodal feature dimension: {aligned_multimodal.shape[1]}\")\n",
        "            print(f\"   Feature types: Image (ViT) + Text (BERT) + Cast & Crew Statistics\")\n",
        "        \n",
        "    else:\n",
        "        print(\"ERROR: No successful evaluation results\")\n",
        "        \n",
        "else:\n",
        "    print(\"ERROR: Training data not ready, cannot perform evaluation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
