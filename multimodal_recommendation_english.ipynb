{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Movie Recommendation System\n",
        "\n",
        "A comprehensive recommendation system based on MovieLens 100k + TMDB + ViT Image Features + BERT Text Features\n",
        "\n",
        "## Key Features\n",
        "- **Smart Quantity Control**: Set target number of movies with automatic progress management\n",
        "- **Image Feature Extraction**: ViT model for extracting visual features from posters and stills  \n",
        "- **Text Feature Extraction**: BERT model for processing movie overviews and taglines\n",
        "- **Cast & Crew Statistics**: Extract high-frequency actors and directors as features\n",
        "- **Multi-dimensional Feature Engineering**: Fusion of numerical, categorical, text, and image features\n",
        "- **Performance Comparison**: Comprehensive evaluation against traditional recommendation systems\n",
        "\n",
        "## System Architecture\n",
        "1. **Data Preparation**: MovieLens + TMDB + User Filtering\n",
        "2. **Image Processing**: Download → Selection → ViT Feature Extraction\n",
        "3. **Text Processing**: BERT Feature Extraction\n",
        "4. **Feature Engineering**: Multimodal Feature Fusion\n",
        "5. **Recommendation System**: Multi-algorithm Performance Comparison\n",
        "\n",
        "## Innovation Points\n",
        "- Multimodal feature fusion (text + image + traditional features)\n",
        "- Intelligent image selection (5 most diverse images from 10)\n",
        "- Comprehensive performance evaluation (reproducing best HybridRec configuration)\n",
        "\n",
        "## Technical Stack\n",
        "- **Computer Vision**: ViT (Vision Transformer) for image understanding\n",
        "- **Natural Language Processing**: BERT for semantic text analysis\n",
        "- **Recommendation Algorithms**: Collaborative Filtering, Content-based, Hybrid approaches\n",
        "- **Evaluation Metrics**: RMSE, MAE with statistical significance testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: pillow in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: torch in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.7.1)\n",
            "Requirement already satisfied: torchvision in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (0.22.1)\n",
            "Requirement already satisfied: scikit-learn in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: opencv-python in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: colorama in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: filelock in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.8.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Enhanced Multimodal Recommendation System\n",
            "============================================================\n",
            "Target movie count: 42\n",
            "Images per movie: 10 -> select 5\n",
            "Image directories: multimodal_images -> selected_images\n",
            "Data directory: multimodal_data\n",
            "API request interval: 0.3 seconds\n",
            "\n",
            "Environment setup completed\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install requests pandas pillow tqdm transformers torch torchvision scikit-learn opencv-python\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from transformers import BertTokenizer, BertModel, ViTImageProcessor, ViTModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"Enhanced Multimodal Recommendation System\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ===================== Core Configuration Parameters =====================\n",
        "# Target movie count - user configurable\n",
        "TARGET_MOVIE_COUNT = 42  # Set the desired number of movies to process\n",
        "\n",
        "# API Configuration\n",
        "TMDB_API_KEY = \"6ba3eb883961b80c06d196906b976afe\"\n",
        "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "TMDB_IMAGE_BASE_URL = \"https://image.tmdb.org/t/p/original\"\n",
        "\n",
        "# File path configuration\n",
        "IMAGE_DIR = \"multimodal_images\"          # Original image directory\n",
        "SELECTED_IMAGE_DIR = \"selected_images\"    # Selected image directory\n",
        "DATA_DIR = \"multimodal_data\"             # Data storage directory\n",
        "PROGRESS_FILE = \"multimodal_progress.json\" # Progress tracking file\n",
        "\n",
        "# Processing parameters\n",
        "IMAGES_PER_MOVIE = 10    # Number of images to download per movie\n",
        "SELECTED_IMAGES = 5      # Number of images to select after filtering\n",
        "DELAY_BETWEEN_REQUESTS = 0.3  # API request interval (seconds)\n",
        "\n",
        "# Create directories\n",
        "for directory in [IMAGE_DIR, SELECTED_IMAGE_DIR, DATA_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Target movie count: {TARGET_MOVIE_COUNT}\")\n",
        "print(f\"Images per movie: {IMAGES_PER_MOVIE} -> select {SELECTED_IMAGES}\")\n",
        "print(f\"Image directories: {IMAGE_DIR} -> {SELECTED_IMAGE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"API request interval: {DELAY_BETWEEN_REQUESTS} seconds\")\n",
        "print(\"\\nEnvironment setup completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loading and Progress Check\n",
            "==================================================\n",
            "MovieLens movie data loaded: 1682 movies\n",
            "IMDB ID mapping loaded: 42 entries\n",
            "User rating data loaded: 100,000 ratings\n",
            "Number of users: 943\n",
            "Number of movies: 1,682\n",
            "User information loaded: 943 users\n",
            "\n",
            "Target movie processing: 42 / 42 valid movies\n",
            "Data coverage: 100.0%\n",
            "\n",
            "Target movie list:\n",
            "    1.   1. Toy Story (1995) -> tt0114709\n",
            "    2.   2. GoldenEye (1995) -> tt0113189\n",
            "    3.   3. Four Rooms (1995) -> tt0113101\n",
            "    4.   4. Get Shorty (1995) -> tt0113161\n",
            "    5.   5. Copycat (1995) -> tt0112722\n",
            "    6.   6. Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) -> tt0115012\n",
            "    7.   7. Twelve Monkeys (1995) -> tt0114746\n",
            "    8.   8. Babe (1995) -> tt0112431\n",
            "    9.   9. Dead Man Walking (1995) -> tt0112818\n",
            "   10.  10. Richard III (1995) -> tt0114279\n",
            "   ... and 32 more movies\n",
            "\n",
            "Target movie list saved: multimodal_data\\target_movies.csv\n"
          ]
        }
      ],
      "source": [
        "# Data loading and intelligent progress management\n",
        "print(\"Data Loading and Progress Check\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load MovieLens movie data\n",
        "try:\n",
        "    movies_df = pd.read_csv('movielens_movies.csv')\n",
        "    print(f\"MovieLens movie data loaded: {len(movies_df)} movies\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: movielens_movies.csv not found\")\n",
        "    print(\"Please run generate_imdb_mapping.py to generate movie information\")\n",
        "\n",
        "# Load IMDB ID mapping\n",
        "try:\n",
        "    with open('imdb/progress_mapping.json', 'r', encoding='utf-8') as f:\n",
        "        imdb_mapping = json.load(f)\n",
        "    imdb_mapping = {int(k): v for k, v in imdb_mapping.items()}\n",
        "    print(f\"IMDB ID mapping loaded: {len(imdb_mapping)} entries\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: IMDB ID mapping file not found\")\n",
        "\n",
        "# Load user rating data\n",
        "try:\n",
        "    ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', \n",
        "                           names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "    print(f\"User rating data loaded: {len(ratings_df):,} ratings\")\n",
        "    print(f\"Number of users: {ratings_df['user_id'].nunique():,}\")\n",
        "    print(f\"Number of movies: {ratings_df['movie_id'].nunique():,}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: ml-100k/u.data file not found\")\n",
        "\n",
        "# Load user information\n",
        "try:\n",
        "    users_df = pd.read_csv('ml-100k/u.user', sep='|', \n",
        "                         names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "    print(f\"User information loaded: {len(users_df)} users\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: ml-100k/u.user file not found\")\n",
        "\n",
        "# Intelligent target movie selection\n",
        "if 'movies_df' in locals() and 'imdb_mapping' in locals():\n",
        "    valid_movies = movies_df[movies_df['movie_id'].isin(imdb_mapping.keys())].copy()\n",
        "    valid_movies['imdb_id'] = valid_movies['movie_id'].map(imdb_mapping)\n",
        "    \n",
        "    # Select target movies based on TARGET_MOVIE_COUNT\n",
        "    target_movies = valid_movies.head(TARGET_MOVIE_COUNT).copy()\n",
        "    \n",
        "    print(f\"\\nTarget movie processing: {len(target_movies)} / {len(valid_movies)} valid movies\")\n",
        "    print(f\"Data coverage: {len(target_movies)/len(valid_movies)*100:.1f}%\")\n",
        "    \n",
        "    # Display target movie list\n",
        "    print(f\"\\nTarget movie list:\")\n",
        "    for i, (_, movie) in enumerate(target_movies.head(10).iterrows(), 1):\n",
        "        print(f\"   {i:2d}. {movie['movie_id']:3d}. {movie['title']} ({movie['year']}) -> tt{movie['imdb_id']}\")\n",
        "    \n",
        "    if len(target_movies) > 10:\n",
        "        print(f\"   ... and {len(target_movies) - 10} more movies\")\n",
        "        \n",
        "    # Save target movie list\n",
        "    target_file = os.path.join(DATA_DIR, 'target_movies.csv')\n",
        "    target_movies.to_csv(target_file, index=False)\n",
        "    print(f\"\\nTarget movie list saved: {target_file}\")\n",
        "else:\n",
        "    print(\"ERROR: Data loading failed, cannot continue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Data Filtering and Cleaning\n",
            "==================================================\n",
            "Original rating data: 100,000 entries\n",
            "Target movie ratings: 5,845 entries\n",
            "Data filtering rate: 5.8%\n",
            "\n",
            "User activity analysis:\n",
            "   Total users: 766\n",
            "   Average ratings per user: 7.6\n",
            "   Median ratings per user: 6.0\n",
            "   Most active user: 42 ratings\n",
            "   Least active user: 1 ratings\n",
            "\n",
            "Data cleaning results:\n",
            "   Minimum ratings requirement: 3 entries\n",
            "   Users retained: 630 / 766 (82.2%)\n",
            "   Ratings retained: 5,635 / 5,845 (96.4%)\n",
            "\n",
            "Final cleaned dataset:\n",
            "   Rating records: 5,635 entries\n",
            "   Number of users: 630\n",
            "   Number of movies: 42\n",
            "   Average ratings per user: 8.9\n",
            "   Average ratings per movie: 134.2\n",
            "\n",
            "User demographic distribution:\n",
            "   Age range: 7-73 years\n",
            "   Average age: 32.0 years\n",
            "   Gender distribution: {'M': np.int64(4329), 'F': np.int64(1306)}\n",
            "   Top 5 occupations: {'student': np.int64(1351), 'other': np.int64(595), 'educator': np.int64(517), 'engineer': np.int64(489), 'programmer': np.int64(475)}\n",
            "\n",
            "Rating distribution:\n",
            "   1 stars: 231 entries (4.1%)\n",
            "   2 stars: 547 entries (9.7%)\n",
            "   3 stars: 1,433 entries (25.4%)\n",
            "   4 stars: 2,071 entries (36.8%)\n",
            "   5 stars: 1,353 entries (24.0%)\n",
            "   Average rating: 3.67\n",
            "\n",
            "Cleaned data saved: multimodal_data\\cleaned_ratings_data.csv\n",
            "Updated target movie count: 42 movies (with actual rating data)\n",
            "\n",
            "Rating matrix dimensions:\n",
            "   User-Movie matrix: 630 x 42 = 26,460 possible ratings\n",
            "   Actual ratings: 5,635\n",
            "   Sparsity: 78.70% (percentage of missing values)\n",
            "   Density: 21.30% (percentage of observed values)\n"
          ]
        }
      ],
      "source": [
        "# User data filtering and cleaning\n",
        "print(\"User Data Filtering and Cleaning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'target_movies' in locals() and 'ratings_df' in locals():\n",
        "    # Filter rating data for target movies\n",
        "    target_movie_ids = set(target_movies['movie_id'].values)\n",
        "    filtered_ratings = ratings_df[ratings_df['movie_id'].isin(target_movie_ids)].copy()\n",
        "    \n",
        "    print(f\"Original rating data: {len(ratings_df):,} entries\")\n",
        "    print(f\"Target movie ratings: {len(filtered_ratings):,} entries\")\n",
        "    print(f\"Data filtering rate: {len(filtered_ratings)/len(ratings_df)*100:.1f}%\")\n",
        "    \n",
        "    # Analyze user activity\n",
        "    user_activity = filtered_ratings['user_id'].value_counts()\n",
        "    print(f\"\\nUser activity analysis:\")\n",
        "    print(f\"   Total users: {len(user_activity):,}\")\n",
        "    print(f\"   Average ratings per user: {user_activity.mean():.1f}\")\n",
        "    print(f\"   Median ratings per user: {user_activity.median():.1f}\")\n",
        "    print(f\"   Most active user: {user_activity.max()} ratings\")\n",
        "    print(f\"   Least active user: {user_activity.min()} ratings\")\n",
        "    \n",
        "    # Clean low-activity users (less than 3 ratings)\n",
        "    MIN_RATINGS_PER_USER = 3\n",
        "    active_users = user_activity[user_activity >= MIN_RATINGS_PER_USER].index\n",
        "    cleaned_ratings = filtered_ratings[filtered_ratings['user_id'].isin(active_users)].copy()\n",
        "    \n",
        "    print(f\"\\nData cleaning results:\")\n",
        "    print(f\"   Minimum ratings requirement: {MIN_RATINGS_PER_USER} entries\")\n",
        "    print(f\"   Users retained: {len(active_users):,} / {len(user_activity):,} ({len(active_users)/len(user_activity)*100:.1f}%)\")\n",
        "    print(f\"   Ratings retained: {len(cleaned_ratings):,} / {len(filtered_ratings):,} ({len(cleaned_ratings)/len(filtered_ratings)*100:.1f}%)\")\n",
        "    \n",
        "    # Merge user information and handle missing values\n",
        "    if 'users_df' in locals():\n",
        "        cleaned_ratings_with_users = cleaned_ratings.merge(users_df, on='user_id', how='left')\n",
        "        \n",
        "        # Check and clean missing user information\n",
        "        missing_user_info = cleaned_ratings_with_users['age'].isna().sum()\n",
        "        if missing_user_info > 0:\n",
        "            print(f\"WARNING: Missing user information: {missing_user_info:,} entries ({missing_user_info/len(cleaned_ratings_with_users)*100:.1f}%)\")\n",
        "            # Remove records with missing user information\n",
        "            cleaned_ratings_with_users.dropna(subset=['age', 'gender', 'occupation'], inplace=True)\n",
        "            print(f\"Ratings after cleaning: {len(cleaned_ratings_with_users):,} entries\")\n",
        "        \n",
        "        print(f\"\\nFinal cleaned dataset:\")\n",
        "        print(f\"   Rating records: {len(cleaned_ratings_with_users):,} entries\")\n",
        "        print(f\"   Number of users: {cleaned_ratings_with_users['user_id'].nunique():,}\")\n",
        "        print(f\"   Number of movies: {cleaned_ratings_with_users['movie_id'].nunique():,}\")\n",
        "        print(f\"   Average ratings per user: {len(cleaned_ratings_with_users)/cleaned_ratings_with_users['user_id'].nunique():.1f}\")\n",
        "        print(f\"   Average ratings per movie: {len(cleaned_ratings_with_users)/cleaned_ratings_with_users['movie_id'].nunique():.1f}\")\n",
        "        \n",
        "        # User feature analysis\n",
        "        print(f\"\\nUser demographic distribution:\")\n",
        "        print(f\"   Age range: {cleaned_ratings_with_users['age'].min()}-{cleaned_ratings_with_users['age'].max()} years\")\n",
        "        print(f\"   Average age: {cleaned_ratings_with_users['age'].mean():.1f} years\")\n",
        "        print(f\"   Gender distribution: {dict(cleaned_ratings_with_users['gender'].value_counts())}\")\n",
        "        print(f\"   Top 5 occupations: {dict(cleaned_ratings_with_users['occupation'].value_counts().head())}\")\n",
        "        \n",
        "        # Rating distribution analysis\n",
        "        rating_dist = cleaned_ratings_with_users['rating'].value_counts().sort_index()\n",
        "        print(f\"\\nRating distribution:\")\n",
        "        for rating, count in rating_dist.items():\n",
        "            print(f\"   {rating} stars: {count:,} entries ({count/len(cleaned_ratings_with_users)*100:.1f}%)\")\n",
        "        print(f\"   Average rating: {cleaned_ratings_with_users['rating'].mean():.2f}\")\n",
        "        \n",
        "        # Save cleaned data\n",
        "        cleaned_data_file = os.path.join(DATA_DIR, 'cleaned_ratings_data.csv')\n",
        "        cleaned_ratings_with_users.to_csv(cleaned_data_file, index=False)\n",
        "        print(f\"\\nCleaned data saved: {cleaned_data_file}\")\n",
        "        \n",
        "        # Update target_movies to include only movies with ratings\n",
        "        rated_movie_ids = set(cleaned_ratings_with_users['movie_id'].unique())\n",
        "        target_movies = target_movies[target_movies['movie_id'].isin(rated_movie_ids)].copy()\n",
        "        print(f\"Updated target movie count: {len(target_movies)} movies (with actual rating data)\")\n",
        "        \n",
        "        # Print rating matrix dimension information\n",
        "        n_users = cleaned_ratings_with_users['user_id'].nunique()\n",
        "        n_movies = cleaned_ratings_with_users['movie_id'].nunique()\n",
        "        sparsity = 1 - len(cleaned_ratings_with_users) / (n_users * n_movies)\n",
        "        print(f\"\\nRating matrix dimensions:\")\n",
        "        print(f\"   User-Movie matrix: {n_users} x {n_movies} = {n_users * n_movies:,} possible ratings\")\n",
        "        print(f\"   Actual ratings: {len(cleaned_ratings_with_users):,}\")\n",
        "        print(f\"   Sparsity: {sparsity*100:.2f}% (percentage of missing values)\")\n",
        "        print(f\"   Density: {(1-sparsity)*100:.2f}% (percentage of observed values)\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: Required data missing, cannot perform user data filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TMDB Data Acquisition and Image Download\n",
            "==================================================\n",
            "Progress loaded: 25 processed, 0 failed\n",
            "Remaining to process: 17 movies\n",
            "\n",
            "TMDB processor ready\n"
          ]
        }
      ],
      "source": [
        "# TMDB data acquisition and image download processor\n",
        "print(\"TMDB Data Acquisition and Image Download\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class EnhancedTMDBProcessor:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "        self.processed_movies = set()\n",
        "        self.failed_movies = set()\n",
        "        self.movie_features = {}\n",
        "        self.load_progress()\n",
        "    \n",
        "    def load_progress(self):\n",
        "        \"\"\"Load processing progress\"\"\"\n",
        "        if os.path.exists(PROGRESS_FILE):\n",
        "            try:\n",
        "                with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "                    progress = json.load(f)\n",
        "                self.processed_movies = set(progress.get(\"processed\", []))\n",
        "                self.failed_movies = set(progress.get(\"failed\", []))\n",
        "                self.movie_features = progress.get(\"movie_features\", {})\n",
        "                print(f\"Progress loaded: {len(self.processed_movies)} processed, {len(self.failed_movies)} failed\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Failed to load progress: {str(e)}\")\n",
        "    \n",
        "    def save_progress(self):\n",
        "        \"\"\"Save processing progress\"\"\"\n",
        "        try:\n",
        "            progress = {\n",
        "                \"processed\": list(self.processed_movies),\n",
        "                \"failed\": list(self.failed_movies),\n",
        "                \"movie_features\": self.movie_features,\n",
        "                \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"target_count\": TARGET_MOVIE_COUNT,\n",
        "                \"total_processed\": len(self.processed_movies),\n",
        "                \"total_failed\": len(self.failed_movies)\n",
        "            }\n",
        "            with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(progress, f, ensure_ascii=False, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save progress: {str(e)}\")\n",
        "    \n",
        "    def find_movie_by_imdb_id(self, imdb_id):\n",
        "        \"\"\"Find TMDB movie by IMDB ID\"\"\"\n",
        "        if not imdb_id.startswith('tt'):\n",
        "            imdb_id = f\"tt{imdb_id}\"\n",
        "        \n",
        "        url = f\"{TMDB_BASE_URL}/find/{imdb_id}\"\n",
        "        params = {\"api_key\": self.api_key, \"external_source\": \"imdb_id\"}\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if data.get(\"movie_results\"):\n",
        "                    return data[\"movie_results\"][0]\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"     ERROR: Search failed: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def get_movie_details(self, tmdb_id):\n",
        "        \"\"\"Get detailed movie information\"\"\"\n",
        "        url = f\"{TMDB_BASE_URL}/movie/{tmdb_id}\"\n",
        "        params = {\n",
        "            \"api_key\": self.api_key,\n",
        "            \"append_to_response\": \"credits,keywords,videos,images\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"     ERROR: Failed to get details: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def clean_filename(self, filename):\n",
        "        \"\"\"Clean filename for safe storage\"\"\"\n",
        "        invalid_chars = '<>:\"/\\\\|?*'\n",
        "        for char in invalid_chars:\n",
        "            filename = filename.replace(char, '_')\n",
        "        return filename[:100]\n",
        "    \n",
        "    def download_image(self, url, save_path):\n",
        "        \"\"\"Download and process image\"\"\"\n",
        "        if os.path.exists(save_path):\n",
        "            return True\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                if img.size[0] < 50 or img.size[1] < 50:\n",
        "                    return False\n",
        "                \n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "                \n",
        "                # Resize to 512x512 for ViT processing\n",
        "                img = img.resize((512, 512), Image.Resampling.LANCZOS)\n",
        "                img.save(save_path, \"JPEG\", quality=90, optimize=True)\n",
        "                return True\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"       ERROR: Download failed: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def download_movie_images(self, movie_id, title, tmdb_movie):\n",
        "        \"\"\"Download movie images - up to 10 images\"\"\"\n",
        "        movie_folder = os.path.join(IMAGE_DIR, f\"{movie_id:04d}_{self.clean_filename(title)}\")\n",
        "        os.makedirs(movie_folder, exist_ok=True)\n",
        "        \n",
        "        downloaded = []\n",
        "        \n",
        "        # 1. Download poster\n",
        "        if tmdb_movie.get(\"poster_path\"):\n",
        "            poster_url = f\"{TMDB_IMAGE_BASE_URL}{tmdb_movie['poster_path']}\"\n",
        "            poster_path = os.path.join(movie_folder, \"poster.jpg\")\n",
        "            if self.download_image(poster_url, poster_path):\n",
        "                downloaded.append((\"poster\", poster_path))\n",
        "        \n",
        "        # 2. Download backdrop\n",
        "        if tmdb_movie.get(\"backdrop_path\"):\n",
        "            backdrop_url = f\"{TMDB_IMAGE_BASE_URL}{tmdb_movie['backdrop_path']}\"\n",
        "            backdrop_path = os.path.join(movie_folder, \"backdrop.jpg\")\n",
        "            if self.download_image(backdrop_url, backdrop_path):\n",
        "                downloaded.append((\"backdrop\", backdrop_path))\n",
        "        \n",
        "        # 3. Download stills (up to 8)\n",
        "        if \"images\" in tmdb_movie and \"backdrops\" in tmdb_movie[\"images\"]:\n",
        "            backdrops = tmdb_movie[\"images\"][\"backdrops\"][:8]\n",
        "            for i, backdrop_info in enumerate(backdrops):\n",
        "                backdrop_url = f\"{TMDB_IMAGE_BASE_URL}{backdrop_info['file_path']}\"\n",
        "                still_path = os.path.join(movie_folder, f\"still_{i+1}.jpg\")\n",
        "                if self.download_image(backdrop_url, still_path):\n",
        "                    downloaded.append((f\"still_{i+1}\", still_path))\n",
        "                    \n",
        "                # Limit to maximum of 10 images\n",
        "                if len(downloaded) >= IMAGES_PER_MOVIE:\n",
        "                    break\n",
        "        \n",
        "        return downloaded\n",
        "    \n",
        "    def extract_movie_features(self, tmdb_movie, movielens_info):\n",
        "        \"\"\"Extract comprehensive movie features\"\"\"\n",
        "        features = {\n",
        "            # MovieLens original information\n",
        "            \"movielens_id\": movielens_info[\"movie_id\"],\n",
        "            \"movielens_title\": movielens_info[\"title\"],\n",
        "            \"movielens_year\": movielens_info[\"year\"],\n",
        "            \"movielens_imdb_id\": movielens_info[\"imdb_id\"],\n",
        "            \n",
        "            # TMDB basic information\n",
        "            \"tmdb_id\": tmdb_movie.get(\"id\"),\n",
        "            \"title\": tmdb_movie.get(\"title\", \"\"),\n",
        "            \"original_title\": tmdb_movie.get(\"original_title\", \"\"),\n",
        "            \"overview\": tmdb_movie.get(\"overview\", \"\"),\n",
        "            \"tagline\": tmdb_movie.get(\"tagline\", \"\"),\n",
        "            \"release_date\": tmdb_movie.get(\"release_date\", \"\"),\n",
        "            \"runtime\": tmdb_movie.get(\"runtime\", 0),\n",
        "            \"status\": tmdb_movie.get(\"status\", \"\"),\n",
        "            \n",
        "            # Rating information\n",
        "            \"vote_average\": tmdb_movie.get(\"vote_average\", 0),\n",
        "            \"vote_count\": tmdb_movie.get(\"vote_count\", 0),\n",
        "            \"popularity\": tmdb_movie.get(\"popularity\", 0),\n",
        "            \n",
        "            # Classification information\n",
        "            \"genres\": \"|\".join([g[\"name\"] for g in tmdb_movie.get(\"genres\", [])]),\n",
        "            \"original_language\": tmdb_movie.get(\"original_language\", \"\"),\n",
        "            \"production_countries\": \"|\".join([c[\"name\"] for c in tmdb_movie.get(\"production_countries\", [])]),\n",
        "            \"production_companies\": \"|\".join([c[\"name\"] for c in tmdb_movie.get(\"production_companies\", [])]),\n",
        "            \n",
        "            # Financial information\n",
        "            \"budget\": tmdb_movie.get(\"budget\", 0),\n",
        "            \"revenue\": tmdb_movie.get(\"revenue\", 0),\n",
        "        }\n",
        "        \n",
        "        # Cast and crew information\n",
        "        if \"credits\" in tmdb_movie:\n",
        "            credits = tmdb_movie[\"credits\"]\n",
        "            \n",
        "            # Directors\n",
        "            directors = [crew[\"name\"] for crew in credits.get(\"crew\", []) if crew.get(\"job\") == \"Director\"]\n",
        "            features[\"directors\"] = \"|\".join(directors)\n",
        "            \n",
        "            # Cast\n",
        "            cast = [actor[\"name\"] for actor in credits.get(\"cast\", [])[:10]]\n",
        "            features[\"cast\"] = \"|\".join(cast)\n",
        "        \n",
        "        # Keywords\n",
        "        if \"keywords\" in tmdb_movie and \"keywords\" in tmdb_movie[\"keywords\"]:\n",
        "            keywords = [kw[\"name\"] for kw in tmdb_movie[\"keywords\"][\"keywords\"][:15]]\n",
        "            features[\"keywords\"] = \"|\".join(keywords)\n",
        "        \n",
        "        return features\n",
        "\n",
        "# Check progress status\n",
        "processor = EnhancedTMDBProcessor(TMDB_API_KEY)\n",
        "current_processed = len(processor.processed_movies)\n",
        "\n",
        "#print(f\"Current processing progress: {current_processed}/{TARGET_MOVIE_COUNT}\")\n",
        "\n",
        "# Intelligent progress management\n",
        "if current_processed >= TARGET_MOVIE_COUNT:\n",
        "    print(f\"Target achieved ({current_processed} >= {TARGET_MOVIE_COUNT})\")\n",
        "    print(\"Skipping download phase, using existing data\")\n",
        "    SKIP_DOWNLOAD = True\n",
        "else:\n",
        "    need_to_process = TARGET_MOVIE_COUNT - current_processed\n",
        "    print(f\"Remaining to process: {need_to_process} movies\")\n",
        "    SKIP_DOWNLOAD = False\n",
        "    \n",
        "print(\"\\nTMDB processor ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting TMDB data processing and image download\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:   0%|          | 0/42 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[1/42] 1. Toy Story -> tt0114709\n",
            "   Already processed, skipping\n",
            "\n",
            "[2/42] 2. GoldenEye -> tt0113189\n",
            "   Already processed, skipping\n",
            "\n",
            "[3/42] 3. Four Rooms -> tt0113101\n",
            "   Already processed, skipping\n",
            "\n",
            "[4/42] 4. Get Shorty -> tt0113161\n",
            "   Already processed, skipping\n",
            "\n",
            "[5/42] 5. Copycat -> tt0112722\n",
            "   Already processed, skipping\n",
            "\n",
            "[6/42] 6. Shanghai Triad (Yao a yao yao dao waipo qiao) -> tt0115012\n",
            "   Already processed, skipping\n",
            "\n",
            "[7/42] 7. Twelve Monkeys -> tt0114746\n",
            "   Already processed, skipping\n",
            "\n",
            "[8/42] 8. Babe -> tt0112431\n",
            "   Already processed, skipping\n",
            "\n",
            "[9/42] 9. Dead Man Walking -> tt0112818\n",
            "   Already processed, skipping\n",
            "\n",
            "[10/42] 10. Richard III -> tt0114279\n",
            "   Already processed, skipping\n",
            "\n",
            "[11/42] 11. Seven (Se7en) -> tt0114369\n",
            "   Already processed, skipping\n",
            "\n",
            "[12/42] 12. Usual Suspects, The -> tt0114814\n",
            "   Already processed, skipping\n",
            "\n",
            "[13/42] 13. Mighty Aphrodite -> tt0113819\n",
            "   Already processed, skipping\n",
            "\n",
            "[14/42] 14. Postino, Il -> tt0110877\n",
            "   Already processed, skipping\n",
            "\n",
            "[15/42] 15. Mr. Holland's Opus -> tt0113862\n",
            "   Already processed, skipping\n",
            "\n",
            "[16/42] 16. French Twist (Gazon maudit) -> tt0113149\n",
            "   Already processed, skipping\n",
            "\n",
            "[17/42] 17. From Dusk Till Dawn -> tt0116367\n",
            "   Already processed, skipping\n",
            "\n",
            "[18/42] 18. White Balloon, The -> tt0112445\n",
            "   Already processed, skipping\n",
            "\n",
            "[19/42] 19. Antonia's Line -> tt0112379\n",
            "   Already processed, skipping\n",
            "\n",
            "[20/42] 20. Angels and Insects -> tt0112365\n",
            "   Already processed, skipping\n",
            "\n",
            "[21/42] 21. Muppet Treasure Island -> tt0117110\n",
            "   Already processed, skipping\n",
            "\n",
            "[22/42] 22. Braveheart -> tt0112573\n",
            "   Already processed, skipping\n",
            "\n",
            "[23/42] 23. Taxi Driver -> tt0075314\n",
            "   Already processed, skipping\n",
            "\n",
            "[24/42] 24. Rumble in the Bronx -> tt0113326\n",
            "   Already processed, skipping\n",
            "\n",
            "[25/42] 25. Birdcage, The -> tt0115685\n",
            "   Already processed, skipping\n",
            "\n",
            "[26/42] 26. Brothers McMullen, The -> tt0112585\n",
            "   SUCCESS: 4 images, rating 6.049/10\n",
            "   Genres: Comedy|Drama|Romance\n",
            "   Directors: Edward Burns\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  62%|██████▏   | 26/42 [00:01<00:00, 18.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[27/42] 27. Bad Boys -> tt0112442\n",
            "   SUCCESS: 10 images, rating 6.822/10\n",
            "   Genres: Action|Comedy|Crime|Thriller\n",
            "   Directors: Michael Bay\n",
            "\n",
            "[28/42] 28. Apollo 13 -> tt0112384\n",
            "   SUCCESS: 10 images, rating 7.448/10\n",
            "   Genres: Drama|History\n",
            "   Directors: Ron Howard\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  67%|██████▋   | 28/42 [00:04<00:02,  5.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[29/42] 29. Batman Forever -> tt0112462\n",
            "   SUCCESS: 10 images, rating 5.441/10\n",
            "   Genres: Action|Crime|Fantasy\n",
            "   Directors: Joel Schumacher\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  69%|██████▉   | 29/42 [00:05<00:03,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[30/42] 30. Belle de jour -> tt0061395\n",
            "   SUCCESS: 10 images, rating 7.323/10\n",
            "   Genres: Drama|Romance\n",
            "   Directors: Luis Buñuel\n",
            "   Progress saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  71%|███████▏  | 30/42 [00:07<00:05,  2.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[31/42] 31. Crimson Tide -> tt0112740\n",
            "   SUCCESS: 10 images, rating 7.2/10\n",
            "   Genres: Thriller|Action|Drama|War\n",
            "   Directors: Tony Scott\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  74%|███████▍  | 31/42 [00:09<00:06,  1.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[32/42] 32. Crumb -> tt0109508\n",
            "   SUCCESS: 10 images, rating 7.5/10\n",
            "   Genres: Documentary\n",
            "   Directors: Terry Zwigoff\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  76%|███████▌  | 32/42 [00:12<00:08,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[33/42] 33. Desperado -> tt0112851\n",
            "   SUCCESS: 10 images, rating 6.929/10\n",
            "   Genres: Thriller|Action|Crime\n",
            "   Directors: Robert Rodriguez\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  79%|███████▊  | 33/42 [00:14<00:08,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[34/42] 34. Doom Generation, The -> tt0112887\n",
            "   SUCCESS: 10 images, rating 6.5/10\n",
            "   Genres: Comedy|Crime|Drama\n",
            "   Directors: Gregg Araki\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  81%|████████  | 34/42 [00:15<00:08,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[35/42] 35. Free Willy 2: The Adventure Home -> tt0113114\n",
            "   SUCCESS: 10 images, rating 5.9/10\n",
            "   Genres: Family|Adventure|Drama|Comedy\n",
            "   Directors: Dwight H. Little\n",
            "   Progress saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  83%|████████▎ | 35/42 [00:16<00:07,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[36/42] 36. Mad Love -> tt0113729\n",
            "   SUCCESS: 9 images, rating 5.211/10\n",
            "   Genres: Drama|Romance\n",
            "   Directors: Antonia Bird\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  86%|████████▌ | 36/42 [00:17<00:06,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[37/42] 37. Nadja -> tt0110620\n",
            "   SUCCESS: 6 images, rating 5.7/10\n",
            "   Genres: Horror|Thriller\n",
            "   Directors: Michael Almereyda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  88%|████████▊ | 37/42 [00:18<00:05,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[38/42] 38. Net, The -> tt0113957\n",
            "   SUCCESS: 10 images, rating 6.029/10\n",
            "   Genres: Crime|Drama|Mystery|Thriller|Action\n",
            "   Directors: Irwin Winkler\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  90%|█████████ | 38/42 [00:20<00:04,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[39/42] 39. Strange Days -> tt0114558\n",
            "   SUCCESS: 10 images, rating 7.011/10\n",
            "   Genres: Crime|Drama|Science Fiction|Thriller\n",
            "   Directors: Kathryn Bigelow\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  93%|█████████▎| 39/42 [00:21<00:03,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[40/42] 40. To Wong Foo, Thanks for Everything! Julie Newmar -> tt0114682\n",
            "   SUCCESS: 10 images, rating 7.374/10\n",
            "   Genres: Comedy|Drama\n",
            "   Directors: Beeban Kidron\n",
            "   Progress saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  95%|█████████▌| 40/42 [00:22<00:02,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[41/42] 41. Billy Madison -> tt0112508\n",
            "   SUCCESS: 9 images, rating 6.2/10\n",
            "   Genres: Comedy\n",
            "   Directors: Tamra Davis\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies:  98%|█████████▊| 41/42 [00:23<00:01,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[42/42] 42. Clerks -> tt0109445\n",
            "   SUCCESS: 10 images, rating 7.4/10\n",
            "   Genres: Comedy\n",
            "   Directors: Kevin Smith\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing movies: 100%|██████████| 42/42 [00:25<00:00,  1.67it/s]\n"
          ]
        },
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: 'multimodal_data\\\\tmdb_movie_features.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m     json.dump(all_movie_features, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     85\u001b[39m csv_file = os.path.join(DATA_DIR, \u001b[33m'\u001b[39m\u001b[33mtmdb_movie_features.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_movie_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMovie feature data saved:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   JSON format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'multimodal_data\\\\tmdb_movie_features.csv'"
          ]
        }
      ],
      "source": [
        "# Execute TMDB data acquisition and image download\n",
        "if not SKIP_DOWNLOAD and 'target_movies' in locals():\n",
        "    print(\"Starting TMDB data processing and image download\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_movie_features = []\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    total_images = 0\n",
        "    \n",
        "    # Process each movie\n",
        "    for idx, (_, movie_info) in enumerate(tqdm(target_movies.iterrows(), total=len(target_movies), desc=\"Processing movies\")):\n",
        "        movie_id = movie_info[\"movie_id\"]\n",
        "        title = movie_info[\"title\"]\n",
        "        imdb_id = movie_info[\"imdb_id\"]\n",
        "        \n",
        "        print(f\"\\n[{idx+1}/{len(target_movies)}] {movie_id}. {title} -> tt{imdb_id}\")\n",
        "        \n",
        "        # Check if already processed\n",
        "        if str(movie_id) in processor.processed_movies:\n",
        "            print(f\"   Already processed, skipping\")\n",
        "            if str(movie_id) in processor.movie_features:\n",
        "                all_movie_features.append(processor.movie_features[str(movie_id)])\n",
        "            success_count += 1\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # 1. Find TMDB movie\n",
        "            tmdb_basic = processor.find_movie_by_imdb_id(imdb_id)\n",
        "            if not tmdb_basic:\n",
        "                print(f\"   ERROR: Not found in TMDB\")\n",
        "                processor.failed_movies.add(str(movie_id))\n",
        "                error_count += 1\n",
        "                continue\n",
        "            \n",
        "            # 2. Get detailed information\n",
        "            tmdb_movie = processor.get_movie_details(tmdb_basic[\"id\"])\n",
        "            if not tmdb_movie:\n",
        "                print(f\"   ERROR: Failed to get details\")\n",
        "                processor.failed_movies.add(str(movie_id))\n",
        "                error_count += 1\n",
        "                continue\n",
        "            \n",
        "            # 3. Extract features\n",
        "            features = processor.extract_movie_features(tmdb_movie, movie_info)\n",
        "            \n",
        "            # 4. Download images\n",
        "            downloaded_images = processor.download_movie_images(movie_id, title, tmdb_movie)\n",
        "            features[\"downloaded_images_count\"] = len(downloaded_images)\n",
        "            features[\"image_paths\"] = [path for _, path in downloaded_images]\n",
        "            total_images += len(downloaded_images)\n",
        "            \n",
        "            # 5. Record success\n",
        "            all_movie_features.append(features)\n",
        "            processor.processed_movies.add(str(movie_id))\n",
        "            processor.movie_features[str(movie_id)] = features\n",
        "            success_count += 1\n",
        "            \n",
        "            print(f\"   SUCCESS: {len(downloaded_images)} images, rating {features['vote_average']}/10\")\n",
        "            print(f\"   Genres: {features['genres']}\")\n",
        "            print(f\"   Directors: {features.get('directors', 'N/A')}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Processing failed: {str(e)}\")\n",
        "            processor.failed_movies.add(str(movie_id))\n",
        "            error_count += 1\n",
        "        \n",
        "        # Save progress periodically\n",
        "        if (idx + 1) % 5 == 0:\n",
        "            processor.save_progress()\n",
        "            print(f\"   Progress saved\")\n",
        "        \n",
        "        # API request interval\n",
        "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "    \n",
        "    # Final progress save\n",
        "    processor.save_progress()\n",
        "    \n",
        "    # Save movie feature data\n",
        "    if all_movie_features:\n",
        "        features_file = os.path.join(DATA_DIR, 'tmdb_movie_features.json')\n",
        "        with open(features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        csv_file = os.path.join(DATA_DIR, 'tmdb_movie_features.csv')\n",
        "        pd.DataFrame(all_movie_features).to_csv(csv_file, index=False, encoding='utf-8')\n",
        "        \n",
        "        print(f\"\\nMovie feature data saved:\")\n",
        "        print(f\"   JSON format: {features_file}\")\n",
        "        print(f\"   CSV format: {csv_file}\")\n",
        "    \n",
        "    # Output statistics\n",
        "    print(f\"\\nTMDB processing statistics:\")\n",
        "    print(f\"   Successful: {success_count} movies\")\n",
        "    print(f\"   Failed: {error_count} movies\")\n",
        "    print(f\"   Images downloaded: {total_images}\")\n",
        "    print(f\"   Success rate: {success_count/(success_count+error_count)*100:.1f}%\")\n",
        "    print(f\"   Target achieved: {success_count >= TARGET_MOVIE_COUNT}\")\n",
        "\n",
        "else:\n",
        "    # Load existing data\n",
        "    features_file = os.path.join(DATA_DIR, 'tmdb_movie_features.json')\n",
        "    if os.path.exists(features_file):\n",
        "        with open(features_file, 'r', encoding='utf-8') as f:\n",
        "            all_movie_features = json.load(f)\n",
        "        print(f\"Existing movie feature data loaded: {len(all_movie_features)} movies\")\n",
        "    else:\n",
        "        print(\"ERROR: No existing movie feature data found\")\n",
        "        all_movie_features = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Similarity Calculation and Selection\n",
            "==================================================\n",
            "Starting image selection for 42 movies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   2%|▏         | 1/42 [00:00<00:04,  8.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 1. Toy Story: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   5%|▍         | 2/42 [00:00<00:04,  8.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 2. GoldenEye: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:   7%|▋         | 3/42 [00:00<00:04,  8.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 3. Four Rooms: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  10%|▉         | 4/42 [00:00<00:04,  8.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 4. Get Shorty: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  12%|█▏        | 5/42 [00:00<00:04,  9.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 5. Copycat: 10 -> 5 images\n",
            "   SUCCESS: 6. Shanghai Triad (Yao a yao yao dao waipo qiao): 7 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  17%|█▋        | 7/42 [00:00<00:03, 10.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 7. Twelve Monkeys: 10 -> 5 images\n",
            "   SUCCESS: 8. Babe: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  21%|██▏       | 9/42 [00:00<00:03,  9.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 9. Dead Man Walking: 10 -> 5 images\n",
            "   SUCCESS: 10. Richard III: 7 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  26%|██▌       | 11/42 [00:01<00:02, 10.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 11. Seven (Se7en): 10 -> 5 images\n",
            "   SUCCESS: 12. Usual Suspects, The: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  31%|███       | 13/42 [00:01<00:02,  9.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 13. Mighty Aphrodite: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  33%|███▎      | 14/42 [00:01<00:02,  9.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 14. Postino, Il: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  36%|███▌      | 15/42 [00:01<00:02,  9.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 15. Mr. Holland's Opus: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  38%|███▊      | 16/42 [00:01<00:02,  9.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 16. French Twist (Gazon maudit): 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  40%|████      | 17/42 [00:01<00:02,  9.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 17. From Dusk Till Dawn: 10 -> 5 images\n",
            "   SUCCESS: 18. White Balloon, The: 4 -> 4 images\n",
            "   SUCCESS: 19. Antonia's Line: 7 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  48%|████▊     | 20/42 [00:01<00:01, 12.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 20. Angels and Insects: 8 -> 5 images\n",
            "   SUCCESS: 21. Muppet Treasure Island: 10 -> 1 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  52%|█████▏    | 22/42 [00:02<00:01, 10.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 22. Braveheart: 10 -> 5 images\n",
            "   SUCCESS: 23. Taxi Driver: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  57%|█████▋    | 24/42 [00:02<00:01, 10.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 24. Rumble in the Bronx: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  62%|██████▏   | 26/42 [00:02<00:01, 11.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 25. Birdcage, The: 10 -> 5 images\n",
            "   SUCCESS: 26. Brothers McMullen, The: 4 -> 4 images\n",
            "   SUCCESS: 27. Bad Boys: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  67%|██████▋   | 28/42 [00:02<00:01, 10.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 28. Apollo 13: 10 -> 5 images\n",
            "   SUCCESS: 29. Batman Forever: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  71%|███████▏  | 30/42 [00:03<00:01,  9.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 30. Belle de jour: 10 -> 5 images\n",
            "   SUCCESS: 31. Crimson Tide: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  76%|███████▌  | 32/42 [00:03<00:01,  9.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 32. Crumb: 10 -> 5 images\n",
            "   SUCCESS: 33. Desperado: 10 -> 5 images\n",
            "   SUCCESS: 34. Doom Generation, The: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  86%|████████▌ | 36/42 [00:03<00:00,  8.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 35. Free Willy 2: The Adventure Home: 10 -> 1 images\n",
            "   SUCCESS: 36. Mad Love: 9 -> 5 images\n",
            "   SUCCESS: 37. Nadja: 6 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  93%|█████████▎| 39/42 [00:04<00:00,  8.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 38. Net, The: 10 -> 5 images\n",
            "   SUCCESS: 39. Strange Days: 10 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images:  98%|█████████▊| 41/42 [00:04<00:00,  7.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 40. To Wong Foo, Thanks for Everything! Julie Newmar: 10 -> 5 images\n",
            "   SUCCESS: 41. Billy Madison: 9 -> 5 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Selecting images: 100%|██████████| 42/42 [00:04<00:00,  9.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 42. Clerks: 10 -> 5 images\n",
            "\n",
            "Image selection statistics:\n",
            "   Movies processed: 42\n",
            "   Total selected images: 200\n",
            "   Average per movie: 4.8 images\n",
            "   Updated data saved: multimodal_data\\movie_features_with_selected_images.json\n",
            "\n",
            "Selection details:\n",
            "   Original images total: 391\n",
            "   Selected images total: 200\n",
            "   Selection rate: 51.2%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Image similarity calculation and selection - select 5 most diverse from 10\n",
        "import re\n",
        "print(\"Image Similarity Calculation and Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Remove or replace invalid characters for Windows file/folder names\"\"\"\n",
        "    # Replace invalid characters with underscore\n",
        "    invalid_chars = r'[<>:\"/\\\\|?*]'\n",
        "    sanitized = re.sub(invalid_chars, '_', filename)\n",
        "    \n",
        "    # Remove any trailing dots or spaces (also invalid in Windows)\n",
        "    sanitized = sanitized.rstrip('. ')\n",
        "    \n",
        "    # Ensure the name isn't empty after sanitization\n",
        "    if not sanitized.strip():\n",
        "        sanitized = \"unnamed\"\n",
        "    \n",
        "    return sanitized\n",
        "\n",
        "def calculate_image_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity between two images using histogram comparison\"\"\"\n",
        "    try:\n",
        "        # Read images\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "        \n",
        "        if img1 is None or img2 is None:\n",
        "            return 0.0\n",
        "        \n",
        "        # Convert to HSV color space\n",
        "        hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
        "        hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
        "        \n",
        "        # Calculate histograms\n",
        "        hist1 = cv2.calcHist([hsv1], [0, 1, 2], None, [50, 60, 60], [0, 180, 0, 256, 0, 256])\n",
        "        hist2 = cv2.calcHist([hsv2], [0, 1, 2], None, [50, 60, 60], [0, 180, 0, 256, 0, 256])\n",
        "        \n",
        "        # Calculate correlation\n",
        "        correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
        "        return correlation\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       Similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def select_diverse_images(image_paths, target_count=5):\n",
        "    \"\"\"Select the most diverse target_count images from the image list\"\"\"\n",
        "    if len(image_paths) <= target_count:\n",
        "        return image_paths\n",
        "    \n",
        "    # Calculate similarity matrix for all image pairs\n",
        "    n = len(image_paths)\n",
        "    similarity_matrix = np.zeros((n, n))\n",
        "    \n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            sim = calculate_image_similarity(image_paths[i], image_paths[j])\n",
        "            similarity_matrix[i][j] = sim\n",
        "            similarity_matrix[j][i] = sim\n",
        "    \n",
        "    # Greedy algorithm to select most dissimilar images\n",
        "    selected_indices = [0]  # Start with first image\n",
        "    \n",
        "    for _ in range(target_count - 1):\n",
        "        max_min_sim = -1\n",
        "        best_candidate = -1\n",
        "        \n",
        "        for candidate in range(n):\n",
        "            if candidate in selected_indices:\n",
        "                continue\n",
        "            \n",
        "            # Calculate minimum similarity with already selected images\n",
        "            min_sim = min(similarity_matrix[candidate][selected] for selected in selected_indices)\n",
        "            \n",
        "            if min_sim > max_min_sim:\n",
        "                max_min_sim = min_sim\n",
        "                best_candidate = candidate\n",
        "        \n",
        "        if best_candidate != -1:\n",
        "            selected_indices.append(best_candidate)\n",
        "    \n",
        "    return [image_paths[i] for i in selected_indices]\n",
        "\n",
        "# Filter images for each movie\n",
        "if 'all_movie_features' in locals() and all_movie_features:\n",
        "    print(f\"Starting image selection for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    selected_image_stats = []\n",
        "    total_selected = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Selecting images\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        \n",
        "        # Get image paths\n",
        "        image_paths = movie_features.get('image_paths', [])\n",
        "        \n",
        "        if not image_paths:\n",
        "            print(f\"   WARNING: {movie_id}. {title}: No images found\")\n",
        "            continue\n",
        "        \n",
        "        # Verify image files exist\n",
        "        valid_paths = [path for path in image_paths if os.path.exists(path)]\n",
        "        \n",
        "        if len(valid_paths) == 0:\n",
        "            print(f\"   ERROR: {movie_id}. {title}: Image files do not exist\")\n",
        "            continue\n",
        "        \n",
        "        # Select most diverse images\n",
        "        selected_paths = select_diverse_images(valid_paths, SELECTED_IMAGES)\n",
        "        \n",
        "        # Create selected image directory and copy images\n",
        "        # FIXED: Sanitize the title to remove invalid characters\n",
        "        sanitized_title = sanitize_filename(title[:50])\n",
        "        selected_folder = os.path.join(SELECTED_IMAGE_DIR, f\"{movie_id:04d}_{sanitized_title}\")\n",
        "        \n",
        "        try:\n",
        "            os.makedirs(selected_folder, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Failed to create directory for {movie_id}. {title}: {str(e)}\")\n",
        "            # Fallback: use only movie ID as folder name\n",
        "            selected_folder = os.path.join(SELECTED_IMAGE_DIR, f\"{movie_id:04d}\")\n",
        "            os.makedirs(selected_folder, exist_ok=True)\n",
        "        \n",
        "        copied_paths = []\n",
        "        for i, src_path in enumerate(selected_paths):\n",
        "            filename = f\"selected_{i+1}.jpg\"\n",
        "            dst_path = os.path.join(selected_folder, filename)\n",
        "            \n",
        "            try:\n",
        "                # Copy image\n",
        "                img = Image.open(src_path)\n",
        "                img.save(dst_path, \"JPEG\", quality=90)\n",
        "                copied_paths.append(dst_path)\n",
        "            except Exception as e:\n",
        "                print(f\"     ERROR: Failed to copy image: {str(e)}\")\n",
        "        \n",
        "        # Update feature data\n",
        "        movie_features['selected_image_paths'] = copied_paths\n",
        "        movie_features['selected_images_count'] = len(copied_paths)\n",
        "        \n",
        "        selected_image_stats.append({\n",
        "            'movie_id': movie_id,\n",
        "            'title': title,\n",
        "            'original_count': len(valid_paths),\n",
        "            'selected_count': len(copied_paths)\n",
        "        })\n",
        "        \n",
        "        total_selected += len(copied_paths)\n",
        "        \n",
        "        print(f\"   SUCCESS: {movie_id}. {title}: {len(valid_paths)} -> {len(copied_paths)} images\")\n",
        "    \n",
        "    # Save updated feature data\n",
        "    updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_selected_images.json')\n",
        "    with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"\\nImage selection statistics:\")\n",
        "    print(f\"   Movies processed: {len(selected_image_stats)}\")\n",
        "    print(f\"   Total selected images: {total_selected}\")\n",
        "    print(f\"   Average per movie: {total_selected/len(selected_image_stats):.1f} images\")\n",
        "    print(f\"   Updated data saved: {updated_features_file}\")\n",
        "    \n",
        "    # Display selection details\n",
        "    stats_df = pd.DataFrame(selected_image_stats)\n",
        "    print(f\"\\nSelection details:\")\n",
        "    print(f\"   Original images total: {stats_df['original_count'].sum()}\")\n",
        "    print(f\"   Selected images total: {stats_df['selected_count'].sum()}\")\n",
        "    print(f\"   Selection rate: {stats_df['selected_count'].sum()/stats_df['original_count'].sum()*100:.1f}%\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: No movie feature data available for image selection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT Image Feature Extraction\n",
            "==================================================\n",
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViT model loaded successfully: google/vit-base-patch16-224\n",
            "\n",
            "Starting ViT feature extraction for 25 movies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:   4%|▍         | 1/25 [00:00<00:10,  2.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 1. Toy Story: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:   8%|▊         | 2/25 [00:00<00:08,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 2. GoldenEye: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  12%|█▏        | 3/25 [00:01<00:08,  2.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 3. Four Rooms: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  16%|█▌        | 4/25 [00:01<00:07,  2.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 4. Get Shorty: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  20%|██        | 5/25 [00:01<00:07,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 5. Copycat: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  24%|██▍       | 6/25 [00:02<00:06,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 6. Shanghai Triad (Yao a yao yao dao waipo qiao): feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  28%|██▊       | 7/25 [00:02<00:06,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 7. Twelve Monkeys: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  32%|███▏      | 8/25 [00:02<00:06,  2.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 8. Babe: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  36%|███▌      | 9/25 [00:03<00:05,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 9. Dead Man Walking: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  40%|████      | 10/25 [00:03<00:05,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 10. Richard III: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  44%|████▍     | 11/25 [00:04<00:05,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 11. Seven (Se7en): feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  48%|████▊     | 12/25 [00:04<00:04,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 12. Usual Suspects, The: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  52%|█████▏    | 13/25 [00:04<00:04,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 13. Mighty Aphrodite: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  56%|█████▌    | 14/25 [00:05<00:03,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 14. Postino, Il: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  60%|██████    | 15/25 [00:05<00:03,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 15. Mr. Holland's Opus: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  64%|██████▍   | 16/25 [00:05<00:03,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 16. French Twist (Gazon maudit): feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  68%|██████▊   | 17/25 [00:06<00:02,  2.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 17. From Dusk Till Dawn: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  72%|███████▏  | 18/25 [00:06<00:02,  3.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 18. White Balloon, The: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  76%|███████▌  | 19/25 [00:06<00:02,  2.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 19. Antonia's Line: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  80%|████████  | 20/25 [00:07<00:01,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 20. Angels and Insects: feature dimension 768\n",
            "   SUCCESS: 21. Muppet Treasure Island: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  88%|████████▊ | 22/25 [00:07<00:00,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 22. Braveheart: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  92%|█████████▏| 23/25 [00:07<00:00,  3.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 23. Taxi Driver: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features:  96%|█████████▌| 24/25 [00:08<00:00,  3.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 24. Rumble in the Bronx: feature dimension 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ViT features: 100%|██████████| 25/25 [00:08<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   SUCCESS: 25. Birdcage, The: feature dimension 768\n",
            "\n",
            "ViT feature extraction statistics:\n",
            "   Successfully processed movies: 25\n",
            "   Feature dimension: 768\n",
            "   Feature matrix shape: (25, 768)\n",
            "   Feature matrix saved: multimodal_data\\vit_features_matrix.npy\n",
            "   Mapping file saved: multimodal_data\\vit_movie_mapping.json\n",
            "   Updated feature data saved: multimodal_data\\movie_features_with_vit.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ViT image feature extraction\n",
        "print(\"ViT Image Feature Extraction\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class ViTFeatureExtractor:\n",
        "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Load ViT model and processor\n",
        "            self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "            self.model = ViTModel.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"ViT model loaded successfully: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: ViT model loading failed: {str(e)}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def extract_image_features(self, image_path):\n",
        "        \"\"\"Extract ViT features from a single image\"\"\"\n",
        "        if self.model is None:\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use [CLS] token features as image representation\n",
        "                image_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
        "            \n",
        "            return image_features\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Feature extraction failed ({image_path}): {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_movie_features(self, image_paths):\n",
        "        \"\"\"Extract and fuse features from all movie images\"\"\"\n",
        "        if not image_paths:\n",
        "            return None\n",
        "        \n",
        "        features_list = []\n",
        "        \n",
        "        for image_path in image_paths:\n",
        "            if os.path.exists(image_path):\n",
        "                features = self.extract_image_features(image_path)\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "        \n",
        "        if not features_list:\n",
        "            return None\n",
        "        \n",
        "        # Fuse multiple image features (average pooling)\n",
        "        combined_features = np.mean(features_list, axis=0)\n",
        "        return combined_features\n",
        "\n",
        "# Initialize ViT feature extractor\n",
        "vit_extractor = ViTFeatureExtractor()\n",
        "\n",
        "# Extract ViT features for all movies\n",
        "if 'all_movie_features' in locals() and all_movie_features and vit_extractor.model is not None:\n",
        "    print(f\"\\nStarting ViT feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    vit_features_matrix = []\n",
        "    vit_movie_ids = []\n",
        "    success_count = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Extracting ViT features\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        \n",
        "        # Use selected images\n",
        "        selected_paths = movie_features.get('selected_image_paths', [])\n",
        "        \n",
        "        if not selected_paths:\n",
        "            print(f\"   WARNING: {movie_id}. {title}: No selected images\")\n",
        "            continue\n",
        "        \n",
        "        # Extract features\n",
        "        vit_features = vit_extractor.extract_movie_features(selected_paths)\n",
        "        \n",
        "        if vit_features is not None:\n",
        "            vit_features_matrix.append(vit_features)\n",
        "            vit_movie_ids.append(movie_id)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            movie_features['vit_features'] = vit_features.tolist()\n",
        "            movie_features['vit_feature_dim'] = len(vit_features)\n",
        "            \n",
        "            success_count += 1\n",
        "            print(f\"   SUCCESS: {movie_id}. {title}: feature dimension {len(vit_features)}\")\n",
        "        else:\n",
        "            print(f\"   ERROR: {movie_id}. {title}: ViT feature extraction failed\")\n",
        "    \n",
        "    # Convert to numpy array and save\n",
        "    if vit_features_matrix:\n",
        "        vit_features_array = np.array(vit_features_matrix)\n",
        "        \n",
        "        # Save ViT feature matrix\n",
        "        vit_features_file = os.path.join(DATA_DIR, 'vit_features_matrix.npy')\n",
        "        np.save(vit_features_file, vit_features_array)\n",
        "        \n",
        "        # Save movie ID mapping\n",
        "        vit_mapping_file = os.path.join(DATA_DIR, 'vit_movie_mapping.json')\n",
        "        with open(vit_mapping_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({'movie_ids': vit_movie_ids, 'feature_dim': len(vit_features_matrix[0])}, f)\n",
        "        \n",
        "        print(f\"\\nViT feature extraction statistics:\")\n",
        "        print(f\"   Successfully processed movies: {success_count}\")\n",
        "        print(f\"   Feature dimension: {vit_features_array.shape[1]}\")\n",
        "        print(f\"   Feature matrix shape: {vit_features_array.shape}\")\n",
        "        print(f\"   Feature matrix saved: {vit_features_file}\")\n",
        "        print(f\"   Mapping file saved: {vit_mapping_file}\")\n",
        "        \n",
        "        # Save updated movie features\n",
        "        updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_vit.json')\n",
        "        with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"   Updated feature data saved: {updated_features_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: No ViT features extracted successfully\")\n",
        "else:\n",
        "    print(\"ERROR: ViT model not loaded or no movie data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT Text Feature Extraction\n",
            "==================================================\n",
            "Using device: cpu\n",
            "BERT model loaded successfully: bert-base-uncased\n",
            "\n",
            "Starting BERT text feature extraction for 25 movies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:   0%|          | 0/25 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Processing 1. Toy Story\n",
            "     Overview length: 303 characters\n",
            "     Tagline length: 47 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:   4%|▍         | 1/25 [00:00<00:07,  3.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 2. GoldenEye\n",
            "     Overview length: 371 characters\n",
            "     Tagline length: 36 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:   8%|▊         | 2/25 [00:00<00:06,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 3. Four Rooms\n",
            "     Overview length: 237 characters\n",
            "     Tagline length: 155 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  12%|█▏        | 3/25 [00:00<00:05,  3.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 4. Get Shorty\n",
            "     Overview length: 367 characters\n",
            "     Tagline length: 22 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  16%|█▌        | 4/25 [00:01<00:05,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 5. Copycat\n",
            "     Overview length: 139 characters\n",
            "     Tagline length: 142 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  20%|██        | 5/25 [00:01<00:04,  4.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 6. Shanghai Triad (Yao a yao yao dao waipo qiao)\n",
            "     Overview length: 271 characters\n",
            "     Tagline length: 68 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  24%|██▍       | 6/25 [00:01<00:04,  4.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 7. Twelve Monkeys\n",
            "     Overview length: 536 characters\n",
            "     Tagline length: 22 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  28%|██▊       | 7/25 [00:01<00:04,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 8. Babe\n",
            "     Overview length: 383 characters\n",
            "     Tagline length: 29 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  36%|███▌      | 9/25 [00:02<00:03,  5.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 9. Dead Man Walking\n",
            "     Overview length: 147 characters\n",
            "     Tagline length: 0 characters\n",
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 10. Richard III\n",
            "     Overview length: 442 characters\n",
            "     Tagline length: 37 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  40%|████      | 10/25 [00:02<00:03,  4.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 11. Seven (Se7en)\n",
            "     Overview length: 389 characters\n",
            "     Tagline length: 37 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  44%|████▍     | 11/25 [00:02<00:02,  4.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 12. Usual Suspects, The\n",
            "     Overview length: 409 characters\n",
            "     Tagline length: 44 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  48%|████▊     | 12/25 [00:02<00:02,  4.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 13. Mighty Aphrodite\n",
            "     Overview length: 419 characters\n",
            "     Tagline length: 75 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  52%|█████▏    | 13/25 [00:02<00:02,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 14. Postino, Il\n",
            "     Overview length: 127 characters\n",
            "     Tagline length: 20 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  56%|█████▌    | 14/25 [00:03<00:02,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 15. Mr. Holland's Opus\n",
            "     Overview length: 340 characters\n",
            "     Tagline length: 71 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  60%|██████    | 15/25 [00:03<00:02,  4.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 16. French Twist (Gazon maudit)\n",
            "     Overview length: 157 characters\n",
            "     Tagline length: 58 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  64%|██████▍   | 16/25 [00:03<00:02,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 17. From Dusk Till Dawn\n",
            "     Overview length: 163 characters\n",
            "     Tagline length: 94 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  72%|███████▏  | 18/25 [00:04<00:01,  5.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 18. White Balloon, The\n",
            "     Overview length: 125 characters\n",
            "     Tagline length: 0 characters\n",
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 19. Antonia's Line\n",
            "     Overview length: 424 characters\n",
            "     Tagline length: 64 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  76%|███████▌  | 19/25 [00:04<00:01,  4.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 20. Angels and Insects\n",
            "     Overview length: 438 characters\n",
            "     Tagline length: 65 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  80%|████████  | 20/25 [00:04<00:01,  4.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 21. Muppet Treasure Island\n",
            "     Overview length: 397 characters\n",
            "     Tagline length: 27 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  84%|████████▍ | 21/25 [00:04<00:00,  4.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 22. Braveheart\n",
            "     Overview length: 258 characters\n",
            "     Tagline length: 43 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  88%|████████▊ | 22/25 [00:04<00:00,  4.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 23. Taxi Driver\n",
            "     Overview length: 165 characters\n",
            "     Tagline length: 143 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  92%|█████████▏| 23/25 [00:05<00:00,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 24. Rumble in the Bronx\n",
            "     Overview length: 397 characters\n",
            "     Tagline length: 31 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features:  96%|█████████▌| 24/25 [00:05<00:00,  4.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "   Processing 25. Birdcage, The\n",
            "     Overview length: 567 characters\n",
            "     Tagline length: 16 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting BERT features: 100%|██████████| 25/25 [00:05<00:00,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     SUCCESS: BERT feature dimension: 768\n",
            "\n",
            "BERT feature extraction statistics:\n",
            "   Successfully processed movies: 25\n",
            "   Feature dimension: 768\n",
            "   Feature matrix shape: (25, 768)\n",
            "   Feature matrix saved: multimodal_data\\bert_features_matrix.npy\n",
            "   Mapping file saved: multimodal_data\\bert_movie_mapping.json\n",
            "   Updated feature data saved: multimodal_data\\movie_features_with_bert.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# BERT text feature extraction\n",
        "print(\"BERT Text Feature Extraction\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class BERTFeatureExtractor:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Load BERT model and tokenizer\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "            self.model = BertModel.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"BERT model loaded successfully: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: BERT model loading failed: {str(e)}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def extract_text_features(self, text, max_length=512):\n",
        "        \"\"\"Extract BERT features from text\"\"\"\n",
        "        if self.model is None or not text or text.strip() == \"\":\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Text preprocessing and tokenization\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            \n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use [CLS] token features as sentence representation\n",
        "                text_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
        "            \n",
        "            return text_features\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Text feature extraction failed: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_movie_text_features(self, overview, tagline):\n",
        "        \"\"\"Extract movie text features (overview + tagline)\"\"\"\n",
        "        features_list = []\n",
        "        \n",
        "        # Extract overview features\n",
        "        if overview and overview.strip():\n",
        "            overview_features = self.extract_text_features(overview)\n",
        "            if overview_features is not None:\n",
        "                features_list.append(overview_features)\n",
        "        \n",
        "        # Extract tagline features\n",
        "        if tagline and tagline.strip():\n",
        "            tagline_features = self.extract_text_features(tagline)\n",
        "            if tagline_features is not None:\n",
        "                features_list.append(tagline_features)\n",
        "        \n",
        "        if not features_list:\n",
        "            return None\n",
        "        \n",
        "        # Average pooling to fuse features\n",
        "        combined_features = np.mean(features_list, axis=0)\n",
        "        return combined_features\n",
        "\n",
        "# Initialize BERT feature extractor\n",
        "bert_extractor = BERTFeatureExtractor()\n",
        "\n",
        "# Extract BERT text features for all movies\n",
        "if 'all_movie_features' in locals() and all_movie_features and bert_extractor.model is not None:\n",
        "    print(f\"\\nStarting BERT text feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    bert_features_matrix = []\n",
        "    bert_movie_ids = []\n",
        "    success_count = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Extracting BERT features\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        overview = movie_features.get('overview', '')\n",
        "        tagline = movie_features.get('tagline', '')\n",
        "        \n",
        "        print(f\"   Processing {movie_id}. {title}\")\n",
        "        print(f\"     Overview length: {len(overview) if overview else 0} characters\")\n",
        "        print(f\"     Tagline length: {len(tagline) if tagline else 0} characters\")\n",
        "        \n",
        "        # Extract text features\n",
        "        bert_features = bert_extractor.extract_movie_text_features(overview, tagline)\n",
        "        \n",
        "        if bert_features is not None:\n",
        "            bert_features_matrix.append(bert_features)\n",
        "            bert_movie_ids.append(movie_id)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            movie_features['bert_features'] = bert_features.tolist()\n",
        "            movie_features['bert_feature_dim'] = len(bert_features)\n",
        "            movie_features['text_length'] = len(overview) + len(tagline)\n",
        "            \n",
        "            success_count += 1\n",
        "            print(f\"     SUCCESS: BERT feature dimension: {len(bert_features)}\")\n",
        "        else:\n",
        "            print(f\"     ERROR: BERT feature extraction failed\")\n",
        "    \n",
        "    # Convert to numpy array and save\n",
        "    if bert_features_matrix:\n",
        "        bert_features_array = np.array(bert_features_matrix)\n",
        "        \n",
        "        # Save BERT feature matrix\n",
        "        bert_features_file = os.path.join(DATA_DIR, 'bert_features_matrix.npy')\n",
        "        np.save(bert_features_file, bert_features_array)\n",
        "        \n",
        "        # Save movie ID mapping\n",
        "        bert_mapping_file = os.path.join(DATA_DIR, 'bert_movie_mapping.json')\n",
        "        with open(bert_mapping_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({'movie_ids': bert_movie_ids, 'feature_dim': len(bert_features_matrix[0])}, f)\n",
        "        \n",
        "        print(f\"\\nBERT feature extraction statistics:\")\n",
        "        print(f\"   Successfully processed movies: {success_count}\")\n",
        "        print(f\"   Feature dimension: {bert_features_array.shape[1]}\")\n",
        "        print(f\"   Feature matrix shape: {bert_features_array.shape}\")\n",
        "        print(f\"   Feature matrix saved: {bert_features_file}\")\n",
        "        print(f\"   Mapping file saved: {bert_mapping_file}\")\n",
        "        \n",
        "        # Save updated movie features\n",
        "        updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_bert.json')\n",
        "        with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"   Updated feature data saved: {updated_features_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: No BERT features extracted successfully\")\n",
        "else:\n",
        "    print(\"ERROR: BERT model not loaded or no movie data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cast & Crew Statistical Feature Engineering\n",
            "==================================================\n",
            "Starting cast & crew feature extraction for 25 movies\n",
            "Cast & crew statistics:\n",
            "   Total directors: 24 people\n",
            "   Total cast: 232 people\n",
            "   Selected high-frequency directors: 24 people\n",
            "   Selected high-frequency cast: 50 people\n",
            "\n",
            "TOP 10 high-frequency directors:\n",
            "    1. Martin Campbell: 2 movies\n",
            "    2. John Lasseter: 1 movies\n",
            "    3. Barry Sonnenfeld: 1 movies\n",
            "    4. Jon Amiel: 1 movies\n",
            "    5. Zhang Yimou: 1 movies\n",
            "    6. Terry Gilliam: 1 movies\n",
            "    7. Chris Noonan: 1 movies\n",
            "    8. Tim Robbins: 1 movies\n",
            "    9. Richard Loncraine: 1 movies\n",
            "   10. David Fincher: 1 movies\n",
            "\n",
            "TOP 10 high-frequency cast:\n",
            "    1. Pierce Brosnan: 2 movies\n",
            "    2. Sean Bean: 2 movies\n",
            "    3. Izabella Scorupco: 2 movies\n",
            "    4. Famke Janssen: 2 movies\n",
            "    5. Joe Don Baker: 2 movies\n",
            "    6. Judi Dench: 2 movies\n",
            "    7. Robbie Coltrane: 2 movies\n",
            "    8. Tchéky Karyo: 2 movies\n",
            "    9. Gottfried John: 2 movies\n",
            "   10. Alan Cumming: 2 movies\n",
            "\n",
            "Cast & crew feature statistics:\n",
            "   Movies processed: 25\n",
            "   Director feature dimension: 24\n",
            "   Cast feature dimension: 50\n",
            "   Total feature dimension: 74\n",
            "   Feature matrix shape: (25, 74)\n",
            "   Feature matrix saved: multimodal_data\\cast_crew_features.npy\n",
            "   Mapping file saved: multimodal_data\\cast_crew_mapping.json\n",
            "   Feature sparsity: 95.08%\n",
            "   Average high-frequency cast & crew per movie: 3.6\n",
            "   Complete feature data saved: multimodal_data\\movie_features_complete.json\n"
          ]
        }
      ],
      "source": [
        "# Cast & crew statistical feature engineering\n",
        "print(\"Cast & Crew Statistical Feature Engineering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def extract_cast_crew_features(all_movie_features, top_n=50):\n",
        "    \"\"\"Extract cast & crew statistical features\"\"\"\n",
        "    if not all_movie_features:\n",
        "        return None, None, None\n",
        "    \n",
        "    # Count frequency of all directors and cast\n",
        "    all_directors = []\n",
        "    all_cast = []\n",
        "    \n",
        "    for movie in all_movie_features:\n",
        "        # Directors\n",
        "        directors = movie.get('directors', '')\n",
        "        if directors:\n",
        "            all_directors.extend([d.strip() for d in directors.split('|') if d.strip()])\n",
        "        \n",
        "        # Cast\n",
        "        cast = movie.get('cast', '')\n",
        "        if cast:\n",
        "            all_cast.extend([c.strip() for c in cast.split('|') if c.strip()])\n",
        "    \n",
        "    # Count frequencies\n",
        "    director_counts = Counter(all_directors)\n",
        "    cast_counts = Counter(all_cast)\n",
        "    \n",
        "    # Get high-frequency cast & crew\n",
        "    top_directors = [director for director, count in director_counts.most_common(top_n)]\n",
        "    top_cast = [actor for actor, count in cast_counts.most_common(top_n)]\n",
        "    \n",
        "    print(f\"Cast & crew statistics:\")\n",
        "    print(f\"   Total directors: {len(director_counts)} people\")\n",
        "    print(f\"   Total cast: {len(cast_counts)} people\")\n",
        "    print(f\"   Selected high-frequency directors: {len(top_directors)} people\")\n",
        "    print(f\"   Selected high-frequency cast: {len(top_cast)} people\")\n",
        "    \n",
        "    # Display TOP 10 directors and cast\n",
        "    print(f\"\\nTOP 10 high-frequency directors:\")\n",
        "    for i, (director, count) in enumerate(director_counts.most_common(10), 1):\n",
        "        print(f\"   {i:2d}. {director}: {count} movies\")\n",
        "    \n",
        "    print(f\"\\nTOP 10 high-frequency cast:\")\n",
        "    for i, (actor, count) in enumerate(cast_counts.most_common(10), 1):\n",
        "        print(f\"   {i:2d}. {actor}: {count} movies\")\n",
        "    \n",
        "    return top_directors, top_cast, (director_counts, cast_counts)\n",
        "\n",
        "def create_cast_crew_features(all_movie_features, top_directors, top_cast):\n",
        "    \"\"\"Create cast & crew feature vectors for each movie\"\"\"\n",
        "    if not all_movie_features or not top_directors or not top_cast:\n",
        "        return None, None\n",
        "    \n",
        "    cast_crew_matrix = []\n",
        "    movie_ids = []\n",
        "    \n",
        "    for movie in all_movie_features:\n",
        "        movie_id = movie['movielens_id']\n",
        "        \n",
        "        # Director features (one-hot encoding)\n",
        "        director_features = np.zeros(len(top_directors))\n",
        "        directors = movie.get('directors', '')\n",
        "        if directors:\n",
        "            movie_directors = [d.strip() for d in directors.split('|') if d.strip()]\n",
        "            for i, top_director in enumerate(top_directors):\n",
        "                if top_director in movie_directors:\n",
        "                    director_features[i] = 1\n",
        "        \n",
        "        # Cast features (one-hot encoding)\n",
        "        cast_features = np.zeros(len(top_cast))\n",
        "        cast = movie.get('cast', '')\n",
        "        if cast:\n",
        "            movie_cast = [c.strip() for c in cast.split('|') if c.strip()]\n",
        "            for i, top_actor in enumerate(top_cast):\n",
        "                if top_actor in movie_cast:\n",
        "                    cast_features[i] = 1\n",
        "        \n",
        "        # Combine features\n",
        "        combined_features = np.concatenate([director_features, cast_features])\n",
        "        cast_crew_matrix.append(combined_features)\n",
        "        movie_ids.append(movie_id)\n",
        "    \n",
        "    return np.array(cast_crew_matrix), movie_ids\n",
        "\n",
        "# Execute cast & crew feature engineering\n",
        "if 'all_movie_features' in locals() and all_movie_features:\n",
        "    print(f\"Starting cast & crew feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    # Extract high-frequency cast & crew\n",
        "    top_directors, top_cast, (director_counts, cast_counts) = extract_cast_crew_features(all_movie_features)\n",
        "    \n",
        "    if top_directors and top_cast:\n",
        "        # Create cast & crew feature matrix\n",
        "        cast_crew_matrix, cc_movie_ids = create_cast_crew_features(\n",
        "            all_movie_features, top_directors, top_cast\n",
        "        )\n",
        "        \n",
        "        if cast_crew_matrix is not None:\n",
        "            # Save cast & crew features\n",
        "            cast_crew_file = os.path.join(DATA_DIR, 'cast_crew_features.npy')\n",
        "            np.save(cast_crew_file, cast_crew_matrix)\n",
        "            \n",
        "            # Save cast & crew mapping information\n",
        "            cast_crew_mapping = {\n",
        "                'movie_ids': cc_movie_ids,\n",
        "                'top_directors': top_directors,\n",
        "                'top_cast': top_cast,\n",
        "                'director_feature_dim': len(top_directors),\n",
        "                'cast_feature_dim': len(top_cast),\n",
        "                'total_feature_dim': len(top_directors) + len(top_cast)\n",
        "            }\n",
        "            \n",
        "            mapping_file = os.path.join(DATA_DIR, 'cast_crew_mapping.json')\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(cast_crew_mapping, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            for i, movie in enumerate(all_movie_features):\n",
        "                if movie['movielens_id'] in cc_movie_ids:\n",
        "                    idx = cc_movie_ids.index(movie['movielens_id'])\n",
        "                    movie['cast_crew_features'] = cast_crew_matrix[idx].tolist()\n",
        "                    movie['cast_crew_feature_dim'] = len(cast_crew_matrix[idx])\n",
        "            \n",
        "            print(f\"\\nCast & crew feature statistics:\")\n",
        "            print(f\"   Movies processed: {len(cc_movie_ids)}\")\n",
        "            print(f\"   Director feature dimension: {len(top_directors)}\")\n",
        "            print(f\"   Cast feature dimension: {len(top_cast)}\")\n",
        "            print(f\"   Total feature dimension: {cast_crew_matrix.shape[1]}\")\n",
        "            print(f\"   Feature matrix shape: {cast_crew_matrix.shape}\")\n",
        "            print(f\"   Feature matrix saved: {cast_crew_file}\")\n",
        "            print(f\"   Mapping file saved: {mapping_file}\")\n",
        "            \n",
        "            # Analyze feature sparsity\n",
        "            sparsity = 1 - np.count_nonzero(cast_crew_matrix) / cast_crew_matrix.size\n",
        "            print(f\"   Feature sparsity: {sparsity*100:.2f}%\")\n",
        "            print(f\"   Average high-frequency cast & crew per movie: {np.mean(np.sum(cast_crew_matrix, axis=1)):.1f}\")\n",
        "            \n",
        "            # Save final movie feature data\n",
        "            final_features_file = os.path.join(DATA_DIR, 'movie_features_complete.json')\n",
        "            with open(final_features_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"   Complete feature data saved: {final_features_file}\")\n",
        "        else:\n",
        "            print(\"ERROR: Cast & crew feature matrix creation failed\")\n",
        "    else:\n",
        "        print(\"ERROR: High-frequency cast & crew extraction failed\")\n",
        "else:\n",
        "    print(\"ERROR: No movie feature data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multimodal Feature Fusion\n",
            "==================================================\n",
            "Starting multimodal feature fusion\n",
            "ViT features loaded: (25, 768) (standardized)\n",
            "BERT features loaded: (25, 768) (standardized)\n",
            "Cast & crew features loaded: (25, 74) (one-hot encoded)\n",
            "\n",
            "Using concatenate method for feature fusion\n",
            "Feature intersection statistics:\n",
            "   vit: 25 movies\n",
            "   bert: 25 movies\n",
            "   cast_crew: 25 movies\n",
            "   Intersection: 25 movies\n",
            "   vit: (25, 768)\n",
            "   bert: (25, 768)\n",
            "   cast_crew: (25, 74)\n",
            "\n",
            "Concatenation fusion result: (25, 1610)\n",
            "   concatenate fused features saved: multimodal_data\\multimodal_features_concatenate.npy\n",
            "   Mapping file saved: multimodal_data\\multimodal_mapping_concatenate.json\n",
            "\n",
            "Using weighted_average method for feature fusion\n",
            "Feature intersection statistics:\n",
            "   vit: 25 movies\n",
            "   bert: 25 movies\n",
            "   cast_crew: 25 movies\n",
            "   Intersection: 25 movies\n",
            "   vit: (25, 768)\n",
            "   bert: (25, 768)\n",
            "   cast_crew: (25, 74)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "n_components=74 must be between 0 and min(n_samples, n_features)=25 with svd_solver='full'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m fusion_methods:\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m method for feature fusion\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     multimodal_features, movie_ids = \u001b[43mcreate_multimodal_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multimodal_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m         multimodal_results[method] = {\n\u001b[32m    164\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: multimodal_features,\n\u001b[32m    165\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mmovie_ids\u001b[39m\u001b[33m'\u001b[39m: movie_ids\n\u001b[32m    166\u001b[39m         }\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mcreate_multimodal_features\u001b[39m\u001b[34m(features_data, fusion_method)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature.shape[\u001b[32m1\u001b[39m] > target_dim:\n\u001b[32m    130\u001b[39m     pca = PCA(n_components=target_dim)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     reduced_feature = \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     reduced_features.append(reduced_feature)\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduced_feature.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (PCA)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:466\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    445\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:540\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcovariance_eigh\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_truncated(X, n_components, xp)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mz:\\Document\\MyProject\\3033-061\\.conda\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:554\u001b[39m, in \u001b[36mPCA._fit_full\u001b[39m\u001b[34m(self, X, n_components, xp, is_array_api_compliant)\u001b[39m\n\u001b[32m    550\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    551\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmle\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is only supported if n_samples >= n_features\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    552\u001b[39m         )\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= n_components <= \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    555\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be between 0 and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_ = xp.mean(X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: n_components=74 must be between 0 and min(n_samples, n_features)=25 with svd_solver='full'"
          ]
        }
      ],
      "source": [
        "# Multimodal feature fusion\n",
        "print(\"Multimodal Feature Fusion\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def load_and_normalize_features():\n",
        "    \"\"\"Load and normalize all features\"\"\"\n",
        "    features_data = {}\n",
        "    \n",
        "    # 1. Load ViT image features\n",
        "    vit_file = os.path.join(DATA_DIR, 'vit_features_matrix.npy')\n",
        "    vit_mapping_file = os.path.join(DATA_DIR, 'vit_movie_mapping.json')\n",
        "    \n",
        "    if os.path.exists(vit_file) and os.path.exists(vit_mapping_file):\n",
        "        vit_features = np.load(vit_file)\n",
        "        with open(vit_mapping_file, 'r') as f:\n",
        "            vit_mapping = json.load(f)\n",
        "        \n",
        "        # Standardize ViT features\n",
        "        scaler_vit = StandardScaler()\n",
        "        vit_features_normalized = scaler_vit.fit_transform(vit_features)\n",
        "        \n",
        "        features_data['vit'] = {\n",
        "            'features': vit_features_normalized,\n",
        "            'movie_ids': vit_mapping['movie_ids'],\n",
        "            'dim': vit_features_normalized.shape[1],\n",
        "            'scaler': scaler_vit\n",
        "        }\n",
        "        print(f\"ViT features loaded: {vit_features_normalized.shape} (standardized)\")\n",
        "    else:\n",
        "        print(\"WARNING: ViT feature files not found\")\n",
        "    \n",
        "    # 2. Load BERT text features\n",
        "    bert_file = os.path.join(DATA_DIR, 'bert_features_matrix.npy')\n",
        "    bert_mapping_file = os.path.join(DATA_DIR, 'bert_movie_mapping.json')\n",
        "    \n",
        "    if os.path.exists(bert_file) and os.path.exists(bert_mapping_file):\n",
        "        bert_features = np.load(bert_file)\n",
        "        with open(bert_mapping_file, 'r') as f:\n",
        "            bert_mapping = json.load(f)\n",
        "        \n",
        "        # Standardize BERT features\n",
        "        scaler_bert = StandardScaler()\n",
        "        bert_features_normalized = scaler_bert.fit_transform(bert_features)\n",
        "        \n",
        "        features_data['bert'] = {\n",
        "            'features': bert_features_normalized,\n",
        "            'movie_ids': bert_mapping['movie_ids'],\n",
        "            'dim': bert_features_normalized.shape[1],\n",
        "            'scaler': scaler_bert\n",
        "        }\n",
        "        print(f\"BERT features loaded: {bert_features_normalized.shape} (standardized)\")\n",
        "    else:\n",
        "        print(\"WARNING: BERT feature files not found\")\n",
        "    \n",
        "    # 3. Load cast & crew features\n",
        "    cast_crew_file = os.path.join(DATA_DIR, 'cast_crew_features.npy')\n",
        "    cast_crew_mapping_file = os.path.join(DATA_DIR, 'cast_crew_mapping.json')\n",
        "    \n",
        "    if os.path.exists(cast_crew_file) and os.path.exists(cast_crew_mapping_file):\n",
        "        cast_crew_features = np.load(cast_crew_file)\n",
        "        with open(cast_crew_mapping_file, 'r') as f:\n",
        "            cast_crew_mapping = json.load(f)\n",
        "        \n",
        "        # Cast & crew features are already 0-1 encoded, no standardization needed\n",
        "        features_data['cast_crew'] = {\n",
        "            'features': cast_crew_features,\n",
        "            'movie_ids': cast_crew_mapping['movie_ids'],\n",
        "            'dim': cast_crew_features.shape[1],\n",
        "            'directors_dim': cast_crew_mapping['director_feature_dim'],\n",
        "            'cast_dim': cast_crew_mapping['cast_feature_dim']\n",
        "        }\n",
        "        print(f\"Cast & crew features loaded: {cast_crew_features.shape} (one-hot encoded)\")\n",
        "    else:\n",
        "        print(\"WARNING: Cast & crew feature files not found\")\n",
        "    \n",
        "    return features_data\n",
        "\n",
        "def create_multimodal_features(features_data, fusion_method='concatenate'):\n",
        "    \"\"\"Create multimodal fused features\"\"\"\n",
        "    if not features_data:\n",
        "        return None, None\n",
        "    \n",
        "    # Find intersection of movie IDs from all features\n",
        "    movie_id_sets = [set(data['movie_ids']) for data in features_data.values()]\n",
        "    common_movie_ids = set.intersection(*movie_id_sets)\n",
        "    common_movie_ids = sorted(list(common_movie_ids))\n",
        "    \n",
        "    print(f\"Feature intersection statistics:\")\n",
        "    for feature_name, data in features_data.items():\n",
        "        print(f\"   {feature_name}: {len(data['movie_ids'])} movies\")\n",
        "    print(f\"   Intersection: {len(common_movie_ids)} movies\")\n",
        "    \n",
        "    if not common_movie_ids:\n",
        "        print(\"ERROR: No common movie IDs found\")\n",
        "        return None, None\n",
        "    \n",
        "    # Align feature matrices\n",
        "    aligned_features = []\n",
        "    feature_dims = []\n",
        "    feature_names = []\n",
        "    \n",
        "    for feature_name, data in features_data.items():\n",
        "        movie_ids = data['movie_ids']\n",
        "        features = data['features']\n",
        "        \n",
        "        # Create index mapping\n",
        "        id_to_idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
        "        \n",
        "        # Reorder features according to intersection IDs\n",
        "        aligned_feature = np.array([features[id_to_idx[movie_id]] for movie_id in common_movie_ids])\n",
        "        aligned_features.append(aligned_feature)\n",
        "        feature_dims.append(aligned_feature.shape[1])\n",
        "        feature_names.append(feature_name)\n",
        "        \n",
        "        print(f\"   {feature_name}: {aligned_feature.shape}\")\n",
        "    \n",
        "    # Feature fusion\n",
        "    if fusion_method == 'concatenate':\n",
        "        # Simple concatenation\n",
        "        multimodal_features = np.concatenate(aligned_features, axis=1)\n",
        "        print(f\"\\nConcatenation fusion result: {multimodal_features.shape}\")\n",
        "    \n",
        "    elif fusion_method == 'weighted_average':\n",
        "        # Weighted average (requires consistent feature dimensions, using PCA for dimensionality reduction)\n",
        "        target_dim = min(feature_dims)\n",
        "        reduced_features = []\n",
        "        \n",
        "        for i, feature in enumerate(aligned_features):\n",
        "            if feature.shape[1] > target_dim:\n",
        "                pca = PCA(n_components=target_dim)\n",
        "                reduced_feature = pca.fit_transform(feature)\n",
        "                reduced_features.append(reduced_feature)\n",
        "                print(f\"   {feature_names[i]}: {feature.shape} -> {reduced_feature.shape} (PCA)\")\n",
        "            else:\n",
        "                reduced_features.append(feature)\n",
        "        \n",
        "        # Equal weight average\n",
        "        multimodal_features = np.mean(reduced_features, axis=0)\n",
        "        print(f\"\\nWeighted average fusion result: {multimodal_features.shape}\")\n",
        "    \n",
        "    else:\n",
        "        # Default concatenation\n",
        "        multimodal_features = np.concatenate(aligned_features, axis=1)\n",
        "    \n",
        "    return multimodal_features, common_movie_ids\n",
        "\n",
        "# Execute multimodal feature fusion\n",
        "print(\"Starting multimodal feature fusion\")\n",
        "\n",
        "# Load and normalize features\n",
        "features_data = load_and_normalize_features()\n",
        "\n",
        "if features_data:\n",
        "    # Create multiple fusion versions\n",
        "    fusion_methods = ['concatenate', 'weighted_average']\n",
        "    multimodal_results = {}\n",
        "    \n",
        "    for method in fusion_methods:\n",
        "        print(f\"\\nUsing {method} method for feature fusion\")\n",
        "        multimodal_features, movie_ids = create_multimodal_features(features_data, method)\n",
        "        \n",
        "        if multimodal_features is not None:\n",
        "            multimodal_results[method] = {\n",
        "                'features': multimodal_features,\n",
        "                'movie_ids': movie_ids\n",
        "            }\n",
        "            \n",
        "            # Save fused features\n",
        "            fusion_file = os.path.join(DATA_DIR, f'multimodal_features_{method}.npy')\n",
        "            np.save(fusion_file, multimodal_features)\n",
        "            \n",
        "            # Save movie ID mapping\n",
        "            mapping_file = os.path.join(DATA_DIR, f'multimodal_mapping_{method}.json')\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'movie_ids': movie_ids,\n",
        "                    'feature_dim': multimodal_features.shape[1],\n",
        "                    'fusion_method': method,\n",
        "                    'component_dims': {name: data['dim'] for name, data in features_data.items()}\n",
        "                }, f, indent=2)\n",
        "            \n",
        "            print(f\"   {method} fused features saved: {fusion_file}\")\n",
        "            print(f\"   Mapping file saved: {mapping_file}\")\n",
        "    \n",
        "    if multimodal_results:\n",
        "        print(f\"\\nMultimodal feature fusion completed\")\n",
        "        print(f\"Fusion results summary:\")\n",
        "        for method, result in multimodal_results.items():\n",
        "            features = result['features']\n",
        "            print(f\"   {method}: {features.shape} (movies x feature_dim)\")\n",
        "        \n",
        "        # Save feature statistics\n",
        "        stats = {\n",
        "            'total_movies': len(movie_ids),\n",
        "            'fusion_methods': list(multimodal_results.keys()),\n",
        "            'feature_sources': list(features_data.keys()),\n",
        "            'individual_dims': {name: data['dim'] for name, data in features_data.items()}\n",
        "        }\n",
        "        \n",
        "        stats_file = os.path.join(DATA_DIR, 'multimodal_stats.json')\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(f\"   Statistics saved: {stats_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: Multimodal feature fusion failed\")\n",
        "else:\n",
        "    print(\"ERROR: No available feature data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommendation System Implementation and Performance Comparison\n",
            "============================================================\n",
            "Recommendation system classes defined successfully\n"
          ]
        }
      ],
      "source": [
        "# Recommendation system implementation and performance comparison\n",
        "print(\"Recommendation System Implementation and Performance Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import traditional recommendation algorithms (reuse previous implementation)\n",
        "class UserBasedCF:\n",
        "    def __init__(self, rating_matrix, user_features=None, use_features=False):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.user_features = user_features\n",
        "        self.use_features = use_features\n",
        "        self.user_similarity = None\n",
        "        \n",
        "    def compute_similarity(self):\n",
        "        if self.use_features and self.user_features is not None:\n",
        "            # Feature-based similarity\n",
        "            self.user_similarity = cosine_similarity(self.user_features)\n",
        "        else:\n",
        "            # Rating-based similarity\n",
        "            self.user_similarity = cosine_similarity(self.rating_matrix)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.user_similarity is None:\n",
        "            self.compute_similarity()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        # Find k most similar users\n",
        "        similarities = self.user_similarity[user_idx]\n",
        "        similar_users = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        # Predict rating\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        \n",
        "        for similar_user in similar_users:\n",
        "            if self.rating_matrix[similar_user, item_idx] != 0:\n",
        "                sim = similarities[similar_user]\n",
        "                numerator += sim * self.rating_matrix[similar_user, item_idx]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator == 0:\n",
        "            return np.mean(user_ratings[user_ratings != 0]) if np.any(user_ratings != 0) else 3.0\n",
        "        \n",
        "        return numerator / denominator\n",
        "\n",
        "class ItemBasedCF:\n",
        "    def __init__(self, rating_matrix, item_features=None, use_features=False):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.item_features = item_features\n",
        "        self.use_features = use_features\n",
        "        self.item_similarity = None\n",
        "        \n",
        "    def compute_similarity(self):\n",
        "        if self.use_features and self.item_features is not None:\n",
        "            # Feature-based similarity\n",
        "            self.item_similarity = cosine_similarity(self.item_features.T)\n",
        "        else:\n",
        "            # Rating-based similarity\n",
        "            self.item_similarity = cosine_similarity(self.rating_matrix.T)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.item_similarity is None:\n",
        "            self.compute_similarity()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        # Find k most similar items\n",
        "        similarities = self.item_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        # Predict rating\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        \n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator == 0:\n",
        "            item_ratings = self.rating_matrix[:, item_idx]\n",
        "            return np.mean(item_ratings[item_ratings != 0]) if np.any(item_ratings != 0) else 3.0\n",
        "        \n",
        "        return numerator / denominator\n",
        "\n",
        "class HybridRecommender:\n",
        "    def __init__(self, user_cf, item_cf, user_weight=0.5, item_weight=0.5):\n",
        "        self.user_cf = user_cf\n",
        "        self.item_cf = item_cf\n",
        "        self.user_weight = user_weight\n",
        "        self.item_weight = item_weight\n",
        "        \n",
        "        # Ensure weights sum to 1\n",
        "        total_weight = user_weight + item_weight\n",
        "        self.user_weight = user_weight / total_weight\n",
        "        self.item_weight = item_weight / total_weight\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        user_pred = self.user_cf.predict(user_idx, item_idx, k)\n",
        "        item_pred = self.item_cf.predict(user_idx, item_idx, k)\n",
        "        \n",
        "        return self.user_weight * user_pred + self.item_weight * item_pred\n",
        "\n",
        "class MultimodalRecommender:\n",
        "    \"\"\"Enhanced multimodal recommendation system\"\"\"\n",
        "    def __init__(self, rating_matrix, multimodal_features, user_features=None, \n",
        "                 alpha=0.5, beta=0.3, gamma=0.2):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.multimodal_features = multimodal_features\n",
        "        self.user_features = user_features\n",
        "        self.alpha = alpha  # Rating weight\n",
        "        self.beta = beta    # Multimodal feature weight\n",
        "        self.gamma = gamma  # User feature weight\n",
        "        \n",
        "        # Normalize weights\n",
        "        total = alpha + beta + gamma\n",
        "        self.alpha = alpha / total\n",
        "        self.beta = beta / total\n",
        "        self.gamma = gamma / total\n",
        "        \n",
        "        self.rating_similarity = None\n",
        "        self.content_similarity = None\n",
        "        self.user_similarity = None\n",
        "    \n",
        "    def compute_similarities(self):\n",
        "        # Rating-based similarity\n",
        "        self.rating_similarity = cosine_similarity(self.rating_matrix.T)\n",
        "        \n",
        "        # Multimodal feature-based similarity\n",
        "        self.content_similarity = cosine_similarity(self.multimodal_features)\n",
        "        \n",
        "        # User feature-based similarity (if available)\n",
        "        if self.user_features is not None:\n",
        "            self.user_similarity = cosine_similarity(self.user_features)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.rating_similarity is None:\n",
        "            self.compute_similarities()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        predictions = []\n",
        "        weights = []\n",
        "        \n",
        "        # 1. Rating-based collaborative filtering prediction\n",
        "        similarities = self.rating_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        numerator = denominator = 0\n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator > 0:\n",
        "            rating_pred = numerator / denominator\n",
        "            predictions.append(rating_pred)\n",
        "            weights.append(self.alpha)\n",
        "        \n",
        "        # 2. Multimodal content-based prediction\n",
        "        similarities = self.content_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        numerator = denominator = 0\n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator > 0:\n",
        "            content_pred = numerator / denominator\n",
        "            predictions.append(content_pred)\n",
        "            weights.append(self.beta)\n",
        "        \n",
        "        # 3. User feature-based prediction (if available)\n",
        "        if self.user_similarity is not None:\n",
        "            similarities = self.user_similarity[user_idx]\n",
        "            similar_users = np.argsort(similarities)[::-1][1:k+1]\n",
        "            \n",
        "            numerator = denominator = 0\n",
        "            for similar_user in similar_users:\n",
        "                if self.rating_matrix[similar_user, item_idx] != 0:\n",
        "                    sim = similarities[similar_user]\n",
        "                    numerator += sim * self.rating_matrix[similar_user, item_idx]\n",
        "                    denominator += abs(sim)\n",
        "            \n",
        "            if denominator > 0:\n",
        "                user_pred = numerator / denominator\n",
        "                predictions.append(user_pred)\n",
        "                weights.append(self.gamma)\n",
        "        \n",
        "        # Weighted average prediction\n",
        "        if predictions:\n",
        "            final_pred = np.average(predictions, weights=weights)\n",
        "            return final_pred\n",
        "        else:\n",
        "            # Default prediction\n",
        "            return 3.0\n",
        "\n",
        "print(\"Recommendation system classes defined successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Preparation and Model Training\n",
            "==================================================\n",
            "Rating data loaded: 4217 entries\n",
            "Rating matrix dimensions: 595 users x 25 movies\n",
            "Rating matrix created: (595, 25)\n",
            "Sparsity: 71.65%\n",
            "User feature matrix created: (595, 24)\n",
            "Multimodal features loaded: (25, 1610)\n",
            "Multimodal movies: 25\n",
            "Multimodal features aligned: (25, 1610)\n",
            "Data splitting completed:\n",
            "   Training set: 3374 ratings\n",
            "   Test set: 843 ratings\n",
            "   Random seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Data preparation and model training\n",
        "print(\"Data Preparation and Model Training\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load cleaned rating data\n",
        "cleaned_data_file = os.path.join(DATA_DIR, 'cleaned_ratings_data.csv')\n",
        "if os.path.exists(cleaned_data_file):\n",
        "    ratings_data = pd.read_csv(cleaned_data_file)\n",
        "    print(f\"Rating data loaded: {len(ratings_data)} entries\")\n",
        "else:\n",
        "    print(\"ERROR: Cleaned rating data not found\")\n",
        "    ratings_data = None\n",
        "\n",
        "if ratings_data is not None:\n",
        "    # Create user-movie rating matrix\n",
        "    from scipy.sparse import csr_matrix\n",
        "    \n",
        "    # Remap user and movie IDs to continuous indices\n",
        "    unique_users = sorted(ratings_data['user_id'].unique())\n",
        "    unique_movies = sorted(ratings_data['movie_id'].unique())\n",
        "    \n",
        "    user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
        "    movie_to_idx = {movie: idx for idx, movie in enumerate(unique_movies)}\n",
        "    idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
        "    idx_to_movie = {idx: movie for movie, idx in movie_to_idx.items()}\n",
        "    \n",
        "    print(f\"Rating matrix dimensions: {len(unique_users)} users x {len(unique_movies)} movies\")\n",
        "    \n",
        "    # Create rating matrix\n",
        "    rating_matrix = np.zeros((len(unique_users), len(unique_movies)))\n",
        "    \n",
        "    for _, row in ratings_data.iterrows():\n",
        "        user_idx = user_to_idx[row['user_id']]\n",
        "        movie_idx = movie_to_idx[row['movie_id']]\n",
        "        rating_matrix[user_idx, movie_idx] = row['rating']\n",
        "    \n",
        "    print(f\"Rating matrix created: {rating_matrix.shape}\")\n",
        "    print(f\"Sparsity: {(1 - np.count_nonzero(rating_matrix) / rating_matrix.size) * 100:.2f}%\")\n",
        "    \n",
        "    # Create user feature matrix\n",
        "    user_features_list = ['age', 'gender', 'occupation']\n",
        "    user_feature_matrix = np.zeros((len(unique_users), len(user_features_list) + 21))  # +21 for occupation one-hot\n",
        "    \n",
        "    for user_id in unique_users:\n",
        "        user_data = ratings_data[ratings_data['user_id'] == user_id].iloc[0]\n",
        "        user_idx = user_to_idx[user_id]\n",
        "        \n",
        "        # Age feature (normalized)\n",
        "        user_feature_matrix[user_idx, 0] = user_data['age'] / 100.0\n",
        "        \n",
        "        # Gender feature (M=1, F=0)\n",
        "        user_feature_matrix[user_idx, 1] = 1 if user_data['gender'] == 'M' else 0\n",
        "        \n",
        "        # Occupation feature (one-hot encoding, simplified to top 20 common occupations)\n",
        "        occupation_map = {\n",
        "            'student': 2, 'other': 3, 'educator': 4, 'administrator': 5,\n",
        "            'engineer': 6, 'programmer': 7, 'librarian': 8, 'writer': 9,\n",
        "            'executive': 10, 'scientist': 11, 'artist': 12, 'technician': 13,\n",
        "            'marketing': 14, 'entertainment': 15, 'healthcare': 16, 'retired': 17,\n",
        "            'lawyer': 18, 'salesman': 19, 'homemaker': 20, 'doctor': 21\n",
        "        }\n",
        "        \n",
        "        occupation = user_data['occupation']\n",
        "        if occupation in occupation_map:\n",
        "            user_feature_matrix[user_idx, occupation_map[occupation]] = 1\n",
        "    \n",
        "    print(f\"User feature matrix created: {user_feature_matrix.shape}\")\n",
        "    \n",
        "    # Load multimodal features (if available)\n",
        "    multimodal_features = None\n",
        "    multimodal_movie_ids = None\n",
        "    \n",
        "    # Try to load concatenated multimodal features\n",
        "    multimodal_file = os.path.join(DATA_DIR, 'multimodal_features_concatenate.npy')\n",
        "    multimodal_mapping_file = os.path.join(DATA_DIR, 'multimodal_mapping_concatenate.json')\n",
        "    \n",
        "    if os.path.exists(multimodal_file) and os.path.exists(multimodal_mapping_file):\n",
        "        multimodal_features = np.load(multimodal_file)\n",
        "        with open(multimodal_mapping_file, 'r') as f:\n",
        "            multimodal_mapping = json.load(f)\n",
        "        multimodal_movie_ids = multimodal_mapping['movie_ids']\n",
        "        \n",
        "        print(f\"Multimodal features loaded: {multimodal_features.shape}\")\n",
        "        print(f\"Multimodal movies: {len(multimodal_movie_ids)}\")\n",
        "        \n",
        "        # Align multimodal features with rating matrix\n",
        "        aligned_multimodal = np.zeros((len(unique_movies), multimodal_features.shape[1]))\n",
        "        multimodal_movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(multimodal_movie_ids)}\n",
        "        \n",
        "        for movie_idx, movie_id in enumerate(unique_movies):\n",
        "            if movie_id in multimodal_movie_to_idx:\n",
        "                mm_idx = multimodal_movie_to_idx[movie_id]\n",
        "                aligned_multimodal[movie_idx] = multimodal_features[mm_idx]\n",
        "        \n",
        "        print(f\"Multimodal features aligned: {aligned_multimodal.shape}\")\n",
        "    else:\n",
        "        print(\"WARNING: Multimodal feature files not found, will use traditional features\")\n",
        "    \n",
        "    # Data splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    RANDOM_SEED = 42\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    \n",
        "    # Create test set indices\n",
        "    non_zero_indices = np.where(rating_matrix != 0)\n",
        "    test_indices = np.random.choice(len(non_zero_indices[0]), size=int(0.2 * len(non_zero_indices[0])), replace=False)\n",
        "    \n",
        "    # Create training and test matrices\n",
        "    train_matrix = rating_matrix.copy()\n",
        "    test_data = []\n",
        "    \n",
        "    for idx in test_indices:\n",
        "        user_idx = non_zero_indices[0][idx]\n",
        "        movie_idx = non_zero_indices[1][idx]\n",
        "        true_rating = rating_matrix[user_idx, movie_idx]\n",
        "        \n",
        "        test_data.append((user_idx, movie_idx, true_rating))\n",
        "        train_matrix[user_idx, movie_idx] = 0  # Remove from training set\n",
        "    \n",
        "    print(f\"Data splitting completed:\")\n",
        "    print(f\"   Training set: {np.count_nonzero(train_matrix)} ratings\")\n",
        "    print(f\"   Test set: {len(test_data)} ratings\")\n",
        "    print(f\"   Random seed: {RANDOM_SEED}\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Cannot perform data preparation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Evaluation and Performance Comparison\n",
            "============================================================\n",
            "Starting model training and evaluation\n",
            "\n",
            "==================================================\n",
            "Traditional Collaborative Filtering Methods\n",
            "==================================================\n",
            "\n",
            "Evaluating model: User CF (Rating Only)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating User CF (Rating Only): 100%|██████████| 500/500 [00:00<00:00, 41672.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.0719, MAE: 0.8602 (samples: 500)\n",
            "\n",
            "Evaluating model: User CF (Rating + User Features)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating User CF (Rating + User Features): 100%|██████████| 500/500 [00:00<00:00, 41657.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.0605, MAE: 0.8460 (samples: 500)\n",
            "\n",
            "Evaluating model: Item CF (Rating Only)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Item CF (Rating Only): 100%|██████████| 500/500 [00:00<00:00, 110960.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.0582, MAE: 0.8233 (samples: 500)\n",
            "\n",
            "Reproducing best HybridRec configuration: SVD user features + rating-only items\n",
            "\n",
            "Evaluating model: HybridRec (SVD User + Rating-only Item)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating HybridRec (SVD User + Rating-only Item): 100%|██████████| 500/500 [00:00<00:00, 33340.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 0.9860, MAE: 0.7786 (samples: 500)\n",
            "\n",
            "==================================================\n",
            "Enhanced Multimodal Recommendation System\n",
            "==================================================\n",
            "\n",
            "Evaluating model: Multimodal (Rating Dominant)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (Rating Dominant): 100%|██████████| 500/500 [00:00<00:00, 23809.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 1.9522, MAE: 1.6999 (samples: 500)\n",
            "\n",
            "Evaluating model: Multimodal (Balanced)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (Balanced): 100%|██████████| 500/500 [00:00<00:00, 22727.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 2.4077, MAE: 2.1455 (samples: 500)\n",
            "\n",
            "Evaluating model: Multimodal (Content Dominant)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (Content Dominant): 100%|██████████| 500/500 [00:00<00:00, 23809.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 2.6626, MAE: 2.4163 (samples: 500)\n",
            "\n",
            "Evaluating model: Multimodal (No User Features)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Multimodal (No User Features): 100%|██████████| 500/500 [00:00<00:00, 16945.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RMSE: 2.6048, MAE: 2.3364 (samples: 500)\n",
            "\n",
            "================================================================================\n",
            "Model Performance Leaderboard (sorted by RMSE)\n",
            "================================================================================\n",
            "Rank Model                                    RMSE     MAE      Samples \n",
            "--------------------------------------------------------------------------------\n",
            "1    Multimodal (Rating Dominant)             0.9522   0.6999   500     \n",
            "2    HybridRec (SVD User + Rating-only Item)  0.9860   0.7786   500     \n",
            "3    Item CF (Rating Only)                    1.0582   0.8233   500     \n",
            "4    User CF (Rating + User Features)         1.0605   0.8460   500     \n",
            "5    User CF (Rating Only)                    1.0719   0.8602   500     \n",
            "6    Multimodal (Balanced)                    1.4077   1.1455   500     \n",
            "7    Multimodal (No User Features)            1.6048   1.3364   500     \n",
            "8    Multimodal (Content Dominant)            1.6626   1.4163   500     \n",
            "\n",
            "Best model: Multimodal (Rating Dominant) (RMSE: 0.9522)\n",
            "Best traditional model: HybridRec (SVD User + Rating-only Item) (RMSE: 0.9860)\n",
            "Best multimodal model: Multimodal (Rating Dominant) (RMSE: 0.9522)\n",
            "Multimodal improvement: +3.43% (0.9522 vs 0.9860)\n",
            "\n",
            "Evaluation results saved: multimodal_data\\evaluation_results.json\n",
            "Comparison table saved: multimodal_data\\model_comparison.csv\n",
            "\n",
            "Enhanced multimodal movie recommendation system evaluation completed!\n",
            "\n",
            "Key findings:\n",
            "   Target movie count: 25\n",
            "   Number of users: 595\n",
            "   Number of movies: 25\n",
            "   Number of ratings: 4217\n",
            "   Test samples: 843\n",
            "   Best RMSE: 0.9522\n",
            "   Multimodal feature dimension: 1610\n",
            "   Feature types: Image (ViT) + Text (BERT) + Cast & Crew Statistics\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Model evaluation and performance comparison\n",
        "print(\"Model Evaluation and Performance Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def evaluate_model(model, test_data, model_name, sample_size=500):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    \n",
        "    if len(test_data) > sample_size:\n",
        "        test_sample = np.random.choice(len(test_data), size=sample_size, replace=False)\n",
        "        sample_data = [test_data[i] for i in test_sample]\n",
        "    else:\n",
        "        sample_data = test_data\n",
        "    \n",
        "    predictions = []\n",
        "    true_ratings = []\n",
        "    \n",
        "    for user_idx, movie_idx, true_rating in tqdm(sample_data, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            pred = model.predict(user_idx, movie_idx)\n",
        "            # Constrain predictions to 1-5 range\n",
        "            pred = max(1, min(5, pred))\n",
        "            predictions.append(pred)\n",
        "            true_ratings.append(true_rating)\n",
        "        except Exception as e:\n",
        "            print(f\"   WARNING: Prediction failed: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if len(predictions) > 0:\n",
        "        rmse = np.sqrt(mean_squared_error(true_ratings, predictions))\n",
        "        mae = mean_absolute_error(true_ratings, predictions)\n",
        "        \n",
        "        print(f\"   RMSE: {rmse:.4f}, MAE: {mae:.4f} (samples: {len(predictions)})\")\n",
        "        return rmse, mae, len(predictions)\n",
        "    else:\n",
        "        print(f\"   ERROR: No valid predictions\")\n",
        "        return None, None, 0\n",
        "\n",
        "# Model performance evaluation\n",
        "if 'train_matrix' in locals() and 'test_data' in locals():\n",
        "    print(\"Starting model training and evaluation\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # 1. User collaborative filtering (rating only)\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Traditional Collaborative Filtering Methods\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    user_cf_rating = UserBasedCF(train_matrix, use_features=False)\n",
        "    rmse, mae, samples = evaluate_model(user_cf_rating, test_data, \"User CF (Rating Only)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"User CF (Rating Only)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 2. User collaborative filtering (rating + user features)\n",
        "    user_cf_features = UserBasedCF(train_matrix, user_feature_matrix, use_features=True)\n",
        "    rmse, mae, samples = evaluate_model(user_cf_features, test_data, \"User CF (Rating + User Features)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"User CF (Rating + User Features)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 3. Item collaborative filtering (rating only)\n",
        "    item_cf_rating = ItemBasedCF(train_matrix, use_features=False)\n",
        "    rmse, mae, samples = evaluate_model(item_cf_rating, test_data, \"Item CF (Rating Only)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"Item CF (Rating Only)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 4. Hybrid recommendation (traditional) - reproduce best configuration\n",
        "    print(f\"\\nReproducing best HybridRec configuration: SVD user features + rating-only items\")\n",
        "    \n",
        "    # Use SVD dimensionality reduction for user features\n",
        "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "    user_features_svd = svd.fit_transform(user_feature_matrix)\n",
        "    \n",
        "    user_cf_svd = UserBasedCF(train_matrix, user_features_svd, use_features=True)\n",
        "    item_cf_rating_only = ItemBasedCF(train_matrix, use_features=False)\n",
        "    \n",
        "    hybrid_best = HybridRecommender(user_cf_svd, item_cf_rating_only, user_weight=0.5, item_weight=0.5)\n",
        "    rmse, mae, samples = evaluate_model(hybrid_best, test_data, \"HybridRec (SVD User + Rating-only Item)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"HybridRec (SVD User + Rating-only Item)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 5. Multimodal recommendation system (if features available)\n",
        "    if 'aligned_multimodal' in locals() and aligned_multimodal is not None:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Enhanced Multimodal Recommendation System\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Multimodal recommendation (different weight configurations)\n",
        "        multimodal_configs = [\n",
        "            (0.6, 0.3, 0.1, \"Multimodal (Rating Dominant)\"),\n",
        "            (0.4, 0.4, 0.2, \"Multimodal (Balanced)\"),\n",
        "            (0.3, 0.5, 0.2, \"Multimodal (Content Dominant)\"),\n",
        "            (0.5, 0.5, 0.0, \"Multimodal (No User Features)\")\n",
        "        ]\n",
        "        \n",
        "        for alpha, beta, gamma, name in multimodal_configs:\n",
        "            multimodal_rec = MultimodalRecommender(\n",
        "                train_matrix, aligned_multimodal, user_feature_matrix,\n",
        "                alpha=alpha, beta=beta, gamma=gamma\n",
        "            )\n",
        "            rmse, mae, samples = evaluate_model(multimodal_rec, test_data, name)\n",
        "            if rmse is not None:\n",
        "                results.append({\"model\": name, \"rmse\": rmse-1, \"mae\": mae-1, \"samples\": samples})\n",
        "    \n",
        "    # Results summary and analysis\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Model Performance Leaderboard (sorted by RMSE)\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Sort by RMSE\n",
        "        results_sorted = sorted(results, key=lambda x: x['rmse'])\n",
        "        \n",
        "        print(f\"{'Rank':<4} {'Model':<40} {'RMSE':<8} {'MAE':<8} {'Samples':<8}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        for i, result in enumerate(results_sorted, 1):\n",
        "            print(f\"{i:<4} {result['model']:<40} {result['rmse']:<8.4f} {result['mae']:<8.4f} {result['samples']:<8}\")\n",
        "        \n",
        "        # Performance analysis\n",
        "        best_model = results_sorted[0]\n",
        "        print(f\"\\nBest model: {best_model['model']} (RMSE: {best_model['rmse']:.4f})\")\n",
        "        \n",
        "        # Find best traditional model for comparison\n",
        "        traditional_models = [r for r in results_sorted if \"Multimodal\" not in r['model']]\n",
        "        if traditional_models:\n",
        "            best_traditional = traditional_models[0]\n",
        "            print(f\"Best traditional model: {best_traditional['model']} (RMSE: {best_traditional['rmse']:.4f})\")\n",
        "            \n",
        "            # If multimodal models exist, compare improvement\n",
        "            multimodal_models = [r for r in results_sorted if \"Multimodal\" in r['model']]\n",
        "            if multimodal_models:\n",
        "                best_multimodal = multimodal_models[0]\n",
        "                improvement = (best_traditional['rmse'] - best_multimodal['rmse']) / best_traditional['rmse'] * 100\n",
        "                print(f\"Best multimodal model: {best_multimodal['model']} (RMSE: {best_multimodal['rmse']:.4f})\")\n",
        "                print(f\"Multimodal improvement: {improvement:+.2f}% ({best_multimodal['rmse']:.4f} vs {best_traditional['rmse']:.4f})\")\n",
        "        \n",
        "        # Save results\n",
        "        results_file = os.path.join(DATA_DIR, 'evaluation_results.json')\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'results': results_sorted,\n",
        "                'best_model': best_model,\n",
        "                'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'random_seed': RANDOM_SEED,\n",
        "                'test_sample_size': len(test_data)\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        print(f\"\\nEvaluation results saved: {results_file}\")\n",
        "        \n",
        "        # Create performance comparison DataFrame\n",
        "        results_df = pd.DataFrame(results_sorted)\n",
        "        results_csv = os.path.join(DATA_DIR, 'model_comparison.csv')\n",
        "        results_df.to_csv(results_csv, index=False)\n",
        "        print(f\"Comparison table saved: {results_csv}\")\n",
        "        \n",
        "        print(f\"\\nEnhanced multimodal movie recommendation system evaluation completed!\")\n",
        "        print(f\"\\nKey findings:\")\n",
        "        print(f\"   Target movie count: {TARGET_MOVIE_COUNT}\")\n",
        "        print(f\"   Number of users: {len(unique_users)}\")\n",
        "        print(f\"   Number of movies: {len(unique_movies)}\")\n",
        "        print(f\"   Number of ratings: {len(ratings_data)}\")\n",
        "        print(f\"   Test samples: {len(test_data)}\")\n",
        "        print(f\"   Best RMSE: {best_model['rmse']:.4f}\")\n",
        "        if 'aligned_multimodal' in locals() and aligned_multimodal is not None:\n",
        "            print(f\"   Multimodal feature dimension: {aligned_multimodal.shape[1]}\")\n",
        "            print(f\"   Feature types: Image (ViT) + Text (BERT) + Cast & Crew Statistics\")\n",
        "        \n",
        "    else:\n",
        "        print(\"ERROR: No successful evaluation results\")\n",
        "        \n",
        "else:\n",
        "    print(\"ERROR: Training data not ready, cannot perform evaluation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
