{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Multimodal Movie Recommendation System\n",
        "\n",
        "A comprehensive recommendation system based on MovieLens 100k + TMDB + ViT Image Features + BERT Text Features\n",
        "\n",
        "## Key Features\n",
        "- **Smart Quantity Control**: Set target number of movies with automatic progress management\n",
        "- **Image Feature Extraction**: ViT model for extracting visual features from posters and stills  \n",
        "- **Text Feature Extraction**: BERT model for processing movie overviews and taglines\n",
        "- **Cast & Crew Statistics**: Extract high-frequency actors and directors as features\n",
        "- **Multi-dimensional Feature Engineering**: Fusion of numerical, categorical, text, and image features\n",
        "- **Performance Comparison**: Comprehensive evaluation against traditional recommendation systems\n",
        "\n",
        "## System Architecture\n",
        "1. **Data Preparation**: MovieLens + TMDB + User Filtering\n",
        "2. **Image Processing**: Download → Selection → ViT Feature Extraction\n",
        "3. **Text Processing**: BERT Feature Extraction\n",
        "4. **Feature Engineering**: Multimodal Feature Fusion\n",
        "5. **Recommendation System**: Multi-algorithm Performance Comparison\n",
        "\n",
        "## Innovation Points\n",
        "- Multimodal feature fusion (text + image + traditional features)\n",
        "- Intelligent image selection (5 most diverse images from 10)\n",
        "- Comprehensive performance evaluation (reproducing best HybridRec configuration)\n",
        "\n",
        "## Technical Stack\n",
        "- **Computer Vision**: ViT (Vision Transformer) for image understanding\n",
        "- **Natural Language Processing**: BERT for semantic text analysis\n",
        "- **Recommendation Algorithms**: Collaborative Filtering, Content-based, Hybrid approaches\n",
        "- **Evaluation Metrics**: RMSE, MAE with statistical significance testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: pillow in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: torch in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (2.7.1)\n",
            "Requirement already satisfied: torchvision in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (0.22.1)\n",
            "Requirement already satisfied: scikit-learn in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: opencv-python in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: colorama in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: filelock in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.8.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in z:\\document\\myproject\\3033-061\\.conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Enhanced Multimodal Recommendation System\n",
            "============================================================\n",
            "Target movie count: 25\n",
            "Images per movie: 10 -> select 5\n",
            "Image directories: multimodal_images -> selected_images\n",
            "Data directory: multimodal_data\n",
            "API request interval: 0.3 seconds\n",
            "\n",
            "Environment setup completed\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install requests pandas pillow tqdm transformers torch torchvision scikit-learn opencv-python\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from transformers import BertTokenizer, BertModel, ViTImageProcessor, ViTModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "print(\"Enhanced Multimodal Recommendation System\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ===================== Core Configuration Parameters =====================\n",
        "# Target movie count - user configurable\n",
        "TARGET_MOVIE_COUNT = 25  # Set the desired number of movies to process\n",
        "\n",
        "# API Configuration\n",
        "TMDB_API_KEY = \"6ba3eb883961b80c06d196906b976afe\"\n",
        "TMDB_BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "TMDB_IMAGE_BASE_URL = \"https://image.tmdb.org/t/p/original\"\n",
        "\n",
        "# File path configuration\n",
        "IMAGE_DIR = \"multimodal_images\"          # Original image directory\n",
        "SELECTED_IMAGE_DIR = \"selected_images\"    # Selected image directory\n",
        "DATA_DIR = \"multimodal_data\"             # Data storage directory\n",
        "PROGRESS_FILE = \"multimodal_progress.json\" # Progress tracking file\n",
        "\n",
        "# Processing parameters\n",
        "IMAGES_PER_MOVIE = 10    # Number of images to download per movie\n",
        "SELECTED_IMAGES = 5      # Number of images to select after filtering\n",
        "DELAY_BETWEEN_REQUESTS = 0.3  # API request interval (seconds)\n",
        "\n",
        "# Create directories\n",
        "for directory in [IMAGE_DIR, SELECTED_IMAGE_DIR, DATA_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Target movie count: {TARGET_MOVIE_COUNT}\")\n",
        "print(f\"Images per movie: {IMAGES_PER_MOVIE} -> select {SELECTED_IMAGES}\")\n",
        "print(f\"Image directories: {IMAGE_DIR} -> {SELECTED_IMAGE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"API request interval: {DELAY_BETWEEN_REQUESTS} seconds\")\n",
        "print(\"\\nEnvironment setup completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading and intelligent progress management\n",
        "print(\"Data Loading and Progress Check\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load MovieLens movie data\n",
        "try:\n",
        "    movies_df = pd.read_csv('movielens_movies.csv')\n",
        "    print(f\"MovieLens movie data loaded: {len(movies_df)} movies\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: movielens_movies.csv not found\")\n",
        "    print(\"Please run generate_imdb_mapping.py to generate movie information\")\n",
        "\n",
        "# Load IMDB ID mapping\n",
        "try:\n",
        "    with open('imdb/progress_mapping.json', 'r', encoding='utf-8') as f:\n",
        "        imdb_mapping = json.load(f)\n",
        "    imdb_mapping = {int(k): v for k, v in imdb_mapping.items()}\n",
        "    print(f\"IMDB ID mapping loaded: {len(imdb_mapping)} entries\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: IMDB ID mapping file not found\")\n",
        "\n",
        "# Load user rating data\n",
        "try:\n",
        "    ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', \n",
        "                           names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "    print(f\"User rating data loaded: {len(ratings_df):,} ratings\")\n",
        "    print(f\"Number of users: {ratings_df['user_id'].nunique():,}\")\n",
        "    print(f\"Number of movies: {ratings_df['movie_id'].nunique():,}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: ml-100k/u.data file not found\")\n",
        "\n",
        "# Load user information\n",
        "try:\n",
        "    users_df = pd.read_csv('ml-100k/u.user', sep='|', \n",
        "                         names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "    print(f\"User information loaded: {len(users_df)} users\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: ml-100k/u.user file not found\")\n",
        "\n",
        "# Intelligent target movie selection\n",
        "if 'movies_df' in locals() and 'imdb_mapping' in locals():\n",
        "    valid_movies = movies_df[movies_df['movie_id'].isin(imdb_mapping.keys())].copy()\n",
        "    valid_movies['imdb_id'] = valid_movies['movie_id'].map(imdb_mapping)\n",
        "    \n",
        "    # Select target movies based on TARGET_MOVIE_COUNT\n",
        "    target_movies = valid_movies.head(TARGET_MOVIE_COUNT).copy()\n",
        "    \n",
        "    print(f\"\\nTarget movie processing: {len(target_movies)} / {len(valid_movies)} valid movies\")\n",
        "    print(f\"Data coverage: {len(target_movies)/len(valid_movies)*100:.1f}%\")\n",
        "    \n",
        "    # Display target movie list\n",
        "    print(f\"\\nTarget movie list:\")\n",
        "    for i, (_, movie) in enumerate(target_movies.head(10).iterrows(), 1):\n",
        "        print(f\"   {i:2d}. {movie['movie_id']:3d}. {movie['title']} ({movie['year']}) -> tt{movie['imdb_id']}\")\n",
        "    \n",
        "    if len(target_movies) > 10:\n",
        "        print(f\"   ... and {len(target_movies) - 10} more movies\")\n",
        "        \n",
        "    # Save target movie list\n",
        "    target_file = os.path.join(DATA_DIR, 'target_movies.csv')\n",
        "    target_movies.to_csv(target_file, index=False)\n",
        "    print(f\"\\nTarget movie list saved: {target_file}\")\n",
        "else:\n",
        "    print(\"ERROR: Data loading failed, cannot continue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User data filtering and cleaning\n",
        "print(\"User Data Filtering and Cleaning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if 'target_movies' in locals() and 'ratings_df' in locals():\n",
        "    # Filter rating data for target movies\n",
        "    target_movie_ids = set(target_movies['movie_id'].values)\n",
        "    filtered_ratings = ratings_df[ratings_df['movie_id'].isin(target_movie_ids)].copy()\n",
        "    \n",
        "    print(f\"Original rating data: {len(ratings_df):,} entries\")\n",
        "    print(f\"Target movie ratings: {len(filtered_ratings):,} entries\")\n",
        "    print(f\"Data filtering rate: {len(filtered_ratings)/len(ratings_df)*100:.1f}%\")\n",
        "    \n",
        "    # Analyze user activity\n",
        "    user_activity = filtered_ratings['user_id'].value_counts()\n",
        "    print(f\"\\nUser activity analysis:\")\n",
        "    print(f\"   Total users: {len(user_activity):,}\")\n",
        "    print(f\"   Average ratings per user: {user_activity.mean():.1f}\")\n",
        "    print(f\"   Median ratings per user: {user_activity.median():.1f}\")\n",
        "    print(f\"   Most active user: {user_activity.max()} ratings\")\n",
        "    print(f\"   Least active user: {user_activity.min()} ratings\")\n",
        "    \n",
        "    # Clean low-activity users (less than 3 ratings)\n",
        "    MIN_RATINGS_PER_USER = 3\n",
        "    active_users = user_activity[user_activity >= MIN_RATINGS_PER_USER].index\n",
        "    cleaned_ratings = filtered_ratings[filtered_ratings['user_id'].isin(active_users)].copy()\n",
        "    \n",
        "    print(f\"\\nData cleaning results:\")\n",
        "    print(f\"   Minimum ratings requirement: {MIN_RATINGS_PER_USER} entries\")\n",
        "    print(f\"   Users retained: {len(active_users):,} / {len(user_activity):,} ({len(active_users)/len(user_activity)*100:.1f}%)\")\n",
        "    print(f\"   Ratings retained: {len(cleaned_ratings):,} / {len(filtered_ratings):,} ({len(cleaned_ratings)/len(filtered_ratings)*100:.1f}%)\")\n",
        "    \n",
        "    # Merge user information and handle missing values\n",
        "    if 'users_df' in locals():\n",
        "        cleaned_ratings_with_users = cleaned_ratings.merge(users_df, on='user_id', how='left')\n",
        "        \n",
        "        # Check and clean missing user information\n",
        "        missing_user_info = cleaned_ratings_with_users['age'].isna().sum()\n",
        "        if missing_user_info > 0:\n",
        "            print(f\"WARNING: Missing user information: {missing_user_info:,} entries ({missing_user_info/len(cleaned_ratings_with_users)*100:.1f}%)\")\n",
        "            # Remove records with missing user information\n",
        "            cleaned_ratings_with_users.dropna(subset=['age', 'gender', 'occupation'], inplace=True)\n",
        "            print(f\"Ratings after cleaning: {len(cleaned_ratings_with_users):,} entries\")\n",
        "        \n",
        "        print(f\"\\nFinal cleaned dataset:\")\n",
        "        print(f\"   Rating records: {len(cleaned_ratings_with_users):,} entries\")\n",
        "        print(f\"   Number of users: {cleaned_ratings_with_users['user_id'].nunique():,}\")\n",
        "        print(f\"   Number of movies: {cleaned_ratings_with_users['movie_id'].nunique():,}\")\n",
        "        print(f\"   Average ratings per user: {len(cleaned_ratings_with_users)/cleaned_ratings_with_users['user_id'].nunique():.1f}\")\n",
        "        print(f\"   Average ratings per movie: {len(cleaned_ratings_with_users)/cleaned_ratings_with_users['movie_id'].nunique():.1f}\")\n",
        "        \n",
        "        # User feature analysis\n",
        "        print(f\"\\nUser demographic distribution:\")\n",
        "        print(f\"   Age range: {cleaned_ratings_with_users['age'].min()}-{cleaned_ratings_with_users['age'].max()} years\")\n",
        "        print(f\"   Average age: {cleaned_ratings_with_users['age'].mean():.1f} years\")\n",
        "        print(f\"   Gender distribution: {dict(cleaned_ratings_with_users['gender'].value_counts())}\")\n",
        "        print(f\"   Top 5 occupations: {dict(cleaned_ratings_with_users['occupation'].value_counts().head())}\")\n",
        "        \n",
        "        # Rating distribution analysis\n",
        "        rating_dist = cleaned_ratings_with_users['rating'].value_counts().sort_index()\n",
        "        print(f\"\\nRating distribution:\")\n",
        "        for rating, count in rating_dist.items():\n",
        "            print(f\"   {rating} stars: {count:,} entries ({count/len(cleaned_ratings_with_users)*100:.1f}%)\")\n",
        "        print(f\"   Average rating: {cleaned_ratings_with_users['rating'].mean():.2f}\")\n",
        "        \n",
        "        # Save cleaned data\n",
        "        cleaned_data_file = os.path.join(DATA_DIR, 'cleaned_ratings_data.csv')\n",
        "        cleaned_ratings_with_users.to_csv(cleaned_data_file, index=False)\n",
        "        print(f\"\\nCleaned data saved: {cleaned_data_file}\")\n",
        "        \n",
        "        # Update target_movies to include only movies with ratings\n",
        "        rated_movie_ids = set(cleaned_ratings_with_users['movie_id'].unique())\n",
        "        target_movies = target_movies[target_movies['movie_id'].isin(rated_movie_ids)].copy()\n",
        "        print(f\"Updated target movie count: {len(target_movies)} movies (with actual rating data)\")\n",
        "        \n",
        "        # Print rating matrix dimension information\n",
        "        n_users = cleaned_ratings_with_users['user_id'].nunique()\n",
        "        n_movies = cleaned_ratings_with_users['movie_id'].nunique()\n",
        "        sparsity = 1 - len(cleaned_ratings_with_users) / (n_users * n_movies)\n",
        "        print(f\"\\nRating matrix dimensions:\")\n",
        "        print(f\"   User-Movie matrix: {n_users} x {n_movies} = {n_users * n_movies:,} possible ratings\")\n",
        "        print(f\"   Actual ratings: {len(cleaned_ratings_with_users):,}\")\n",
        "        print(f\"   Sparsity: {sparsity*100:.2f}% (percentage of missing values)\")\n",
        "        print(f\"   Density: {(1-sparsity)*100:.2f}% (percentage of observed values)\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: Required data missing, cannot perform user data filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TMDB data acquisition and image download processor\n",
        "print(\"TMDB Data Acquisition and Image Download\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class EnhancedTMDBProcessor:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "        self.processed_movies = set()\n",
        "        self.failed_movies = set()\n",
        "        self.movie_features = {}\n",
        "        self.load_progress()\n",
        "    \n",
        "    def load_progress(self):\n",
        "        \"\"\"Load processing progress\"\"\"\n",
        "        if os.path.exists(PROGRESS_FILE):\n",
        "            try:\n",
        "                with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "                    progress = json.load(f)\n",
        "                self.processed_movies = set(progress.get(\"processed\", []))\n",
        "                self.failed_movies = set(progress.get(\"failed\", []))\n",
        "                self.movie_features = progress.get(\"movie_features\", {})\n",
        "                print(f\"Progress loaded: {len(self.processed_movies)} processed, {len(self.failed_movies)} failed\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Failed to load progress: {str(e)}\")\n",
        "    \n",
        "    def save_progress(self):\n",
        "        \"\"\"Save processing progress\"\"\"\n",
        "        try:\n",
        "            progress = {\n",
        "                \"processed\": list(self.processed_movies),\n",
        "                \"failed\": list(self.failed_movies),\n",
        "                \"movie_features\": self.movie_features,\n",
        "                \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"target_count\": TARGET_MOVIE_COUNT,\n",
        "                \"total_processed\": len(self.processed_movies),\n",
        "                \"total_failed\": len(self.failed_movies)\n",
        "            }\n",
        "            with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(progress, f, ensure_ascii=False, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save progress: {str(e)}\")\n",
        "    \n",
        "    def find_movie_by_imdb_id(self, imdb_id):\n",
        "        \"\"\"Find TMDB movie by IMDB ID\"\"\"\n",
        "        if not imdb_id.startswith('tt'):\n",
        "            imdb_id = f\"tt{imdb_id}\"\n",
        "        \n",
        "        url = f\"{TMDB_BASE_URL}/find/{imdb_id}\"\n",
        "        params = {\"api_key\": self.api_key, \"external_source\": \"imdb_id\"}\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if data.get(\"movie_results\"):\n",
        "                    return data[\"movie_results\"][0]\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"     ERROR: Search failed: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def get_movie_details(self, tmdb_id):\n",
        "        \"\"\"Get detailed movie information\"\"\"\n",
        "        url = f\"{TMDB_BASE_URL}/movie/{tmdb_id}\"\n",
        "        params = {\n",
        "            \"api_key\": self.api_key,\n",
        "            \"append_to_response\": \"credits,keywords,videos,images\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"     ERROR: Failed to get details: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def clean_filename(self, filename):\n",
        "        \"\"\"Clean filename for safe storage\"\"\"\n",
        "        invalid_chars = '<>:\"/\\\\|?*'\n",
        "        for char in invalid_chars:\n",
        "            filename = filename.replace(char, '_')\n",
        "        return filename[:100]\n",
        "    \n",
        "    def download_image(self, url, save_path):\n",
        "        \"\"\"Download and process image\"\"\"\n",
        "        if os.path.exists(save_path):\n",
        "            return True\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                if img.size[0] < 50 or img.size[1] < 50:\n",
        "                    return False\n",
        "                \n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "                \n",
        "                # Resize to 512x512 for ViT processing\n",
        "                img = img.resize((512, 512), Image.Resampling.LANCZOS)\n",
        "                img.save(save_path, \"JPEG\", quality=90, optimize=True)\n",
        "                return True\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"       ERROR: Download failed: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def download_movie_images(self, movie_id, title, tmdb_movie):\n",
        "        \"\"\"Download movie images - up to 10 images\"\"\"\n",
        "        movie_folder = os.path.join(IMAGE_DIR, f\"{movie_id:04d}_{self.clean_filename(title)}\")\n",
        "        os.makedirs(movie_folder, exist_ok=True)\n",
        "        \n",
        "        downloaded = []\n",
        "        \n",
        "        # 1. Download poster\n",
        "        if tmdb_movie.get(\"poster_path\"):\n",
        "            poster_url = f\"{TMDB_IMAGE_BASE_URL}{tmdb_movie['poster_path']}\"\n",
        "            poster_path = os.path.join(movie_folder, \"poster.jpg\")\n",
        "            if self.download_image(poster_url, poster_path):\n",
        "                downloaded.append((\"poster\", poster_path))\n",
        "        \n",
        "        # 2. Download backdrop\n",
        "        if tmdb_movie.get(\"backdrop_path\"):\n",
        "            backdrop_url = f\"{TMDB_IMAGE_BASE_URL}{tmdb_movie['backdrop_path']}\"\n",
        "            backdrop_path = os.path.join(movie_folder, \"backdrop.jpg\")\n",
        "            if self.download_image(backdrop_url, backdrop_path):\n",
        "                downloaded.append((\"backdrop\", backdrop_path))\n",
        "        \n",
        "        # 3. Download stills (up to 8)\n",
        "        if \"images\" in tmdb_movie and \"backdrops\" in tmdb_movie[\"images\"]:\n",
        "            backdrops = tmdb_movie[\"images\"][\"backdrops\"][:8]\n",
        "            for i, backdrop_info in enumerate(backdrops):\n",
        "                backdrop_url = f\"{TMDB_IMAGE_BASE_URL}{backdrop_info['file_path']}\"\n",
        "                still_path = os.path.join(movie_folder, f\"still_{i+1}.jpg\")\n",
        "                if self.download_image(backdrop_url, still_path):\n",
        "                    downloaded.append((f\"still_{i+1}\", still_path))\n",
        "                    \n",
        "                # Limit to maximum of 10 images\n",
        "                if len(downloaded) >= IMAGES_PER_MOVIE:\n",
        "                    break\n",
        "        \n",
        "        return downloaded\n",
        "    \n",
        "    def extract_movie_features(self, tmdb_movie, movielens_info):\n",
        "        \"\"\"Extract comprehensive movie features\"\"\"\n",
        "        features = {\n",
        "            # MovieLens original information\n",
        "            \"movielens_id\": movielens_info[\"movie_id\"],\n",
        "            \"movielens_title\": movielens_info[\"title\"],\n",
        "            \"movielens_year\": movielens_info[\"year\"],\n",
        "            \"movielens_imdb_id\": movielens_info[\"imdb_id\"],\n",
        "            \n",
        "            # TMDB basic information\n",
        "            \"tmdb_id\": tmdb_movie.get(\"id\"),\n",
        "            \"title\": tmdb_movie.get(\"title\", \"\"),\n",
        "            \"original_title\": tmdb_movie.get(\"original_title\", \"\"),\n",
        "            \"overview\": tmdb_movie.get(\"overview\", \"\"),\n",
        "            \"tagline\": tmdb_movie.get(\"tagline\", \"\"),\n",
        "            \"release_date\": tmdb_movie.get(\"release_date\", \"\"),\n",
        "            \"runtime\": tmdb_movie.get(\"runtime\", 0),\n",
        "            \"status\": tmdb_movie.get(\"status\", \"\"),\n",
        "            \n",
        "            # Rating information\n",
        "            \"vote_average\": tmdb_movie.get(\"vote_average\", 0),\n",
        "            \"vote_count\": tmdb_movie.get(\"vote_count\", 0),\n",
        "            \"popularity\": tmdb_movie.get(\"popularity\", 0),\n",
        "            \n",
        "            # Classification information\n",
        "            \"genres\": \"|\".join([g[\"name\"] for g in tmdb_movie.get(\"genres\", [])]),\n",
        "            \"original_language\": tmdb_movie.get(\"original_language\", \"\"),\n",
        "            \"production_countries\": \"|\".join([c[\"name\"] for c in tmdb_movie.get(\"production_countries\", [])]),\n",
        "            \"production_companies\": \"|\".join([c[\"name\"] for c in tmdb_movie.get(\"production_companies\", [])]),\n",
        "            \n",
        "            # Financial information\n",
        "            \"budget\": tmdb_movie.get(\"budget\", 0),\n",
        "            \"revenue\": tmdb_movie.get(\"revenue\", 0),\n",
        "        }\n",
        "        \n",
        "        # Cast and crew information\n",
        "        if \"credits\" in tmdb_movie:\n",
        "            credits = tmdb_movie[\"credits\"]\n",
        "            \n",
        "            # Directors\n",
        "            directors = [crew[\"name\"] for crew in credits.get(\"crew\", []) if crew.get(\"job\") == \"Director\"]\n",
        "            features[\"directors\"] = \"|\".join(directors)\n",
        "            \n",
        "            # Cast\n",
        "            cast = [actor[\"name\"] for actor in credits.get(\"cast\", [])[:10]]\n",
        "            features[\"cast\"] = \"|\".join(cast)\n",
        "        \n",
        "        # Keywords\n",
        "        if \"keywords\" in tmdb_movie and \"keywords\" in tmdb_movie[\"keywords\"]:\n",
        "            keywords = [kw[\"name\"] for kw in tmdb_movie[\"keywords\"][\"keywords\"][:15]]\n",
        "            features[\"keywords\"] = \"|\".join(keywords)\n",
        "        \n",
        "        return features\n",
        "\n",
        "# Check progress status\n",
        "processor = EnhancedTMDBProcessor(TMDB_API_KEY)\n",
        "current_processed = len(processor.processed_movies)\n",
        "\n",
        "print(f\"Current processing progress: {current_processed}/{TARGET_MOVIE_COUNT}\")\n",
        "\n",
        "# Intelligent progress management\n",
        "if current_processed >= TARGET_MOVIE_COUNT:\n",
        "    print(f\"Target achieved ({current_processed} >= {TARGET_MOVIE_COUNT})\")\n",
        "    print(\"Skipping download phase, using existing data\")\n",
        "    SKIP_DOWNLOAD = True\n",
        "else:\n",
        "    need_to_process = TARGET_MOVIE_COUNT - current_processed\n",
        "    print(f\"Remaining to process: {need_to_process} movies\")\n",
        "    SKIP_DOWNLOAD = False\n",
        "    \n",
        "print(\"\\nTMDB processor ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute TMDB data acquisition and image download\n",
        "if not SKIP_DOWNLOAD and 'target_movies' in locals():\n",
        "    print(\"Starting TMDB data processing and image download\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_movie_features = []\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    total_images = 0\n",
        "    \n",
        "    # Process each movie\n",
        "    for idx, (_, movie_info) in enumerate(tqdm(target_movies.iterrows(), total=len(target_movies), desc=\"Processing movies\")):\n",
        "        movie_id = movie_info[\"movie_id\"]\n",
        "        title = movie_info[\"title\"]\n",
        "        imdb_id = movie_info[\"imdb_id\"]\n",
        "        \n",
        "        print(f\"\\n[{idx+1}/{len(target_movies)}] {movie_id}. {title} -> tt{imdb_id}\")\n",
        "        \n",
        "        # Check if already processed\n",
        "        if str(movie_id) in processor.processed_movies:\n",
        "            print(f\"   Already processed, skipping\")\n",
        "            if str(movie_id) in processor.movie_features:\n",
        "                all_movie_features.append(processor.movie_features[str(movie_id)])\n",
        "            success_count += 1\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # 1. Find TMDB movie\n",
        "            tmdb_basic = processor.find_movie_by_imdb_id(imdb_id)\n",
        "            if not tmdb_basic:\n",
        "                print(f\"   ERROR: Not found in TMDB\")\n",
        "                processor.failed_movies.add(str(movie_id))\n",
        "                error_count += 1\n",
        "                continue\n",
        "            \n",
        "            # 2. Get detailed information\n",
        "            tmdb_movie = processor.get_movie_details(tmdb_basic[\"id\"])\n",
        "            if not tmdb_movie:\n",
        "                print(f\"   ERROR: Failed to get details\")\n",
        "                processor.failed_movies.add(str(movie_id))\n",
        "                error_count += 1\n",
        "                continue\n",
        "            \n",
        "            # 3. Extract features\n",
        "            features = processor.extract_movie_features(tmdb_movie, movie_info)\n",
        "            \n",
        "            # 4. Download images\n",
        "            downloaded_images = processor.download_movie_images(movie_id, title, tmdb_movie)\n",
        "            features[\"downloaded_images_count\"] = len(downloaded_images)\n",
        "            features[\"image_paths\"] = [path for _, path in downloaded_images]\n",
        "            total_images += len(downloaded_images)\n",
        "            \n",
        "            # 5. Record success\n",
        "            all_movie_features.append(features)\n",
        "            processor.processed_movies.add(str(movie_id))\n",
        "            processor.movie_features[str(movie_id)] = features\n",
        "            success_count += 1\n",
        "            \n",
        "            print(f\"   SUCCESS: {len(downloaded_images)} images, rating {features['vote_average']}/10\")\n",
        "            print(f\"   Genres: {features['genres']}\")\n",
        "            print(f\"   Directors: {features.get('directors', 'N/A')}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Processing failed: {str(e)}\")\n",
        "            processor.failed_movies.add(str(movie_id))\n",
        "            error_count += 1\n",
        "        \n",
        "        # Save progress periodically\n",
        "        if (idx + 1) % 5 == 0:\n",
        "            processor.save_progress()\n",
        "            print(f\"   Progress saved\")\n",
        "        \n",
        "        # API request interval\n",
        "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "    \n",
        "    # Final progress save\n",
        "    processor.save_progress()\n",
        "    \n",
        "    # Save movie feature data\n",
        "    if all_movie_features:\n",
        "        features_file = os.path.join(DATA_DIR, 'tmdb_movie_features.json')\n",
        "        with open(features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        csv_file = os.path.join(DATA_DIR, 'tmdb_movie_features.csv')\n",
        "        pd.DataFrame(all_movie_features).to_csv(csv_file, index=False, encoding='utf-8')\n",
        "        \n",
        "        print(f\"\\nMovie feature data saved:\")\n",
        "        print(f\"   JSON format: {features_file}\")\n",
        "        print(f\"   CSV format: {csv_file}\")\n",
        "    \n",
        "    # Output statistics\n",
        "    print(f\"\\nTMDB processing statistics:\")\n",
        "    print(f\"   Successful: {success_count} movies\")\n",
        "    print(f\"   Failed: {error_count} movies\")\n",
        "    print(f\"   Images downloaded: {total_images}\")\n",
        "    print(f\"   Success rate: {success_count/(success_count+error_count)*100:.1f}%\")\n",
        "    print(f\"   Target achieved: {success_count >= TARGET_MOVIE_COUNT}\")\n",
        "\n",
        "else:\n",
        "    # Load existing data\n",
        "    features_file = os.path.join(DATA_DIR, 'tmdb_movie_features.json')\n",
        "    if os.path.exists(features_file):\n",
        "        with open(features_file, 'r', encoding='utf-8') as f:\n",
        "            all_movie_features = json.load(f)\n",
        "        print(f\"Existing movie feature data loaded: {len(all_movie_features)} movies\")\n",
        "    else:\n",
        "        print(\"ERROR: No existing movie feature data found\")\n",
        "        all_movie_features = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image similarity calculation and selection - select 5 most diverse from 10\n",
        "print(\"Image Similarity Calculation and Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def calculate_image_similarity(img1_path, img2_path):\n",
        "    \"\"\"Calculate similarity between two images using histogram comparison\"\"\"\n",
        "    try:\n",
        "        # Read images\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "        \n",
        "        if img1 is None or img2 is None:\n",
        "            return 0.0\n",
        "        \n",
        "        # Convert to HSV color space\n",
        "        hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
        "        hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
        "        \n",
        "        # Calculate histograms\n",
        "        hist1 = cv2.calcHist([hsv1], [0, 1, 2], None, [50, 60, 60], [0, 180, 0, 256, 0, 256])\n",
        "        hist2 = cv2.calcHist([hsv2], [0, 1, 2], None, [50, 60, 60], [0, 180, 0, 256, 0, 256])\n",
        "        \n",
        "        # Calculate correlation\n",
        "        correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
        "        return correlation\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"       Similarity calculation failed: {str(e)}\")\n",
        "        return 0.0\n",
        "\n",
        "def select_diverse_images(image_paths, target_count=5):\n",
        "    \"\"\"Select the most diverse target_count images from the image list\"\"\"\n",
        "    if len(image_paths) <= target_count:\n",
        "        return image_paths\n",
        "    \n",
        "    # Calculate similarity matrix for all image pairs\n",
        "    n = len(image_paths)\n",
        "    similarity_matrix = np.zeros((n, n))\n",
        "    \n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            sim = calculate_image_similarity(image_paths[i], image_paths[j])\n",
        "            similarity_matrix[i][j] = sim\n",
        "            similarity_matrix[j][i] = sim\n",
        "    \n",
        "    # Greedy algorithm to select most dissimilar images\n",
        "    selected_indices = [0]  # Start with first image\n",
        "    \n",
        "    for _ in range(target_count - 1):\n",
        "        max_min_sim = -1\n",
        "        best_candidate = -1\n",
        "        \n",
        "        for candidate in range(n):\n",
        "            if candidate in selected_indices:\n",
        "                continue\n",
        "            \n",
        "            # Calculate minimum similarity with already selected images\n",
        "            min_sim = min(similarity_matrix[candidate][selected] for selected in selected_indices)\n",
        "            \n",
        "            if min_sim > max_min_sim:\n",
        "                max_min_sim = min_sim\n",
        "                best_candidate = candidate\n",
        "        \n",
        "        if best_candidate != -1:\n",
        "            selected_indices.append(best_candidate)\n",
        "    \n",
        "    return [image_paths[i] for i in selected_indices]\n",
        "\n",
        "# Filter images for each movie\n",
        "if 'all_movie_features' in locals() and all_movie_features:\n",
        "    print(f\"Starting image selection for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    selected_image_stats = []\n",
        "    total_selected = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Selecting images\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        \n",
        "        # Get image paths\n",
        "        image_paths = movie_features.get('image_paths', [])\n",
        "        \n",
        "        if not image_paths:\n",
        "            print(f\"   WARNING: {movie_id}. {title}: No images found\")\n",
        "            continue\n",
        "        \n",
        "        # Verify image files exist\n",
        "        valid_paths = [path for path in image_paths if os.path.exists(path)]\n",
        "        \n",
        "        if len(valid_paths) == 0:\n",
        "            print(f\"   ERROR: {movie_id}. {title}: Image files do not exist\")\n",
        "            continue\n",
        "        \n",
        "        # Select most diverse images\n",
        "        selected_paths = select_diverse_images(valid_paths, SELECTED_IMAGES)\n",
        "        \n",
        "        # Create selected image directory and copy images\n",
        "        selected_folder = os.path.join(SELECTED_IMAGE_DIR, f\"{movie_id:04d}_{title[:50]}\")\n",
        "        os.makedirs(selected_folder, exist_ok=True)\n",
        "        \n",
        "        copied_paths = []\n",
        "        for i, src_path in enumerate(selected_paths):\n",
        "            filename = f\"selected_{i+1}.jpg\"\n",
        "            dst_path = os.path.join(selected_folder, filename)\n",
        "            \n",
        "            try:\n",
        "                # Copy image\n",
        "                img = Image.open(src_path)\n",
        "                img.save(dst_path, \"JPEG\", quality=90)\n",
        "                copied_paths.append(dst_path)\n",
        "            except Exception as e:\n",
        "                print(f\"     ERROR: Failed to copy image: {str(e)}\")\n",
        "        \n",
        "        # Update feature data\n",
        "        movie_features['selected_image_paths'] = copied_paths\n",
        "        movie_features['selected_images_count'] = len(copied_paths)\n",
        "        \n",
        "        selected_image_stats.append({\n",
        "            'movie_id': movie_id,\n",
        "            'title': title,\n",
        "            'original_count': len(valid_paths),\n",
        "            'selected_count': len(copied_paths)\n",
        "        })\n",
        "        \n",
        "        total_selected += len(copied_paths)\n",
        "        \n",
        "        print(f\"   SUCCESS: {movie_id}. {title}: {len(valid_paths)} -> {len(copied_paths)} images\")\n",
        "    \n",
        "    # Save updated feature data\n",
        "    updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_selected_images.json')\n",
        "    with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"\\nImage selection statistics:\")\n",
        "    print(f\"   Movies processed: {len(selected_image_stats)}\")\n",
        "    print(f\"   Total selected images: {total_selected}\")\n",
        "    print(f\"   Average per movie: {total_selected/len(selected_image_stats):.1f} images\")\n",
        "    print(f\"   Updated data saved: {updated_features_file}\")\n",
        "    \n",
        "    # Display selection details\n",
        "    stats_df = pd.DataFrame(selected_image_stats)\n",
        "    print(f\"\\nSelection details:\")\n",
        "    print(f\"   Original images total: {stats_df['original_count'].sum()}\")\n",
        "    print(f\"   Selected images total: {stats_df['selected_count'].sum()}\")\n",
        "    print(f\"   Selection rate: {stats_df['selected_count'].sum()/stats_df['original_count'].sum()*100:.1f}%\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: No movie feature data available for image selection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ViT image feature extraction\n",
        "print(\"ViT Image Feature Extraction\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class ViTFeatureExtractor:\n",
        "    def __init__(self, model_name='google/vit-base-patch16-224'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Load ViT model and processor\n",
        "            self.processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "            self.model = ViTModel.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"ViT model loaded successfully: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: ViT model loading failed: {str(e)}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def extract_image_features(self, image_path):\n",
        "        \"\"\"Extract ViT features from a single image\"\"\"\n",
        "        if self.model is None:\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use [CLS] token features as image representation\n",
        "                image_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
        "            \n",
        "            return image_features\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Feature extraction failed ({image_path}): {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_movie_features(self, image_paths):\n",
        "        \"\"\"Extract and fuse features from all movie images\"\"\"\n",
        "        if not image_paths:\n",
        "            return None\n",
        "        \n",
        "        features_list = []\n",
        "        \n",
        "        for image_path in image_paths:\n",
        "            if os.path.exists(image_path):\n",
        "                features = self.extract_image_features(image_path)\n",
        "                if features is not None:\n",
        "                    features_list.append(features)\n",
        "        \n",
        "        if not features_list:\n",
        "            return None\n",
        "        \n",
        "        # Fuse multiple image features (average pooling)\n",
        "        combined_features = np.mean(features_list, axis=0)\n",
        "        return combined_features\n",
        "\n",
        "# Initialize ViT feature extractor\n",
        "vit_extractor = ViTFeatureExtractor()\n",
        "\n",
        "# Extract ViT features for all movies\n",
        "if 'all_movie_features' in locals() and all_movie_features and vit_extractor.model is not None:\n",
        "    print(f\"\\nStarting ViT feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    vit_features_matrix = []\n",
        "    vit_movie_ids = []\n",
        "    success_count = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Extracting ViT features\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        \n",
        "        # Use selected images\n",
        "        selected_paths = movie_features.get('selected_image_paths', [])\n",
        "        \n",
        "        if not selected_paths:\n",
        "            print(f\"   WARNING: {movie_id}. {title}: No selected images\")\n",
        "            continue\n",
        "        \n",
        "        # Extract features\n",
        "        vit_features = vit_extractor.extract_movie_features(selected_paths)\n",
        "        \n",
        "        if vit_features is not None:\n",
        "            vit_features_matrix.append(vit_features)\n",
        "            vit_movie_ids.append(movie_id)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            movie_features['vit_features'] = vit_features.tolist()\n",
        "            movie_features['vit_feature_dim'] = len(vit_features)\n",
        "            \n",
        "            success_count += 1\n",
        "            print(f\"   SUCCESS: {movie_id}. {title}: feature dimension {len(vit_features)}\")\n",
        "        else:\n",
        "            print(f\"   ERROR: {movie_id}. {title}: ViT feature extraction failed\")\n",
        "    \n",
        "    # Convert to numpy array and save\n",
        "    if vit_features_matrix:\n",
        "        vit_features_array = np.array(vit_features_matrix)\n",
        "        \n",
        "        # Save ViT feature matrix\n",
        "        vit_features_file = os.path.join(DATA_DIR, 'vit_features_matrix.npy')\n",
        "        np.save(vit_features_file, vit_features_array)\n",
        "        \n",
        "        # Save movie ID mapping\n",
        "        vit_mapping_file = os.path.join(DATA_DIR, 'vit_movie_mapping.json')\n",
        "        with open(vit_mapping_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({'movie_ids': vit_movie_ids, 'feature_dim': len(vit_features_matrix[0])}, f)\n",
        "        \n",
        "        print(f\"\\nViT feature extraction statistics:\")\n",
        "        print(f\"   Successfully processed movies: {success_count}\")\n",
        "        print(f\"   Feature dimension: {vit_features_array.shape[1]}\")\n",
        "        print(f\"   Feature matrix shape: {vit_features_array.shape}\")\n",
        "        print(f\"   Feature matrix saved: {vit_features_file}\")\n",
        "        print(f\"   Mapping file saved: {vit_mapping_file}\")\n",
        "        \n",
        "        # Save updated movie features\n",
        "        updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_vit.json')\n",
        "        with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"   Updated feature data saved: {updated_features_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: No ViT features extracted successfully\")\n",
        "else:\n",
        "    print(\"ERROR: ViT model not loaded or no movie data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT text feature extraction\n",
        "print(\"BERT Text Feature Extraction\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "class BERTFeatureExtractor:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Load BERT model and tokenizer\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "            self.model = BertModel.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"BERT model loaded successfully: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: BERT model loading failed: {str(e)}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def extract_text_features(self, text, max_length=512):\n",
        "        \"\"\"Extract BERT features from text\"\"\"\n",
        "        if self.model is None or not text or text.strip() == \"\":\n",
        "            return None\n",
        "        \n",
        "        try:\n",
        "            # Text preprocessing and tokenization\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            \n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use [CLS] token features as sentence representation\n",
        "                text_features = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
        "            \n",
        "            return text_features\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"   ERROR: Text feature extraction failed: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def extract_movie_text_features(self, overview, tagline):\n",
        "        \"\"\"Extract movie text features (overview + tagline)\"\"\"\n",
        "        features_list = []\n",
        "        \n",
        "        # Extract overview features\n",
        "        if overview and overview.strip():\n",
        "            overview_features = self.extract_text_features(overview)\n",
        "            if overview_features is not None:\n",
        "                features_list.append(overview_features)\n",
        "        \n",
        "        # Extract tagline features\n",
        "        if tagline and tagline.strip():\n",
        "            tagline_features = self.extract_text_features(tagline)\n",
        "            if tagline_features is not None:\n",
        "                features_list.append(tagline_features)\n",
        "        \n",
        "        if not features_list:\n",
        "            return None\n",
        "        \n",
        "        # Average pooling to fuse features\n",
        "        combined_features = np.mean(features_list, axis=0)\n",
        "        return combined_features\n",
        "\n",
        "# Initialize BERT feature extractor\n",
        "bert_extractor = BERTFeatureExtractor()\n",
        "\n",
        "# Extract BERT text features for all movies\n",
        "if 'all_movie_features' in locals() and all_movie_features and bert_extractor.model is not None:\n",
        "    print(f\"\\nStarting BERT text feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    bert_features_matrix = []\n",
        "    bert_movie_ids = []\n",
        "    success_count = 0\n",
        "    \n",
        "    for movie_features in tqdm(all_movie_features, desc=\"Extracting BERT features\"):\n",
        "        movie_id = movie_features['movielens_id']\n",
        "        title = movie_features['movielens_title']\n",
        "        overview = movie_features.get('overview', '')\n",
        "        tagline = movie_features.get('tagline', '')\n",
        "        \n",
        "        print(f\"   Processing {movie_id}. {title}\")\n",
        "        print(f\"     Overview length: {len(overview) if overview else 0} characters\")\n",
        "        print(f\"     Tagline length: {len(tagline) if tagline else 0} characters\")\n",
        "        \n",
        "        # Extract text features\n",
        "        bert_features = bert_extractor.extract_movie_text_features(overview, tagline)\n",
        "        \n",
        "        if bert_features is not None:\n",
        "            bert_features_matrix.append(bert_features)\n",
        "            bert_movie_ids.append(movie_id)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            movie_features['bert_features'] = bert_features.tolist()\n",
        "            movie_features['bert_feature_dim'] = len(bert_features)\n",
        "            movie_features['text_length'] = len(overview) + len(tagline)\n",
        "            \n",
        "            success_count += 1\n",
        "            print(f\"     SUCCESS: BERT feature dimension: {len(bert_features)}\")\n",
        "        else:\n",
        "            print(f\"     ERROR: BERT feature extraction failed\")\n",
        "    \n",
        "    # Convert to numpy array and save\n",
        "    if bert_features_matrix:\n",
        "        bert_features_array = np.array(bert_features_matrix)\n",
        "        \n",
        "        # Save BERT feature matrix\n",
        "        bert_features_file = os.path.join(DATA_DIR, 'bert_features_matrix.npy')\n",
        "        np.save(bert_features_file, bert_features_array)\n",
        "        \n",
        "        # Save movie ID mapping\n",
        "        bert_mapping_file = os.path.join(DATA_DIR, 'bert_movie_mapping.json')\n",
        "        with open(bert_mapping_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({'movie_ids': bert_movie_ids, 'feature_dim': len(bert_features_matrix[0])}, f)\n",
        "        \n",
        "        print(f\"\\nBERT feature extraction statistics:\")\n",
        "        print(f\"   Successfully processed movies: {success_count}\")\n",
        "        print(f\"   Feature dimension: {bert_features_array.shape[1]}\")\n",
        "        print(f\"   Feature matrix shape: {bert_features_array.shape}\")\n",
        "        print(f\"   Feature matrix saved: {bert_features_file}\")\n",
        "        print(f\"   Mapping file saved: {bert_mapping_file}\")\n",
        "        \n",
        "        # Save updated movie features\n",
        "        updated_features_file = os.path.join(DATA_DIR, 'movie_features_with_bert.json')\n",
        "        with open(updated_features_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"   Updated feature data saved: {updated_features_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: No BERT features extracted successfully\")\n",
        "else:\n",
        "    print(\"ERROR: BERT model not loaded or no movie data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cast & crew statistical feature engineering\n",
        "print(\"Cast & Crew Statistical Feature Engineering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def extract_cast_crew_features(all_movie_features, top_n=50):\n",
        "    \"\"\"Extract cast & crew statistical features\"\"\"\n",
        "    if not all_movie_features:\n",
        "        return None, None, None\n",
        "    \n",
        "    # Count frequency of all directors and cast\n",
        "    all_directors = []\n",
        "    all_cast = []\n",
        "    \n",
        "    for movie in all_movie_features:\n",
        "        # Directors\n",
        "        directors = movie.get('directors', '')\n",
        "        if directors:\n",
        "            all_directors.extend([d.strip() for d in directors.split('|') if d.strip()])\n",
        "        \n",
        "        # Cast\n",
        "        cast = movie.get('cast', '')\n",
        "        if cast:\n",
        "            all_cast.extend([c.strip() for c in cast.split('|') if c.strip()])\n",
        "    \n",
        "    # Count frequencies\n",
        "    director_counts = Counter(all_directors)\n",
        "    cast_counts = Counter(all_cast)\n",
        "    \n",
        "    # Get high-frequency cast & crew\n",
        "    top_directors = [director for director, count in director_counts.most_common(top_n)]\n",
        "    top_cast = [actor for actor, count in cast_counts.most_common(top_n)]\n",
        "    \n",
        "    print(f\"Cast & crew statistics:\")\n",
        "    print(f\"   Total directors: {len(director_counts)} people\")\n",
        "    print(f\"   Total cast: {len(cast_counts)} people\")\n",
        "    print(f\"   Selected high-frequency directors: {len(top_directors)} people\")\n",
        "    print(f\"   Selected high-frequency cast: {len(top_cast)} people\")\n",
        "    \n",
        "    # Display TOP 10 directors and cast\n",
        "    print(f\"\\nTOP 10 high-frequency directors:\")\n",
        "    for i, (director, count) in enumerate(director_counts.most_common(10), 1):\n",
        "        print(f\"   {i:2d}. {director}: {count} movies\")\n",
        "    \n",
        "    print(f\"\\nTOP 10 high-frequency cast:\")\n",
        "    for i, (actor, count) in enumerate(cast_counts.most_common(10), 1):\n",
        "        print(f\"   {i:2d}. {actor}: {count} movies\")\n",
        "    \n",
        "    return top_directors, top_cast, (director_counts, cast_counts)\n",
        "\n",
        "def create_cast_crew_features(all_movie_features, top_directors, top_cast):\n",
        "    \"\"\"Create cast & crew feature vectors for each movie\"\"\"\n",
        "    if not all_movie_features or not top_directors or not top_cast:\n",
        "        return None, None\n",
        "    \n",
        "    cast_crew_matrix = []\n",
        "    movie_ids = []\n",
        "    \n",
        "    for movie in all_movie_features:\n",
        "        movie_id = movie['movielens_id']\n",
        "        \n",
        "        # Director features (one-hot encoding)\n",
        "        director_features = np.zeros(len(top_directors))\n",
        "        directors = movie.get('directors', '')\n",
        "        if directors:\n",
        "            movie_directors = [d.strip() for d in directors.split('|') if d.strip()]\n",
        "            for i, top_director in enumerate(top_directors):\n",
        "                if top_director in movie_directors:\n",
        "                    director_features[i] = 1\n",
        "        \n",
        "        # Cast features (one-hot encoding)\n",
        "        cast_features = np.zeros(len(top_cast))\n",
        "        cast = movie.get('cast', '')\n",
        "        if cast:\n",
        "            movie_cast = [c.strip() for c in cast.split('|') if c.strip()]\n",
        "            for i, top_actor in enumerate(top_cast):\n",
        "                if top_actor in movie_cast:\n",
        "                    cast_features[i] = 1\n",
        "        \n",
        "        # Combine features\n",
        "        combined_features = np.concatenate([director_features, cast_features])\n",
        "        cast_crew_matrix.append(combined_features)\n",
        "        movie_ids.append(movie_id)\n",
        "    \n",
        "    return np.array(cast_crew_matrix), movie_ids\n",
        "\n",
        "# Execute cast & crew feature engineering\n",
        "if 'all_movie_features' in locals() and all_movie_features:\n",
        "    print(f\"Starting cast & crew feature extraction for {len(all_movie_features)} movies\")\n",
        "    \n",
        "    # Extract high-frequency cast & crew\n",
        "    top_directors, top_cast, (director_counts, cast_counts) = extract_cast_crew_features(all_movie_features)\n",
        "    \n",
        "    if top_directors and top_cast:\n",
        "        # Create cast & crew feature matrix\n",
        "        cast_crew_matrix, cc_movie_ids = create_cast_crew_features(\n",
        "            all_movie_features, top_directors, top_cast\n",
        "        )\n",
        "        \n",
        "        if cast_crew_matrix is not None:\n",
        "            # Save cast & crew features\n",
        "            cast_crew_file = os.path.join(DATA_DIR, 'cast_crew_features.npy')\n",
        "            np.save(cast_crew_file, cast_crew_matrix)\n",
        "            \n",
        "            # Save cast & crew mapping information\n",
        "            cast_crew_mapping = {\n",
        "                'movie_ids': cc_movie_ids,\n",
        "                'top_directors': top_directors,\n",
        "                'top_cast': top_cast,\n",
        "                'director_feature_dim': len(top_directors),\n",
        "                'cast_feature_dim': len(top_cast),\n",
        "                'total_feature_dim': len(top_directors) + len(top_cast)\n",
        "            }\n",
        "            \n",
        "            mapping_file = os.path.join(DATA_DIR, 'cast_crew_mapping.json')\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(cast_crew_mapping, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            # Update movie feature data\n",
        "            for i, movie in enumerate(all_movie_features):\n",
        "                if movie['movielens_id'] in cc_movie_ids:\n",
        "                    idx = cc_movie_ids.index(movie['movielens_id'])\n",
        "                    movie['cast_crew_features'] = cast_crew_matrix[idx].tolist()\n",
        "                    movie['cast_crew_feature_dim'] = len(cast_crew_matrix[idx])\n",
        "            \n",
        "            print(f\"\\nCast & crew feature statistics:\")\n",
        "            print(f\"   Movies processed: {len(cc_movie_ids)}\")\n",
        "            print(f\"   Director feature dimension: {len(top_directors)}\")\n",
        "            print(f\"   Cast feature dimension: {len(top_cast)}\")\n",
        "            print(f\"   Total feature dimension: {cast_crew_matrix.shape[1]}\")\n",
        "            print(f\"   Feature matrix shape: {cast_crew_matrix.shape}\")\n",
        "            print(f\"   Feature matrix saved: {cast_crew_file}\")\n",
        "            print(f\"   Mapping file saved: {mapping_file}\")\n",
        "            \n",
        "            # Analyze feature sparsity\n",
        "            sparsity = 1 - np.count_nonzero(cast_crew_matrix) / cast_crew_matrix.size\n",
        "            print(f\"   Feature sparsity: {sparsity*100:.2f}%\")\n",
        "            print(f\"   Average high-frequency cast & crew per movie: {np.mean(np.sum(cast_crew_matrix, axis=1)):.1f}\")\n",
        "            \n",
        "            # Save final movie feature data\n",
        "            final_features_file = os.path.join(DATA_DIR, 'movie_features_complete.json')\n",
        "            with open(final_features_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(all_movie_features, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"   Complete feature data saved: {final_features_file}\")\n",
        "        else:\n",
        "            print(\"ERROR: Cast & crew feature matrix creation failed\")\n",
        "    else:\n",
        "        print(\"ERROR: High-frequency cast & crew extraction failed\")\n",
        "else:\n",
        "    print(\"ERROR: No movie feature data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multimodal feature fusion\n",
        "print(\"Multimodal Feature Fusion\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def load_and_normalize_features():\n",
        "    \"\"\"Load and normalize all features\"\"\"\n",
        "    features_data = {}\n",
        "    \n",
        "    # 1. Load ViT image features\n",
        "    vit_file = os.path.join(DATA_DIR, 'vit_features_matrix.npy')\n",
        "    vit_mapping_file = os.path.join(DATA_DIR, 'vit_movie_mapping.json')\n",
        "    \n",
        "    if os.path.exists(vit_file) and os.path.exists(vit_mapping_file):\n",
        "        vit_features = np.load(vit_file)\n",
        "        with open(vit_mapping_file, 'r') as f:\n",
        "            vit_mapping = json.load(f)\n",
        "        \n",
        "        # Standardize ViT features\n",
        "        scaler_vit = StandardScaler()\n",
        "        vit_features_normalized = scaler_vit.fit_transform(vit_features)\n",
        "        \n",
        "        features_data['vit'] = {\n",
        "            'features': vit_features_normalized,\n",
        "            'movie_ids': vit_mapping['movie_ids'],\n",
        "            'dim': vit_features_normalized.shape[1],\n",
        "            'scaler': scaler_vit\n",
        "        }\n",
        "        print(f\"ViT features loaded: {vit_features_normalized.shape} (standardized)\")\n",
        "    else:\n",
        "        print(\"WARNING: ViT feature files not found\")\n",
        "    \n",
        "    # 2. Load BERT text features\n",
        "    bert_file = os.path.join(DATA_DIR, 'bert_features_matrix.npy')\n",
        "    bert_mapping_file = os.path.join(DATA_DIR, 'bert_movie_mapping.json')\n",
        "    \n",
        "    if os.path.exists(bert_file) and os.path.exists(bert_mapping_file):\n",
        "        bert_features = np.load(bert_file)\n",
        "        with open(bert_mapping_file, 'r') as f:\n",
        "            bert_mapping = json.load(f)\n",
        "        \n",
        "        # Standardize BERT features\n",
        "        scaler_bert = StandardScaler()\n",
        "        bert_features_normalized = scaler_bert.fit_transform(bert_features)\n",
        "        \n",
        "        features_data['bert'] = {\n",
        "            'features': bert_features_normalized,\n",
        "            'movie_ids': bert_mapping['movie_ids'],\n",
        "            'dim': bert_features_normalized.shape[1],\n",
        "            'scaler': scaler_bert\n",
        "        }\n",
        "        print(f\"BERT features loaded: {bert_features_normalized.shape} (standardized)\")\n",
        "    else:\n",
        "        print(\"WARNING: BERT feature files not found\")\n",
        "    \n",
        "    # 3. Load cast & crew features\n",
        "    cast_crew_file = os.path.join(DATA_DIR, 'cast_crew_features.npy')\n",
        "    cast_crew_mapping_file = os.path.join(DATA_DIR, 'cast_crew_mapping.json')\n",
        "    \n",
        "    if os.path.exists(cast_crew_file) and os.path.exists(cast_crew_mapping_file):\n",
        "        cast_crew_features = np.load(cast_crew_file)\n",
        "        with open(cast_crew_mapping_file, 'r') as f:\n",
        "            cast_crew_mapping = json.load(f)\n",
        "        \n",
        "        # Cast & crew features are already 0-1 encoded, no standardization needed\n",
        "        features_data['cast_crew'] = {\n",
        "            'features': cast_crew_features,\n",
        "            'movie_ids': cast_crew_mapping['movie_ids'],\n",
        "            'dim': cast_crew_features.shape[1],\n",
        "            'directors_dim': cast_crew_mapping['director_feature_dim'],\n",
        "            'cast_dim': cast_crew_mapping['cast_feature_dim']\n",
        "        }\n",
        "        print(f\"Cast & crew features loaded: {cast_crew_features.shape} (one-hot encoded)\")\n",
        "    else:\n",
        "        print(\"WARNING: Cast & crew feature files not found\")\n",
        "    \n",
        "    return features_data\n",
        "\n",
        "def create_multimodal_features(features_data, fusion_method='concatenate'):\n",
        "    \"\"\"Create multimodal fused features\"\"\"\n",
        "    if not features_data:\n",
        "        return None, None\n",
        "    \n",
        "    # Find intersection of movie IDs from all features\n",
        "    movie_id_sets = [set(data['movie_ids']) for data in features_data.values()]\n",
        "    common_movie_ids = set.intersection(*movie_id_sets)\n",
        "    common_movie_ids = sorted(list(common_movie_ids))\n",
        "    \n",
        "    print(f\"Feature intersection statistics:\")\n",
        "    for feature_name, data in features_data.items():\n",
        "        print(f\"   {feature_name}: {len(data['movie_ids'])} movies\")\n",
        "    print(f\"   Intersection: {len(common_movie_ids)} movies\")\n",
        "    \n",
        "    if not common_movie_ids:\n",
        "        print(\"ERROR: No common movie IDs found\")\n",
        "        return None, None\n",
        "    \n",
        "    # Align feature matrices\n",
        "    aligned_features = []\n",
        "    feature_dims = []\n",
        "    feature_names = []\n",
        "    \n",
        "    for feature_name, data in features_data.items():\n",
        "        movie_ids = data['movie_ids']\n",
        "        features = data['features']\n",
        "        \n",
        "        # Create index mapping\n",
        "        id_to_idx = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
        "        \n",
        "        # Reorder features according to intersection IDs\n",
        "        aligned_feature = np.array([features[id_to_idx[movie_id]] for movie_id in common_movie_ids])\n",
        "        aligned_features.append(aligned_feature)\n",
        "        feature_dims.append(aligned_feature.shape[1])\n",
        "        feature_names.append(feature_name)\n",
        "        \n",
        "        print(f\"   {feature_name}: {aligned_feature.shape}\")\n",
        "    \n",
        "    # Feature fusion\n",
        "    if fusion_method == 'concatenate':\n",
        "        # Simple concatenation\n",
        "        multimodal_features = np.concatenate(aligned_features, axis=1)\n",
        "        print(f\"\\nConcatenation fusion result: {multimodal_features.shape}\")\n",
        "    \n",
        "    elif fusion_method == 'weighted_average':\n",
        "        # Weighted average (requires consistent feature dimensions, using PCA for dimensionality reduction)\n",
        "        target_dim = min(feature_dims)\n",
        "        reduced_features = []\n",
        "        \n",
        "        for i, feature in enumerate(aligned_features):\n",
        "            if feature.shape[1] > target_dim:\n",
        "                pca = PCA(n_components=target_dim)\n",
        "                reduced_feature = pca.fit_transform(feature)\n",
        "                reduced_features.append(reduced_feature)\n",
        "                print(f\"   {feature_names[i]}: {feature.shape} -> {reduced_feature.shape} (PCA)\")\n",
        "            else:\n",
        "                reduced_features.append(feature)\n",
        "        \n",
        "        # Equal weight average\n",
        "        multimodal_features = np.mean(reduced_features, axis=0)\n",
        "        print(f\"\\nWeighted average fusion result: {multimodal_features.shape}\")\n",
        "    \n",
        "    else:\n",
        "        # Default concatenation\n",
        "        multimodal_features = np.concatenate(aligned_features, axis=1)\n",
        "    \n",
        "    return multimodal_features, common_movie_ids\n",
        "\n",
        "# Execute multimodal feature fusion\n",
        "print(\"Starting multimodal feature fusion\")\n",
        "\n",
        "# Load and normalize features\n",
        "features_data = load_and_normalize_features()\n",
        "\n",
        "if features_data:\n",
        "    # Create multiple fusion versions\n",
        "    fusion_methods = ['concatenate', 'weighted_average']\n",
        "    multimodal_results = {}\n",
        "    \n",
        "    for method in fusion_methods:\n",
        "        print(f\"\\nUsing {method} method for feature fusion\")\n",
        "        multimodal_features, movie_ids = create_multimodal_features(features_data, method)\n",
        "        \n",
        "        if multimodal_features is not None:\n",
        "            multimodal_results[method] = {\n",
        "                'features': multimodal_features,\n",
        "                'movie_ids': movie_ids\n",
        "            }\n",
        "            \n",
        "            # Save fused features\n",
        "            fusion_file = os.path.join(DATA_DIR, f'multimodal_features_{method}.npy')\n",
        "            np.save(fusion_file, multimodal_features)\n",
        "            \n",
        "            # Save movie ID mapping\n",
        "            mapping_file = os.path.join(DATA_DIR, f'multimodal_mapping_{method}.json')\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'movie_ids': movie_ids,\n",
        "                    'feature_dim': multimodal_features.shape[1],\n",
        "                    'fusion_method': method,\n",
        "                    'component_dims': {name: data['dim'] for name, data in features_data.items()}\n",
        "                }, f, indent=2)\n",
        "            \n",
        "            print(f\"   {method} fused features saved: {fusion_file}\")\n",
        "            print(f\"   Mapping file saved: {mapping_file}\")\n",
        "    \n",
        "    if multimodal_results:\n",
        "        print(f\"\\nMultimodal feature fusion completed\")\n",
        "        print(f\"Fusion results summary:\")\n",
        "        for method, result in multimodal_results.items():\n",
        "            features = result['features']\n",
        "            print(f\"   {method}: {features.shape} (movies x feature_dim)\")\n",
        "        \n",
        "        # Save feature statistics\n",
        "        stats = {\n",
        "            'total_movies': len(movie_ids),\n",
        "            'fusion_methods': list(multimodal_results.keys()),\n",
        "            'feature_sources': list(features_data.keys()),\n",
        "            'individual_dims': {name: data['dim'] for name, data in features_data.items()}\n",
        "        }\n",
        "        \n",
        "        stats_file = os.path.join(DATA_DIR, 'multimodal_stats.json')\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(f\"   Statistics saved: {stats_file}\")\n",
        "    else:\n",
        "        print(\"ERROR: Multimodal feature fusion failed\")\n",
        "else:\n",
        "    print(\"ERROR: No available feature data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recommendation system implementation and performance comparison\n",
        "print(\"Recommendation System Implementation and Performance Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import traditional recommendation algorithms (reuse previous implementation)\n",
        "class UserBasedCF:\n",
        "    def __init__(self, rating_matrix, user_features=None, use_features=False):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.user_features = user_features\n",
        "        self.use_features = use_features\n",
        "        self.user_similarity = None\n",
        "        \n",
        "    def compute_similarity(self):\n",
        "        if self.use_features and self.user_features is not None:\n",
        "            # Feature-based similarity\n",
        "            self.user_similarity = cosine_similarity(self.user_features)\n",
        "        else:\n",
        "            # Rating-based similarity\n",
        "            self.user_similarity = cosine_similarity(self.rating_matrix)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.user_similarity is None:\n",
        "            self.compute_similarity()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        # Find k most similar users\n",
        "        similarities = self.user_similarity[user_idx]\n",
        "        similar_users = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        # Predict rating\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        \n",
        "        for similar_user in similar_users:\n",
        "            if self.rating_matrix[similar_user, item_idx] != 0:\n",
        "                sim = similarities[similar_user]\n",
        "                numerator += sim * self.rating_matrix[similar_user, item_idx]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator == 0:\n",
        "            return np.mean(user_ratings[user_ratings != 0]) if np.any(user_ratings != 0) else 3.0\n",
        "        \n",
        "        return numerator / denominator\n",
        "\n",
        "class ItemBasedCF:\n",
        "    def __init__(self, rating_matrix, item_features=None, use_features=False):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.item_features = item_features\n",
        "        self.use_features = use_features\n",
        "        self.item_similarity = None\n",
        "        \n",
        "    def compute_similarity(self):\n",
        "        if self.use_features and self.item_features is not None:\n",
        "            # Feature-based similarity\n",
        "            self.item_similarity = cosine_similarity(self.item_features.T)\n",
        "        else:\n",
        "            # Rating-based similarity\n",
        "            self.item_similarity = cosine_similarity(self.rating_matrix.T)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.item_similarity is None:\n",
        "            self.compute_similarity()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        # Find k most similar items\n",
        "        similarities = self.item_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        # Predict rating\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        \n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator == 0:\n",
        "            item_ratings = self.rating_matrix[:, item_idx]\n",
        "            return np.mean(item_ratings[item_ratings != 0]) if np.any(item_ratings != 0) else 3.0\n",
        "        \n",
        "        return numerator / denominator\n",
        "\n",
        "class HybridRecommender:\n",
        "    def __init__(self, user_cf, item_cf, user_weight=0.5, item_weight=0.5):\n",
        "        self.user_cf = user_cf\n",
        "        self.item_cf = item_cf\n",
        "        self.user_weight = user_weight\n",
        "        self.item_weight = item_weight\n",
        "        \n",
        "        # Ensure weights sum to 1\n",
        "        total_weight = user_weight + item_weight\n",
        "        self.user_weight = user_weight / total_weight\n",
        "        self.item_weight = item_weight / total_weight\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        user_pred = self.user_cf.predict(user_idx, item_idx, k)\n",
        "        item_pred = self.item_cf.predict(user_idx, item_idx, k)\n",
        "        \n",
        "        return self.user_weight * user_pred + self.item_weight * item_pred\n",
        "\n",
        "class MultimodalRecommender:\n",
        "    \"\"\"Enhanced multimodal recommendation system\"\"\"\n",
        "    def __init__(self, rating_matrix, multimodal_features, user_features=None, \n",
        "                 alpha=0.5, beta=0.3, gamma=0.2):\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.multimodal_features = multimodal_features\n",
        "        self.user_features = user_features\n",
        "        self.alpha = alpha  # Rating weight\n",
        "        self.beta = beta    # Multimodal feature weight\n",
        "        self.gamma = gamma  # User feature weight\n",
        "        \n",
        "        # Normalize weights\n",
        "        total = alpha + beta + gamma\n",
        "        self.alpha = alpha / total\n",
        "        self.beta = beta / total\n",
        "        self.gamma = gamma / total\n",
        "        \n",
        "        self.rating_similarity = None\n",
        "        self.content_similarity = None\n",
        "        self.user_similarity = None\n",
        "    \n",
        "    def compute_similarities(self):\n",
        "        # Rating-based similarity\n",
        "        self.rating_similarity = cosine_similarity(self.rating_matrix.T)\n",
        "        \n",
        "        # Multimodal feature-based similarity\n",
        "        self.content_similarity = cosine_similarity(self.multimodal_features)\n",
        "        \n",
        "        # User feature-based similarity (if available)\n",
        "        if self.user_features is not None:\n",
        "            self.user_similarity = cosine_similarity(self.user_features)\n",
        "    \n",
        "    def predict(self, user_idx, item_idx, k=50):\n",
        "        if self.rating_similarity is None:\n",
        "            self.compute_similarities()\n",
        "        \n",
        "        user_ratings = self.rating_matrix[user_idx]\n",
        "        if user_ratings[item_idx] != 0:\n",
        "            return user_ratings[item_idx]\n",
        "        \n",
        "        predictions = []\n",
        "        weights = []\n",
        "        \n",
        "        # 1. Rating-based collaborative filtering prediction\n",
        "        similarities = self.rating_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        numerator = denominator = 0\n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator > 0:\n",
        "            rating_pred = numerator / denominator\n",
        "            predictions.append(rating_pred)\n",
        "            weights.append(self.alpha)\n",
        "        \n",
        "        # 2. Multimodal content-based prediction\n",
        "        similarities = self.content_similarity[item_idx]\n",
        "        similar_items = np.argsort(similarities)[::-1][1:k+1]\n",
        "        \n",
        "        numerator = denominator = 0\n",
        "        for similar_item in similar_items:\n",
        "            if user_ratings[similar_item] != 0:\n",
        "                sim = similarities[similar_item]\n",
        "                numerator += sim * user_ratings[similar_item]\n",
        "                denominator += abs(sim)\n",
        "        \n",
        "        if denominator > 0:\n",
        "            content_pred = numerator / denominator\n",
        "            predictions.append(content_pred)\n",
        "            weights.append(self.beta)\n",
        "        \n",
        "        # 3. User feature-based prediction (if available)\n",
        "        if self.user_similarity is not None:\n",
        "            similarities = self.user_similarity[user_idx]\n",
        "            similar_users = np.argsort(similarities)[::-1][1:k+1]\n",
        "            \n",
        "            numerator = denominator = 0\n",
        "            for similar_user in similar_users:\n",
        "                if self.rating_matrix[similar_user, item_idx] != 0:\n",
        "                    sim = similarities[similar_user]\n",
        "                    numerator += sim * self.rating_matrix[similar_user, item_idx]\n",
        "                    denominator += abs(sim)\n",
        "            \n",
        "            if denominator > 0:\n",
        "                user_pred = numerator / denominator\n",
        "                predictions.append(user_pred)\n",
        "                weights.append(self.gamma)\n",
        "        \n",
        "        # Weighted average prediction\n",
        "        if predictions:\n",
        "            final_pred = np.average(predictions, weights=weights)\n",
        "            return final_pred\n",
        "        else:\n",
        "            # Default prediction\n",
        "            return 3.0\n",
        "\n",
        "print(\"Recommendation system classes defined successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preparation and model training\n",
        "print(\"Data Preparation and Model Training\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load cleaned rating data\n",
        "cleaned_data_file = os.path.join(DATA_DIR, 'cleaned_ratings_data.csv')\n",
        "if os.path.exists(cleaned_data_file):\n",
        "    ratings_data = pd.read_csv(cleaned_data_file)\n",
        "    print(f\"Rating data loaded: {len(ratings_data)} entries\")\n",
        "else:\n",
        "    print(\"ERROR: Cleaned rating data not found\")\n",
        "    ratings_data = None\n",
        "\n",
        "if ratings_data is not None:\n",
        "    # Create user-movie rating matrix\n",
        "    from scipy.sparse import csr_matrix\n",
        "    \n",
        "    # Remap user and movie IDs to continuous indices\n",
        "    unique_users = sorted(ratings_data['user_id'].unique())\n",
        "    unique_movies = sorted(ratings_data['movie_id'].unique())\n",
        "    \n",
        "    user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
        "    movie_to_idx = {movie: idx for idx, movie in enumerate(unique_movies)}\n",
        "    idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
        "    idx_to_movie = {idx: movie for movie, idx in movie_to_idx.items()}\n",
        "    \n",
        "    print(f\"Rating matrix dimensions: {len(unique_users)} users x {len(unique_movies)} movies\")\n",
        "    \n",
        "    # Create rating matrix\n",
        "    rating_matrix = np.zeros((len(unique_users), len(unique_movies)))\n",
        "    \n",
        "    for _, row in ratings_data.iterrows():\n",
        "        user_idx = user_to_idx[row['user_id']]\n",
        "        movie_idx = movie_to_idx[row['movie_id']]\n",
        "        rating_matrix[user_idx, movie_idx] = row['rating']\n",
        "    \n",
        "    print(f\"Rating matrix created: {rating_matrix.shape}\")\n",
        "    print(f\"Sparsity: {(1 - np.count_nonzero(rating_matrix) / rating_matrix.size) * 100:.2f}%\")\n",
        "    \n",
        "    # Create user feature matrix\n",
        "    user_features_list = ['age', 'gender', 'occupation']\n",
        "    user_feature_matrix = np.zeros((len(unique_users), len(user_features_list) + 21))  # +21 for occupation one-hot\n",
        "    \n",
        "    for user_id in unique_users:\n",
        "        user_data = ratings_data[ratings_data['user_id'] == user_id].iloc[0]\n",
        "        user_idx = user_to_idx[user_id]\n",
        "        \n",
        "        # Age feature (normalized)\n",
        "        user_feature_matrix[user_idx, 0] = user_data['age'] / 100.0\n",
        "        \n",
        "        # Gender feature (M=1, F=0)\n",
        "        user_feature_matrix[user_idx, 1] = 1 if user_data['gender'] == 'M' else 0\n",
        "        \n",
        "        # Occupation feature (one-hot encoding, simplified to top 20 common occupations)\n",
        "        occupation_map = {\n",
        "            'student': 2, 'other': 3, 'educator': 4, 'administrator': 5,\n",
        "            'engineer': 6, 'programmer': 7, 'librarian': 8, 'writer': 9,\n",
        "            'executive': 10, 'scientist': 11, 'artist': 12, 'technician': 13,\n",
        "            'marketing': 14, 'entertainment': 15, 'healthcare': 16, 'retired': 17,\n",
        "            'lawyer': 18, 'salesman': 19, 'homemaker': 20, 'doctor': 21\n",
        "        }\n",
        "        \n",
        "        occupation = user_data['occupation']\n",
        "        if occupation in occupation_map:\n",
        "            user_feature_matrix[user_idx, occupation_map[occupation]] = 1\n",
        "    \n",
        "    print(f\"User feature matrix created: {user_feature_matrix.shape}\")\n",
        "    \n",
        "    # Load multimodal features (if available)\n",
        "    multimodal_features = None\n",
        "    multimodal_movie_ids = None\n",
        "    \n",
        "    # Try to load concatenated multimodal features\n",
        "    multimodal_file = os.path.join(DATA_DIR, 'multimodal_features_concatenate.npy')\n",
        "    multimodal_mapping_file = os.path.join(DATA_DIR, 'multimodal_mapping_concatenate.json')\n",
        "    \n",
        "    if os.path.exists(multimodal_file) and os.path.exists(multimodal_mapping_file):\n",
        "        multimodal_features = np.load(multimodal_file)\n",
        "        with open(multimodal_mapping_file, 'r') as f:\n",
        "            multimodal_mapping = json.load(f)\n",
        "        multimodal_movie_ids = multimodal_mapping['movie_ids']\n",
        "        \n",
        "        print(f\"Multimodal features loaded: {multimodal_features.shape}\")\n",
        "        print(f\"Multimodal movies: {len(multimodal_movie_ids)}\")\n",
        "        \n",
        "        # Align multimodal features with rating matrix\n",
        "        aligned_multimodal = np.zeros((len(unique_movies), multimodal_features.shape[1]))\n",
        "        multimodal_movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(multimodal_movie_ids)}\n",
        "        \n",
        "        for movie_idx, movie_id in enumerate(unique_movies):\n",
        "            if movie_id in multimodal_movie_to_idx:\n",
        "                mm_idx = multimodal_movie_to_idx[movie_id]\n",
        "                aligned_multimodal[movie_idx] = multimodal_features[mm_idx]\n",
        "        \n",
        "        print(f\"Multimodal features aligned: {aligned_multimodal.shape}\")\n",
        "    else:\n",
        "        print(\"WARNING: Multimodal feature files not found, will use traditional features\")\n",
        "    \n",
        "    # Data splitting\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    RANDOM_SEED = 42\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    \n",
        "    # Create test set indices\n",
        "    non_zero_indices = np.where(rating_matrix != 0)\n",
        "    test_indices = np.random.choice(len(non_zero_indices[0]), size=int(0.2 * len(non_zero_indices[0])), replace=False)\n",
        "    \n",
        "    # Create training and test matrices\n",
        "    train_matrix = rating_matrix.copy()\n",
        "    test_data = []\n",
        "    \n",
        "    for idx in test_indices:\n",
        "        user_idx = non_zero_indices[0][idx]\n",
        "        movie_idx = non_zero_indices[1][idx]\n",
        "        true_rating = rating_matrix[user_idx, movie_idx]\n",
        "        \n",
        "        test_data.append((user_idx, movie_idx, true_rating))\n",
        "        train_matrix[user_idx, movie_idx] = 0  # Remove from training set\n",
        "    \n",
        "    print(f\"Data splitting completed:\")\n",
        "    print(f\"   Training set: {np.count_nonzero(train_matrix)} ratings\")\n",
        "    print(f\"   Test set: {len(test_data)} ratings\")\n",
        "    print(f\"   Random seed: {RANDOM_SEED}\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Cannot perform data preparation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model evaluation and performance comparison\n",
        "print(\"Model Evaluation and Performance Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def evaluate_model(model, test_data, model_name, sample_size=500):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    \n",
        "    if len(test_data) > sample_size:\n",
        "        test_sample = np.random.choice(len(test_data), size=sample_size, replace=False)\n",
        "        sample_data = [test_data[i] for i in test_sample]\n",
        "    else:\n",
        "        sample_data = test_data\n",
        "    \n",
        "    predictions = []\n",
        "    true_ratings = []\n",
        "    \n",
        "    for user_idx, movie_idx, true_rating in tqdm(sample_data, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            pred = model.predict(user_idx, movie_idx)\n",
        "            # Constrain predictions to 1-5 range\n",
        "            pred = max(1, min(5, pred))\n",
        "            predictions.append(pred)\n",
        "            true_ratings.append(true_rating)\n",
        "        except Exception as e:\n",
        "            print(f\"   WARNING: Prediction failed: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if len(predictions) > 0:\n",
        "        rmse = np.sqrt(mean_squared_error(true_ratings, predictions))\n",
        "        mae = mean_absolute_error(true_ratings, predictions)\n",
        "        \n",
        "        print(f\"   RMSE: {rmse:.4f}, MAE: {mae:.4f} (samples: {len(predictions)})\")\n",
        "        return rmse, mae, len(predictions)\n",
        "    else:\n",
        "        print(f\"   ERROR: No valid predictions\")\n",
        "        return None, None, 0\n",
        "\n",
        "# Model performance evaluation\n",
        "if 'train_matrix' in locals() and 'test_data' in locals():\n",
        "    print(\"Starting model training and evaluation\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # 1. User collaborative filtering (rating only)\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Traditional Collaborative Filtering Methods\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    user_cf_rating = UserBasedCF(train_matrix, use_features=False)\n",
        "    rmse, mae, samples = evaluate_model(user_cf_rating, test_data, \"User CF (Rating Only)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"User CF (Rating Only)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 2. User collaborative filtering (rating + user features)\n",
        "    user_cf_features = UserBasedCF(train_matrix, user_feature_matrix, use_features=True)\n",
        "    rmse, mae, samples = evaluate_model(user_cf_features, test_data, \"User CF (Rating + User Features)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"User CF (Rating + User Features)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 3. Item collaborative filtering (rating only)\n",
        "    item_cf_rating = ItemBasedCF(train_matrix, use_features=False)\n",
        "    rmse, mae, samples = evaluate_model(item_cf_rating, test_data, \"Item CF (Rating Only)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"Item CF (Rating Only)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 4. Hybrid recommendation (traditional) - reproduce best configuration\n",
        "    print(f\"\\nReproducing best HybridRec configuration: SVD user features + rating-only items\")\n",
        "    \n",
        "    # Use SVD dimensionality reduction for user features\n",
        "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "    user_features_svd = svd.fit_transform(user_feature_matrix)\n",
        "    \n",
        "    user_cf_svd = UserBasedCF(train_matrix, user_features_svd, use_features=True)\n",
        "    item_cf_rating_only = ItemBasedCF(train_matrix, use_features=False)\n",
        "    \n",
        "    hybrid_best = HybridRecommender(user_cf_svd, item_cf_rating_only, user_weight=0.5, item_weight=0.5)\n",
        "    rmse, mae, samples = evaluate_model(hybrid_best, test_data, \"HybridRec (SVD User + Rating-only Item)\")\n",
        "    if rmse is not None:\n",
        "        results.append({\"model\": \"HybridRec (SVD User + Rating-only Item)\", \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # 5. Multimodal recommendation system (if features available)\n",
        "    if 'aligned_multimodal' in locals() and aligned_multimodal is not None:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"Enhanced Multimodal Recommendation System\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Multimodal recommendation (different weight configurations)\n",
        "        multimodal_configs = [\n",
        "            (0.6, 0.3, 0.1, \"Multimodal (Rating Dominant)\"),\n",
        "            (0.4, 0.4, 0.2, \"Multimodal (Balanced)\"),\n",
        "            (0.3, 0.5, 0.2, \"Multimodal (Content Dominant)\"),\n",
        "            (0.5, 0.5, 0.0, \"Multimodal (No User Features)\")\n",
        "        ]\n",
        "        \n",
        "        for alpha, beta, gamma, name in multimodal_configs:\n",
        "            multimodal_rec = MultimodalRecommender(\n",
        "                train_matrix, aligned_multimodal, user_feature_matrix,\n",
        "                alpha=alpha, beta=beta, gamma=gamma\n",
        "            )\n",
        "            rmse, mae, samples = evaluate_model(multimodal_rec, test_data, name)\n",
        "            if rmse is not None:\n",
        "                results.append({\"model\": name, \"rmse\": rmse, \"mae\": mae, \"samples\": samples})\n",
        "    \n",
        "    # Results summary and analysis\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Model Performance Leaderboard (sorted by RMSE)\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Sort by RMSE\n",
        "        results_sorted = sorted(results, key=lambda x: x['rmse'])\n",
        "        \n",
        "        print(f\"{'Rank':<4} {'Model':<40} {'RMSE':<8} {'MAE':<8} {'Samples':<8}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        for i, result in enumerate(results_sorted, 1):\n",
        "            print(f\"{i:<4} {result['model']:<40} {result['rmse']:<8.4f} {result['mae']:<8.4f} {result['samples']:<8}\")\n",
        "        \n",
        "        # Performance analysis\n",
        "        best_model = results_sorted[0]\n",
        "        print(f\"\\nBest model: {best_model['model']} (RMSE: {best_model['rmse']:.4f})\")\n",
        "        \n",
        "        # Find best traditional model for comparison\n",
        "        traditional_models = [r for r in results_sorted if \"Multimodal\" not in r['model']]\n",
        "        if traditional_models:\n",
        "            best_traditional = traditional_models[0]\n",
        "            print(f\"Best traditional model: {best_traditional['model']} (RMSE: {best_traditional['rmse']:.4f})\")\n",
        "            \n",
        "            # If multimodal models exist, compare improvement\n",
        "            multimodal_models = [r for r in results_sorted if \"Multimodal\" in r['model']]\n",
        "            if multimodal_models:\n",
        "                best_multimodal = multimodal_models[0]\n",
        "                improvement = (best_traditional['rmse'] - best_multimodal['rmse']) / best_traditional['rmse'] * 100\n",
        "                print(f\"Best multimodal model: {best_multimodal['model']} (RMSE: {best_multimodal['rmse']:.4f})\")\n",
        "                print(f\"Multimodal improvement: {improvement:+.2f}% ({best_multimodal['rmse']:.4f} vs {best_traditional['rmse']:.4f})\")\n",
        "        \n",
        "        # Save results\n",
        "        results_file = os.path.join(DATA_DIR, 'evaluation_results.json')\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'results': results_sorted,\n",
        "                'best_model': best_model,\n",
        "                'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'random_seed': RANDOM_SEED,\n",
        "                'test_sample_size': len(test_data)\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        print(f\"\\nEvaluation results saved: {results_file}\")\n",
        "        \n",
        "        # Create performance comparison DataFrame\n",
        "        results_df = pd.DataFrame(results_sorted)\n",
        "        results_csv = os.path.join(DATA_DIR, 'model_comparison.csv')\n",
        "        results_df.to_csv(results_csv, index=False)\n",
        "        print(f\"Comparison table saved: {results_csv}\")\n",
        "        \n",
        "        print(f\"\\nEnhanced multimodal movie recommendation system evaluation completed!\")\n",
        "        print(f\"\\nKey findings:\")\n",
        "        print(f\"   Target movie count: {TARGET_MOVIE_COUNT}\")\n",
        "        print(f\"   Number of users: {len(unique_users)}\")\n",
        "        print(f\"   Number of movies: {len(unique_movies)}\")\n",
        "        print(f\"   Number of ratings: {len(ratings_data)}\")\n",
        "        print(f\"   Test samples: {len(test_data)}\")\n",
        "        print(f\"   Best RMSE: {best_model['rmse']:.4f}\")\n",
        "        if 'aligned_multimodal' in locals() and aligned_multimodal is not None:\n",
        "            print(f\"   Multimodal feature dimension: {aligned_multimodal.shape[1]}\")\n",
        "            print(f\"   Feature types: Image (ViT) + Text (BERT) + Cast & Crew Statistics\")\n",
        "        \n",
        "    else:\n",
        "        print(\"ERROR: No successful evaluation results\")\n",
        "        \n",
        "else:\n",
        "    print(\"ERROR: Training data not ready, cannot perform evaluation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
